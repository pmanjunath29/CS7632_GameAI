{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Jx5dkudPV6"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "Read the assignment instructions here: https://github.com/markriedl/coinrun-game-ai-assignment\n",
        "\n",
        "Those new to pytorch may find this [primer](https://colab.research.google.com/drive/1DgkVmi6GksWOByhYVQpyUB4Rk3PUq0Cp) helpful.\n",
        "\n",
        "Those new to convolutional neural networks may find this [example](https://colab.research.google.com/drive/1740OwEI4oi5QlPtq4UFd3ha44yxef7gU) helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbcPfbI2CBGr"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Run the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rodhvmBKCDI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#del os.environ['LD_PRELOAD']\n",
        "#!apt-get remove libtcmalloc*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApGB65bRXdWL",
        "outputId": "2eb66949-f4c8-4a50-85b0-ce7982071cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,150 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,334 kB]\n",
            "Fetched 4,823 kB in 4s (1,348 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu4).\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "qt5-default is already the newest version (5.12.8+dfsg-0ubuntu2.1).\n",
            "Suggested packages:\n",
            "  mpich-doc\n",
            "The following NEW packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 mpich\n",
            "0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 3,476 kB of archives.\n",
            "After this operation, 18.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 hwloc-nox amd64 2.1.0+dfsg-4 [159 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libmpich12 amd64 3.3.2-2build1 [1,177 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 mpich amd64 3.3.2-2build1 [395 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libmpich-dev amd64 3.3.2-2build1 [1,745 kB]\n",
            "Fetched 3,476 kB in 3s (1,333 kB/s)\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../hwloc-nox_2.1.0+dfsg-4_amd64.deb ...\n",
            "Unpacking hwloc-nox (2.1.0+dfsg-4) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../libmpich12_3.3.2-2build1_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (3.3.2-2build1) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../mpich_3.3.2-2build1_amd64.deb ...\n",
            "Unpacking mpich (3.3.2-2build1) ...\n",
            "Selecting previously unselected package libmpich-dev:amd64.\n",
            "Preparing to unpack .../libmpich-dev_3.3.2-2build1_amd64.deb ...\n",
            "Unpacking libmpich-dev:amd64 (3.3.2-2build1) ...\n",
            "Setting up hwloc-nox (2.1.0+dfsg-4) ...\n",
            "Setting up libmpich12:amd64 (3.3.2-2build1) ...\n",
            "Setting up mpich (3.3.2-2build1) ...\n",
            "Setting up libmpich-dev:amd64 (3.3.2-2build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install mpich build-essential qt5-default pkg-config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nfOUNUHYHk4",
        "outputId": "f594990c-b502-4462-ce09-e92a45a2ea54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'coinrun-game-ai-assignment'...\n",
            "remote: Enumerating objects: 691, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 691 (delta 21), reused 1 (delta 0), pack-reused 646\u001b[K\n",
            "Receiving objects: 100% (691/691), 37.74 MiB | 20.72 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/markriedl/coinrun-game-ai-assignment.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCz2c9bYYNPr",
        "outputId": "8f16adb8-aa6b-4bd8-cf83-759b956d0ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/openai/baselines/archive/7139a66d333b94c2dafc4af35f6a8c7598361df6.zip (from -r coinrun-game-ai-assignment/requirements.txt (line 9))\n",
            "  Downloading https://github.com/openai/baselines/archive/7139a66d333b94c2dafc4af35f6a8c7598361df6.zip (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r coinrun-game-ai-assignment/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r coinrun-game-ai-assignment/requirements.txt (line 3)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r coinrun-game-ai-assignment/requirements.txt (line 4)) (0.15.1+cu118)\n",
            "Collecting gym~=0.17.1\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyglet~=1.5.0\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpi4py~=3.0\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from -r coinrun-game-ai-assignment/requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (16.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.32.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (23.3.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.54.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (16.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.1->-r coinrun-game-ai-assignment/requirements.txt (line 5)) (1.10.1)\n",
            "Collecting pyglet~=1.5.0\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet~=1.5.0->-r coinrun-game-ai-assignment/requirements.txt (line 6)) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.5->-r coinrun-game-ai-assignment/requirements.txt (line 9)) (4.65.0)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m257.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.5->-r coinrun-game-ai-assignment/requirements.txt (line 9)) (4.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.5->-r coinrun-game-ai-assignment/requirements.txt (line 9)) (8.1.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.5->-r coinrun-game-ai-assignment/requirements.txt (line 9)) (4.7.0.72)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r coinrun-game-ai-assignment/requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (2.1.2)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->baselines==0.1.5->-r coinrun-game-ai-assignment/requirements.txt (line 9)) (3.5.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r coinrun-game-ai-assignment/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r coinrun-game-ai-assignment/requirements.txt (line 2)) (3.2.2)\n",
            "Building wheels for collected packages: gym, mpi4py, baselines\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654650 sha256=9b7410d7bca233381d87b871b6ad97b38549119729a5fce85dcb9943937cd7fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=3365657 sha256=357d6e18b9fdb8338e999eaa1888cf5272ebe45d6a3175b9a0410ef59d25457a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.5-py3-none-any.whl size=210142 sha256=9c89fe60dfeb961a5b96d2401d08920ba7e8a4fd59c2797879b06089d079b230\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b79_klvv/wheels/64/71/d0/08b826a130fa20d9d809f8f3ab11153cac7d95d142d2137cf6\n",
            "Successfully built gym mpi4py baselines\n",
            "Installing collected packages: pyglet, mpi4py, dill, cloudpickle, gym, baselines\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed baselines-0.1.5 cloudpickle-1.6.0 dill-0.3.6 gym-0.17.3 mpi4py-3.1.4 pyglet-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r coinrun-game-ai-assignment/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MBLQF6MYUw0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'coinrun-game-ai-assignment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW2gBefjYZU1",
        "outputId": "9be34573-c195-48ad-cd5e-b787990ff922"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzI7EuW1C4Dj"
      },
      "source": [
        "If the prior cell repots ```False```, then use notebooks settings to turn GPU support on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg9yJt6MYhJl",
        "outputId": "cbc3cfbd-779b-4214-dea5-076aba5176b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /tmp/openai-2023-04-29-20-47-23-255815\n",
            "step 0 rews [0.]\n",
            "step 1 rews [0.]\n",
            "step 2 rews [0.]\n",
            "step 3 rews [0.]\n",
            "step 4 rews [0.]\n",
            "step 5 rews [0.0999999]\n",
            "step 6 rews [0.0999999]\n",
            "step 7 rews [0.0999999]\n",
            "step 8 rews [0.0999999]\n",
            "step 9 rews [0.0999999]\n"
          ]
        }
      ],
      "source": [
        "# THIS TESTS THE COINRUN INSTALLATION\n",
        "from coinrun.random_agent import random_agent\n",
        "\n",
        "random_agent(max_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgvgUwlQ733"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOihwhu6QDsA"
      },
      "outputs": [],
      "source": [
        "### RUN THIS BUT DO NOT EDIT THIS CELL\n",
        "def in_ipynb():\n",
        "  try:\n",
        "    result = get_ipython().__class__.__name__\n",
        "    if 'Shell' in result:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "IN_PYNB = in_ipynb()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8I-vSF8QBud"
      },
      "outputs": [],
      "source": [
        "### RUN THIS BUT DO NOT EDIT THIS CELL\n",
        "import gym\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "from coinrun import setup_utils, make\n",
        "import coinrun.main_utils as utils\n",
        "from coinrun.config import Config\n",
        "if not IN_PYNB:\n",
        "    from gym.envs.classic_control import rendering\n",
        "from coinrun import policies, wrappers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import pdb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6OiEyfpYstU"
      },
      "outputs": [],
      "source": [
        "### RUN THIS BUT DO NOT EDIT THIS CELL\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Resize the screen to this\n",
        "RESIZE_CONST = 40 \n",
        "\n",
        "\n",
        "# Game seed information\n",
        "NUM_LEVELS = 1 # repeat the same level over and over\n",
        "EASY_LEVEL = 1 # Start on a very small map, no enemies\n",
        "EASY_LEVEL2 = 5 # Very small map, no enemies\n",
        "MEDIUM_LEVEL = 20 # Medium length, no enemies\n",
        "MEDIUM_LEVEL2 = 45 # Medium length, no enemies\n",
        "ONE_MONSTER = 10 # Short map with one monster\n",
        "HARD_LEVEL = 7 # Longer and with monsters\n",
        "LAVA_LEVEL = 3 # Longer and with lava and pits\n",
        "\n",
        "# Defaults\n",
        "RENDER_SCREEN = False\n",
        "SAVE_FILENAME = 'saved.model'\n",
        "LOAD_FILENAME = 'saved.model'\n",
        "MODEL_PATH = 'saved_models' \n",
        "SEED = EASY_LEVEL\n",
        "\n",
        "# Don't play with this\n",
        "EVAL_EPSILON = 0.1\n",
        "EVAL_WINDOW_SIZE = 3\n",
        "EVAL_COUNT = 10\n",
        "TIMEOUT = 1000\n",
        "COIN_REWARD = 100\n",
        "\n",
        "### Data structure for holding experiences for replay\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "### Function for resizing the screen\n",
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(RESIZE_CONST, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "### Save the model. Extra information can be added to the end of the filename\n",
        "def save_model(model, filename, extras = None):\n",
        "    if extras is not None:\n",
        "        filename = filename + '.' + str(extras)\n",
        "    print(\"Saving\", filename, \"...\")\n",
        "    torch.save(model, os.path.join(MODEL_PATH, filename))\n",
        "    print(\"Done saving.\")\n",
        " \n",
        "### Load the model. If there are multiple versions with extra information at the\n",
        "### end of the filename, get the latest.\n",
        "def load_model(filename, extras = None):\n",
        "    if extras is not None:\n",
        "        filename = filename + '.' + str(extras)\n",
        "    model = None\n",
        "    candidates = [os.path.join(MODEL_PATH, f) for f in os.listdir(MODEL_PATH) if filename in f]\n",
        "    if len(candidates) > 0:\n",
        "        candidates = sorted(candidates, key=lambda f:os.stat(f).st_mtime, reverse=True)\n",
        "        filename = candidates[0]\n",
        "        print(\"Loading\", filename, \"...\")\n",
        "        model = torch.load(filename)\n",
        "        print(\"Done loading.\")\n",
        "    return model\n",
        "  \n",
        "### Give a text description of the outcome of an episode and also a score\n",
        "### Score is duration, unless the agent died.\n",
        "def episode_status(duration, reward):\n",
        "    status = \"\"\n",
        "    score = 0\n",
        "    if duration >= TIMEOUT:\n",
        "        status = \"timeout\"\n",
        "        score = duration\n",
        "    elif reward < COIN_REWARD:\n",
        "        status = \"died\"\n",
        "        score = TIMEOUT\n",
        "    else:\n",
        "        status = \"coin\"\n",
        "        score = duration\n",
        "    return status, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s933aiIDpoL9"
      },
      "source": [
        "Every time the code runs an ```evaluate()``` of the agent's model, a new directory will be created in ```/content/cache``` and screenshots of the agent in action will be stored. Filenames are of the form ```/content/cache/eval_number/eval + screen_number + .jpeg```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5N9wLAepnhC"
      },
      "outputs": [],
      "source": [
        "RUN_NUM = 0\n",
        "SCREEN_SAVE = True\n",
        "SCREEN_COUNT = 0\n",
        "SCREEN_SAVE_PREFIX = 'eval'\n",
        "SCREEN_SAVE_POSTFIX = '.jpeg'\n",
        "TEMP_DIR = 'cache'\n",
        "SCREEN_SAVE_RATIO = 0.5\n",
        "\n",
        "#Screen is numpy array\n",
        "def save_screen(screen):\n",
        "  global SCREEN_COUNT\n",
        "  screen_array_t = np.transpose(screen, (1, 2, 0))\n",
        "  img = Image.fromarray(np.uint8(screen_array_t * 255))\n",
        "  width, height = img.size\n",
        "  img = img.resize((int(width*SCREEN_SAVE_RATIO), int(height*SCREEN_SAVE_RATIO)), Image.LANCZOS)\n",
        "  if not os.path.isdir(TEMP_DIR):\n",
        "    os.mkdir(TEMP_DIR)\n",
        "  if not os.path.isdir(os.path.join(TEMP_DIR, str(RUN_NUM))):\n",
        "    os.mkdir(os.path.join(TEMP_DIR, str(RUN_NUM)))\n",
        "  img.save(os.path.join(TEMP_DIR, str(RUN_NUM), SCREEN_SAVE_PREFIX + str(SCREEN_COUNT) + SCREEN_SAVE_POSTFIX), \"JPEG\")\n",
        "  SCREEN_COUNT = SCREEN_COUNT + 1\n",
        "\n",
        "def show_movie(num):\n",
        "  dir_name = str(num)\n",
        "  files = []\n",
        "  for file in os.listdir(os.path.join(TEMP_DIR, dir_name)):\n",
        "    files.append(file)\n",
        "  sorted_files = sorted(files, key=lambda f:int(f[len(SCREEN_SAVE_PREFIX):-len(SCREEN_SAVE_POSTFIX)]))\n",
        "  for file in sorted_files:\n",
        "    img = Image.open(os.path.join(TEMP_DIR, dir_name, file), 'r')\n",
        "    plt.imshow(img)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "def make_anim(num):\n",
        "  dir_name = str(num)\n",
        "  files = []\n",
        "  images = []\n",
        "  for file in os.listdir(os.path.join(TEMP_DIR, dir_name)):\n",
        "    files.append(file)\n",
        "  sorted_files = sorted(files, key=lambda f:int(f[len(SCREEN_SAVE_PREFIX):-len(SCREEN_SAVE_POSTFIX)]))\n",
        "  for file in sorted_files:\n",
        "    try:\n",
        "      img = Image.open(os.path.join(TEMP_DIR, dir_name, file))\n",
        "      images.append(img)\n",
        "    except:\n",
        "      print(os.path.join(TEMP_DIR, dir_name, file), \"did not load.\")\n",
        "  images[0].save('movie' + str(num) + '.gif', \"GIF\",\n",
        "                      save_all=True,\n",
        "                      append_images=images[1:],\n",
        "                      duration=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDC4SQYzRAY8"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B930j8EMQq2l"
      },
      "outputs": [],
      "source": [
        "# YOU MAY EDIT THESE IF NECESSARY\n",
        "\n",
        "BATCH_SIZE = 128            # How many replay experiences to run through neural net at once\n",
        "GAMMA = 0.999               # How much to discount the future [0..1]\n",
        "BOOTSTRAP = 10000           # How many steps to run to fill up replay memory before training starts\n",
        "TARGET_UPDATE = 0           # Delays updating the network for loss calculations. 0=don't delay, or 1+ number of episodes\n",
        "REPLAY_CAPACITY = 10000     # How big is the replay memory\n",
        "EPSILON = 0.9               # Use random action if less than epsilon [0..1]\n",
        "EVAL_INTERVAL = 10          # How many episodes of training before evaluation\n",
        "NUM_EPISODES = 500          # Max number of training episodes\n",
        "RANDOM_SEED = None          # Seed for random number generator, for reproducability, use None for random seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGXg2Wp0Q2m5"
      },
      "source": [
        "# Reference Functions\n",
        "\n",
        "Run these cells but do not edit them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcBRVLjwUIl6"
      },
      "source": [
        "## Unit Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_McIDrJPgnT"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL BUT DO NOT EDIT.\n",
        "\n",
        "def testReplayMemory():\n",
        "    print(\"Testing ReplayMemory...\")\n",
        "    capacity = 100\n",
        "    test_replay_memory = ReplayMemory(capacity)\n",
        "    for i in range(capacity):\n",
        "        test_replay_memory.push(i, i, i, i)\n",
        "    assert (len(test_replay_memory) == capacity),\"size test failed\"\n",
        "    for i in range(len(test_replay_memory)):\n",
        "        item = test_replay_memory.memory[i]\n",
        "        assert (item[0] == i), \"item\" + str(i) + \"not holding the correct value\"\n",
        "    for i in range(capacity//2):\n",
        "        test_replay_memory.push(capacity+i, capacity+i, capacity+i, capacity+i)\n",
        "    assert (len(test_replay_memory) == capacity), \"size test 2 failed\"\n",
        "    # check items\n",
        "    for i in range(len(test_replay_memory)):\n",
        "        item = test_replay_memory.memory[i]\n",
        "        if i < capacity // 2:\n",
        "            assert (item[0] == i+capacity), \"not holding the correct value after looping (first half)\"\n",
        "        else:\n",
        "            assert (item[0] == i), \"not holding the correct value after looping (second half)\"\n",
        "    print(\"ReplayMemory test passed.\")\n",
        "    return True\n",
        "  \n",
        "def testMakeBatch():\n",
        "    print(\"Testing doMakeBatch...\")\n",
        "    batch_size = 128\n",
        "    capacity = batch_size * 2\n",
        "    test_replay_memory = ReplayMemory(capacity)\n",
        "    state = None\n",
        "    new_state = None\n",
        "    action = None\n",
        "    reward = None\n",
        "    # Test types and shapes of return values\n",
        "    for i in range(capacity):\n",
        "        state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        new_state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        action = torch.randn(1, 1, device=DEVICE)\n",
        "        reward = torch.randn(1, 1, device=DEVICE)\n",
        "        test_replay_memory.push(state, action, new_state, reward)\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(test_replay_memory, batch_size)\n",
        "    assert(type(states_batch) == torch.Tensor and states_batch.size() == (batch_size, 3, 80, 80)), \"states batch not correct shape.\"\n",
        "    assert(type(actions_batch) == torch.Tensor and actions_batch.size() == (batch_size, 1)), \"actions batch not correct shape.\"\n",
        "    assert(type(next_states_batch) == torch.Tensor and next_states_batch.size() == (batch_size, 3, 80, 80)), \"next states batch not correct shape.\"\n",
        "    assert(type(rewards_batch) == torch.Tensor and rewards_batch.size() == (batch_size, 1)), \"rewards batch not correct shape.\"\n",
        "    assert(type(non_final_mask) == type(torch.tensor(batch_size, dtype=torch.bool, device=DEVICE)) and non_final_mask.size()[0] == batch_size), \"non-final mask not correct shape.\"\n",
        "\n",
        "    # Test mask\n",
        "    test_replay_memory = ReplayMemory(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        new_state = None\n",
        "        if i % 2 == 0:\n",
        "            new_state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        action = torch.randn(1, 1, device=DEVICE)\n",
        "        reward = torch.randn(1, 1, device=DEVICE)\n",
        "        test_replay_memory.push(state, action, new_state, reward)\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(test_replay_memory, batch_size)\n",
        "    assert(non_final_mask.sum() == batch_size//2), \"non_final_mask not masking properly.\"\n",
        "    print(\"doMakeBatch test passed.\")\n",
        "    return True\n",
        "\n",
        "class UnitTestDQN(nn.Module):\n",
        "    def __init__(self, h, w, num_actions):\n",
        "        super(UnitTestDQN, self).__init__()\n",
        "        self.num_actions = num_actions\n",
        "    def forward(self, x):\n",
        "        assert(False), \"Network should not be queried when epsilon = 1.0.\" \n",
        "        return None\n",
        "\n",
        "def testSelectAction():\n",
        "    print(\"Testing select_action...\")\n",
        "    from scipy.stats import chisquare\n",
        "    sample_size = 10000\n",
        "    num_tests = 100\n",
        "    pass_rate = 0.9\n",
        "    screen_height = 40\n",
        "    screen_width = 40\n",
        "    epsilon = 1.0\n",
        "    num_actions = 7\n",
        "    test_results = {True: 0, False: 0}\n",
        "    significance_level = 0.02\n",
        "    net = UnitTestDQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "    for j in range(num_tests):\n",
        "        samples = {}\n",
        "        for i in range(sample_size):\n",
        "            action, new_epsilon = select_action(state, net, num_actions, epsilon, steps_done = 0, bootstrap_threshold = 2)\n",
        "            assert(type(action) == torch.Tensor and action.size() == (1,1)), \"Action not correct shape.\"\n",
        "            assert(new_epsilon == epsilon), \"Epsilon should not change during bootstrapping.\"\n",
        "            action = action.item()\n",
        "            if action not in samples:\n",
        "                samples[action] = 0\n",
        "            samples[action] = samples[action] + 1\n",
        "        expected = [sample_size / num_actions] * num_actions\n",
        "        statistic, pvalue = chisquare(f_obs=list(samples.values()), f_exp=expected)\n",
        "        test_results[pvalue >= significance_level] += 1\n",
        "    assert(test_results[True] > pass_rate * num_tests), \"Random sample is not from uniform distribution.\"    \n",
        "    print(\"select_action test passed.\")\n",
        "    return True\n",
        "\n",
        "def testPredictQValues():\n",
        "    print(\"Testing doPredictQValues...\")\n",
        "    batch_size = 128\n",
        "    screen_height = 80\n",
        "    screen_width = 80\n",
        "    num_actions = 7\n",
        "    net = DQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    states_batch = torch.randn(batch_size, 3, 80, 80, device=DEVICE)\n",
        "    actions_batch = torch.randint(0, 7, (128, 1), device=DEVICE).long()\n",
        "    state_action_values = doPredictQValues(net, states_batch, actions_batch)\n",
        "    assert(type(state_action_values) == torch.Tensor and state_action_values.size() == (128, 1)), \"Return value not correct shape.\"\n",
        "    print(\"doPredictQValues test passed.\")\n",
        "    return True\n",
        "\n",
        "def testPredictNextStateUtilities():\n",
        "    print(\"Testing doPredictNextStateUtilities...\")\n",
        "    screen_height = 80\n",
        "    screen_width = 80\n",
        "    num_actions = 7\n",
        "    batch_size = 128\n",
        "    passed = False\n",
        "    net = DQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    # First option to try is that the batch is full sized.\n",
        "    try:\n",
        "        next_states_batch = torch.ones(batch_size, 3, 80, 80, device=DEVICE)\n",
        "        non_final_mask = torch.ones(batch_size, dtype=torch.bool, device=DEVICE)\n",
        "        for i in range(batch_size):\n",
        "            if i % 2 == 1:\n",
        "                next_states_batch[i].fill_(0)\n",
        "                non_final_mask[i] = 0\n",
        "        next_state_values = doPredictNextStateUtilities(net, next_states_batch, non_final_mask, batch_size)\n",
        "        assert(type(next_state_values) == torch.Tensor and next_state_values.size() == (batch_size, 1)), \"Return value not correct shape (attempt 1).\"\n",
        "        for i in range(batch_size):\n",
        "            if i % 2 == 1:\n",
        "                assert(next_state_values[i].sum() == 0), \"Element \" + str(i) + \"is not 0.0 when non_final_mask[i] = 0\"\n",
        "        passed = True\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "        print(\"Will try alternative test.\")\n",
        "    if not passed:\n",
        "        # Next option is that batch is not full sized.\n",
        "        try:\n",
        "            next_states_batch = torch.ones(batch_size-1, 3, 80, 80, device=DEVICE)\n",
        "            non_final_mask = torch.ones(batch_size, dtype=torch.bool, device=DEVICE)\n",
        "            non_final_mask[0] = 0\n",
        "            next_state_values = doPredictNextStateUtilities(net, next_states_batch, non_final_mask, batch_size)\n",
        "            assert(type(next_state_values) == torch.Tensor and next_state_values.size() == (batch_size, 1)), \"Return value not correctd shape (attempt 2).\"\n",
        "            passed = True\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "            print(\"No further alternative tests available.\")\n",
        "    if passed:\n",
        "        print(\"doPredictNextStateUtilities test passed.\")\n",
        "        return True\n",
        "    assert(False), \"doPredictNextStateUtilities did NOT pass test.\"\n",
        "\n",
        "def testComputeExpectedQValues():\n",
        "    print(\"Testing doComputeExpectedQValues...\")\n",
        "    batch_size = 128\n",
        "    gamma = 0.5\n",
        "    next_state_values = torch.ones(batch_size).unsqueeze(1)\n",
        "    rewards_batch = torch.ones(batch_size).unsqueeze(1)\n",
        "    expected_state_action_values = doComputeExpectedQValues(next_state_values, rewards_batch, gamma)\n",
        "    assert(type(expected_state_action_values) == torch.Tensor and expected_state_action_values.size()[0] == batch_size), \"Return value not expected shape.\"\n",
        "    for i in range(batch_size):\n",
        "        assert(expected_state_action_values[i].item() == 1.5), \"Element \" + str(i) + \" doesn't have the correct value.\"\n",
        "    print(\"doComputeExpectedQValues test passed.\")\n",
        "    return True\n",
        "\n",
        "def testComputeLoss():\n",
        "    print(\"Testing doComputeLoss...\")\n",
        "    batch_size = 128\n",
        "    state_action_values = torch.randn(batch_size, device=DEVICE)\n",
        "    expected_state_action_values = torch.randn(batch_size, device=DEVICE)\n",
        "    loss = doComputeLoss(state_action_values, expected_state_action_values)\n",
        "    assert(type(loss) == torch.Tensor and len(loss.size()) == 0), \"Loss not of expected shape.\"\n",
        "    print(\"doComputeLoss test passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def unit_test():\n",
        "    testReplayMemory()\n",
        "    testMakeBatch()\n",
        "    testSelectAction()\n",
        "    testPredictQValues()\n",
        "    testPredictNextStateUtilities()\n",
        "    testComputeExpectedQValues()\n",
        "    testComputeLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpQNNe2yUN_d"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Read this code. It will be helpful to know how the functions you will be modifying are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPuXQWPDQTem"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL BUT DO NOT EDIT.\n",
        "\n",
        "### Training loop.\n",
        "### Each episode is a game that runs until the agent gets the coin or the game times out.\n",
        "### Train for a given number of episodes.\n",
        "def train(num_episodes = NUM_EPISODES, load_filename = None, save_filename = None, eval_interval = EVAL_INTERVAL, replay_capacity = REPLAY_CAPACITY, bootstrap_threshold = BOOTSTRAP, epsilon = EPSILON, eval_epsilon = EVAL_EPSILON, gamma = GAMMA, batch_size = BATCH_SIZE, target_update = TARGET_UPDATE, random_seed = RANDOM_SEED, num_levels = NUM_LEVELS, seed = SEED):\n",
        "    # Set the random seed\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    # Set up the environment\n",
        "    setup_utils.setup_and_load(use_cmd_line_args=False, is_high_res=True, num_levels=num_levels, set_seed=seed)\n",
        "    env = make('standard', num_envs=1)\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "\n",
        "    # Get screen size so that we can initialize layers correctly based on shape returned from AI gym. \n",
        "    init_screen = get_screen(env)\n",
        "    _, _, screen_height, screen_width = init_screen.shape\n",
        "    print(\"screen size: \", screen_height, screen_width)\n",
        "\n",
        "    # Are we resuming from an existing model?\n",
        "    policy_net = None\n",
        "    if load_filename is not None and os.path.isfile(os.path.join(MODEL_PATH, load_filename)):\n",
        "        print(\"Loading model...\")\n",
        "        policy_net = load_model(load_filename)\n",
        "        policy_net = policy_net.to(DEVICE)\n",
        "        print(\"Done loading.\")\n",
        "    else:\n",
        "        print(\"Making new model.\")\n",
        "        policy_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    # Make a copy of the policy network for evaluation purposes\n",
        "    eval_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    eval_net.load_state_dict(policy_net.state_dict())\n",
        "    eval_net.eval()\n",
        "    # Target network is a snapshot of the policy network that lags behind (for stablity)\n",
        "    target_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "    \n",
        "    # Instantiate the optimizer\n",
        "    optimizer = None\n",
        "    if len(list(policy_net.parameters())) > 0:\n",
        "        optimizer = initializeOptimizer(policy_net.parameters())\n",
        "    \n",
        "    # Instantiate the replay memory\n",
        "    replay_memory = ReplayMemory(replay_capacity)\n",
        "\n",
        "    steps_done = 0               # How many steps have been run\n",
        "    best_eval = float('inf')     # The best model evaluation to date\n",
        "\n",
        "    ### Do training until episodes complete \n",
        "    print(\"training...\")\n",
        "    i_episode = 0            # The episode number\n",
        "    \n",
        "    # Stop when we reach max episodes\n",
        "    while i_episode < num_episodes:\n",
        "        print(\"episode:\", i_episode, \"epsilon:\", epsilon)\n",
        "        max_reward = 0       # The best reward we've seen this episode\n",
        "        done = False         # Has the game ended (timed out or got the coin)\n",
        "        episode_steps = 0    # Number of steps performed in this episode\n",
        "        # Initialize the environment and state\n",
        "        env.reset()\n",
        "        \n",
        "        # Current screen. There is no last screen because we get velocity on the screen itself.\n",
        "        state = get_screen(env)\n",
        "\n",
        "        # Do forever until the loop breaks \n",
        "        while not done:\n",
        "            # Select and perform an action\n",
        "            action, epsilon = select_action(state, policy_net, env.NUM_ACTIONS, epsilon, steps_done, bootstrap_threshold)\n",
        "            steps_done = steps_done + 1\n",
        "            episode_steps = episode_steps + 1\n",
        "            \n",
        "            # for debugging\n",
        "            if RENDER_SCREEN and not IN_PYNB:\n",
        "                env.render() \n",
        "\n",
        "            # Run the action in the environment\n",
        "            if action is not None: \n",
        "                _, reward, done, _ = env.step(np.array([action.item()]))\n",
        "\n",
        "                # Record if this was the best reward we've seen so far\n",
        "                max_reward = max(reward, max_reward)\n",
        "                \n",
        "                # Turn the reward into a tensor  \n",
        "                reward = torch.tensor([reward], device=DEVICE)\n",
        "\n",
        "                # Observe new state\n",
        "                current_screen = get_screen(env)\n",
        "\n",
        "                # Did the game end?\n",
        "                if not done:\n",
        "                    next_state = current_screen\n",
        "                else:\n",
        "                    next_state = None\n",
        "\n",
        "                # Store the transition in memory\n",
        "                replay_memory.push(state, action, next_state, reward)\n",
        "\n",
        "                # Move to the next state\n",
        "                state = next_state\n",
        "\n",
        "                # If we are past bootstrapping we should perform one step of the optimization\n",
        "                if steps_done > bootstrap_threshold:\n",
        "                  optimize_model(policy_net, target_net if target_update > 0 else policy_net, replay_memory, optimizer, batch_size, gamma)\n",
        "            else:\n",
        "                # Do nothing if select_action() is not implemented and returning None\n",
        "                env.step(np.array([0]))\n",
        "                \n",
        "            # If we are done, print some statistics\n",
        "            if done:\n",
        "                print(\"duration:\", episode_steps)\n",
        "                print(\"max reward:\", max_reward)\n",
        "                status, _ = episode_status(episode_steps, max_reward)\n",
        "                print(\"result:\", status)\n",
        "                print(\"total steps:\", steps_done, '\\n')\n",
        "\n",
        "            # Should we update the target network?\n",
        "            if target_update > 0 and i_episode % target_update == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "                \n",
        "        # Should we evaluate?\n",
        "        if steps_done > bootstrap_threshold and i_episode > 0 and i_episode % eval_interval == 0:\n",
        "            test_average_duration = 0       # Track the average eval duration\n",
        "            test_average_max_reward = 0     # Track the average max reward\n",
        "            # copy all the weights into the evaluation network\n",
        "            eval_net.load_state_dict(policy_net.state_dict())\n",
        "            # Evaluate 10 times\n",
        "            for _ in range(EVAL_COUNT):\n",
        "                # Call the evaluation function\n",
        "                test_duration, test_max_reward = evaluate(eval_net, eval_epsilon, env, test_seed=seed)\n",
        "                status, score = episode_status(test_duration, test_max_reward)\n",
        "                test_duration = score # Set test_duration to score to factor in death-penalty\n",
        "                test_average_duration = test_average_duration + test_duration\n",
        "                test_average_max_reward = test_average_max_reward + test_max_reward\n",
        "            test_average_duration = test_average_duration / EVAL_COUNT\n",
        "            test_average_max_reward = test_average_max_reward / EVAL_COUNT\n",
        "            print(\"Average duration:\", test_average_duration)\n",
        "            print(\"Average max reward:\", test_average_max_reward)\n",
        "            # If this is the best window average we've seen, save the model\n",
        "            if test_average_duration < best_eval:\n",
        "                best_eval = test_average_duration\n",
        "                if save_filename is not None:\n",
        "                    save_model(policy_net, save_filename, i_episode)\n",
        "            print(' ')\n",
        "        # Only increment episode number if we are done with bootstrapping\n",
        "        if steps_done > bootstrap_threshold:\n",
        "          i_episode = i_episode + 1\n",
        "    print('Training complete')\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "    env.close()\n",
        "    return policy_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCkpDANAURJT"
      },
      "source": [
        "\n",
        "## Optimization Function\n",
        "\n",
        "Read this code. It will be helpful to know how the functions you will be modifying are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l6qXukQUVyk"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL BUT DO NOT EDIT.\n",
        "\n",
        "### Take a DQN and do one forward-backward pass.\n",
        "### Since this is Q-learning, we will run a forward pass to get Q-values for state-action pairs and then \n",
        "### give the true value as the Q-values after the Q-update equation.\n",
        "def optimize_model(policy_net, target_net, replay_memory, optimizer, batch_size, gamma):\n",
        "    if len(replay_memory) < batch_size:\n",
        "        return\n",
        "    ### step 1: sample from the replay memory. Get BATCH_SIZE transitions\n",
        "    ### Step 2: Get a list of non-final next states.\n",
        "    ###         a. Create a mask, a tensor of length BATCH_SIZE where each element i is 1 if \n",
        "    ###            batch.next_state[i] is not None and 0 otherwise.\n",
        "    ###         b. Create a tensor of shape [BATCH_SIZE, color(3), height, width] by concatenating\n",
        "    ###            all non-final (not None) batch.next_states together.\n",
        "    ### Step 3: set up batches for state, action, and reward\n",
        "    ###         a. Create a tensor of shape [BATCH_SIZE, color(3), height, width] holding states\n",
        "    ###         b. Create a tensor of shape [BATCH_SIZE, 1] holding actions\n",
        "    ###         c. Create a tensor of shape [BATCH_SIZE, 1] holding rewards\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(replay_memory, batch_size)\n",
        "\n",
        "    ### Step 4: Get the action values predicted.\n",
        "    ###         a. Call policy_net(state_batch) to get a tensor of shape [BATCH_SIZE, NUM_ACTIONS] containing Q-values\n",
        "    ###         b. For each batch, get the Q-value for the corresponding action in action_batch (hint: torch.gather)\n",
        "    state_action_values = doPredictQValues(policy_net, states_batch, actions_batch)\n",
        "\n",
        "    ### Step 5: Get the utility values of next_states.\n",
        "    next_state_values = doPredictNextStateUtilities(target_net, next_states_batch, non_final_mask, batch_size)\n",
        "    \n",
        "    ### Step 6: Compute the expected Q values.\n",
        "    expected_state_action_values = doComputeExpectedQValues(next_state_values, rewards_batch, gamma)\n",
        "\n",
        "    ### Step 7: Computer Huber loss (smooth L1 loss)\n",
        "    ###         Compare state action values from step 5 to expected state action values from step 7\n",
        "    loss = doComputeLoss(state_action_values, expected_state_action_values)\n",
        "    ### Step 8: Back propagation\n",
        "    ###         a. Zero out gradients\n",
        "    ###         b. call loss.backward()\n",
        "    ###         c. Prevent gradient explosion by clipping gradients between -1 and 1\n",
        "    ###            (hint: param.grad.data is the gradients. See torch.clamp_() )\n",
        "    ###         d. Tell the optimizer that another step has occurred: optimizer.step()\n",
        "    if optimizer is not None:\n",
        "        optimizer.zero_grad()\n",
        "        doBackprop(loss, policy_net.parameters())\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXuPMZw7UZla"
      },
      "source": [
        "## Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGy1g-scUbX3"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL BUT DO NOT EDIT.\n",
        "\n",
        "### Evaluate the DQN\n",
        "### If environment is given, use that. Otherwise make a new environment.\n",
        "def evaluate(policy_net, epsilon = EVAL_EPSILON, env = None, test_seed = SEED):\n",
        "    global RUN_NUM\n",
        "    RUN_NUM = RUN_NUM + 1\n",
        "    setup_utils.setup_and_load(use_cmd_line_args=False, is_high_res=True, num_levels=NUM_LEVELS, set_seed=test_seed)\n",
        "    \n",
        "\n",
        "    # Make an environment if we don't already have one\n",
        "    if env is None:\n",
        "        env = make('standard', num_envs=1)\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "\n",
        "    # Get screen size so that we can initialize layers correctly based on shape\n",
        "    # returned from AI gym. \n",
        "    init_screen = get_screen(env, SCREEN_SAVE)\n",
        "    _, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "    # Get the network ready for evaluation (turns off some things like dropout if used)\n",
        "    policy_net.eval()\n",
        "\n",
        "    # Current screen. There is no last screen\n",
        "    state = get_screen(env, SCREEN_SAVE)\n",
        "\n",
        "    steps_done = 0         # Number of steps executed\n",
        "    max_reward = 0         # Max reward seen\n",
        "    done = False           # Is the game over?\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    while not done:\n",
        "        # Select and perform an action\n",
        "        action, _ = select_action(state, policy_net, env.NUM_ACTIONS, epsilon, steps_done=0, bootstrap_threshold=0)\n",
        "        steps_done = steps_done + 1\n",
        "\n",
        "        if RENDER_SCREEN and not IN_PYNB:\n",
        "            env.render()          \n",
        "\n",
        "        # Execute the action\n",
        "        if action is not None:\n",
        "            _, reward, done, _ = env.step(np.array([action.item()]))\n",
        "\n",
        "            # Is this the best reward we've seen?\n",
        "            max_reward = max(reward, max_reward)\n",
        "\n",
        "            # Observe new state\n",
        "            state = get_screen(env, SCREEN_SAVE)\n",
        "        else:\n",
        "            # Do nothing if select_action() is not implemented and returning None\n",
        "            env.step(np.array([0]))\n",
        "\n",
        "    print(\"duration:\", steps_done)\n",
        "    print(\"max reward:\", max_reward)\n",
        "    status, _ = episode_status(steps_done, max_reward)\n",
        "    print(\"result:\", status, '\\n')\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "    return steps_done, max_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qW_WqgyDPkT"
      },
      "source": [
        "# Your Implementation\n",
        "\n",
        "You must complete the following functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXdyG9ZRfnu"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3SKJfm-Ek6Z"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    ### Create all the nodes in the computation graph.\n",
        "    ### We won't say how to put the nodes together into a computation graph. That is done\n",
        "    ### automatically when forward() is called.\n",
        "    def __init__(self, h, w, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.num_actions = num_actions\n",
        "        ### WRITE YOUR CODE BELOW HERE\n",
        "        self.conv1 = torch.nn.Conv2d(3, 10, 5)\n",
        "        self.batchnorm1 = torch.nn.BatchNorm2d(10)\n",
        "        self.leaky_relu = torch.nn.LeakyReLU(0.01)\n",
        "        self.conv2 = torch.nn.Conv2d(10, 30, 5)\n",
        "        self.batchnorm2 = torch.nn.BatchNorm2d(30)\n",
        "        self.linear = torch.nn.Linear(30 * (h - 8) * (w - 8), self.num_actions)\n",
        "        ### WRITE YOUR CODE ABOVE HERE\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        q_values = None\n",
        "        ### WRITE YOUR CODE BELOW HERE\n",
        "        conv1_out = self.conv1(x)\n",
        "        #print(\"conv1_out: \" + str(conv1_out.shape))\n",
        "        batchnorm1_out = self.batchnorm1(conv1_out)\n",
        "        #print(\"batchnorm1_out: \" + str(batchnorm1_out.shape))\n",
        "        activ1_out = self.leaky_relu(batchnorm1_out)\n",
        "        #print(\"activ1_out: \" + str(activ1_out.shape))\n",
        "        conv2_out = self.conv2(activ1_out)\n",
        "        #print(\"conv2_out: \" + str(conv2_out.shape))\n",
        "        batchnorm2_out = self.batchnorm2(conv2_out)\n",
        "        #print(\"batchnorm2_out: \" + str(batchnorm2_out.shape))\n",
        "        activ2_out = self.leaky_relu(batchnorm2_out)\n",
        "        #print(\"activ2_out: \" + str(activ2_out.shape))\n",
        "        activ2_out = activ2_out.view(activ2_out.size(0), -1)\n",
        "        #print(\"activ2_out: \" + str(activ2_out.shape))\n",
        "        linear_out = self.linear(activ2_out)\n",
        "        q_values = linear_out\n",
        "        ### WRITE YOUR CODE ABOVE HERE\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNKfO3pQRlrx"
      },
      "source": [
        "## get_screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3RlDdQigCx0"
      },
      "outputs": [],
      "source": [
        "### Take the environment and return a tensor containing screen data as a 3D tensor containing (color, height, width) information.\n",
        "### Optional: the screen may be manipulated, for example, it could be cropped\n",
        "def get_screen(env, save = False):\n",
        "    # Returned screen requested by gym is 512x512x3. Transpose it into torch order (Color, Height, Width).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    ### DO ANY SCREEN MANIPULATIONS NECESSARY (IF ANY)\n",
        "\n",
        "    ### END SCREEN MANIPULATIONS\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    if save:\n",
        "      save_screen(screen)\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospeXs0qRppd"
      },
      "source": [
        "## ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqLyc5EggFeT"
      },
      "outputs": [],
      "source": [
        "### Store transitions to use to prevent catastrophic forgetting.\n",
        "### ReplayMemory implements a ring buffer. Items are placed into memory\n",
        "###    until memory reaches capacity, and then new items start replacing old items\n",
        "###    at the beginning of the array. \n",
        "### Member variables:\n",
        "###    capacity: (int) number of transitions that can be stored\n",
        "###    memory: (array) holds transitions (state, action, next_state, reward)\n",
        "###    position: (int) index of current location in memory to place the next transition.\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    ### Store a transition in memory.\n",
        "    ### To implement: put new items at the end of the memory array, unless capacity is reached.\n",
        "    ###    Combine the arguments into a new Transition object.\n",
        "    ###    If capacity is reached, start overwriting the beginning of the array.\n",
        "    ###    Use the position index to keep track of where to put the next item. \n",
        "    def push(self, state, action, next_state, reward):\n",
        "        ### WRITE YOUR CODE BELOW HERE\n",
        "        if len(self.memory) < self.capacity:\n",
        "          self.memory.append(Transition(state, action, next_state, reward))\n",
        "          self.position += 1\n",
        "        elif len(self.memory) == self.capacity:\n",
        "          if self.position == self.capacity:\n",
        "            self.position = 0\n",
        "          self.memory[self.position] = Transition(state, action, next_state, reward)\n",
        "          self.position += 1\n",
        "        ### WRITE YOUR CODE ABOVE HERE\n",
        "\n",
        "    ### Return a batch of transition objects from memory containing batch_size elements.\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    ### This allows one to call len() on a ReplayMemory object. E.g. len(replay_memory)\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xskf9M0_RurS"
      },
      "source": [
        "## initializeOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C17xRYCRrKc"
      },
      "outputs": [],
      "source": [
        "### Choose and instantiate an optimizer. A default example is given, which you can change.\n",
        "### Input:\n",
        "### - parameters: the DQN parameters\n",
        "### Output:\n",
        "### - the optimizer object\n",
        "def initializeOptimizer(parameters):\n",
        "    optimizer = None\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    #optimizer = optim.SGD(parameters, lr=0.01)\n",
        "    optimizer = optim.Adam(parameters, lr = 0.001)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVVcvZO8RzAC"
      },
      "source": [
        "## select_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fje2XRRYgYUp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "### Select an action to perform. \n",
        "### If a random number [0..1] is greater than epsilon, then query the policy_network,\n",
        "### otherwise use a random action.\n",
        "### Inputs:\n",
        "### - state: a tensor of shape 3 x screen_height x screen_width\n",
        "### - policy_net: a DQN object\n",
        "### - num_actions: number of actions available\n",
        "### - epsilon: float [0..1] indicating whether to choose random or use the network\n",
        "### - steps_done: number of previously executed steps\n",
        "### - bootstrap_threshold: number of steps that must be executed before training begins\n",
        "### This function should return:\n",
        "### - A tensor of shape 1 x 1 that contains the number of the action to execute\n",
        "### - The new epsilon value to use next time\n",
        "def select_action(state, policy_net, num_actions, epsilon, steps_done = 0, bootstrap_threshold = 0):\n",
        "    action = None\n",
        "    new_epsilon = epsilon\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    rand_num = random.uniform(0, 1)\n",
        "    if rand_num > epsilon:\n",
        "      action = policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "      action = torch.tensor([[random.randrange(num_actions)]], device=DEVICE, dtype=torch.long)\n",
        "    if steps_done > bootstrap_threshold:\n",
        "      new_epsilon = max(0.9 * math.exp(-1.0 * (steps_done - bootstrap_threshold) / 100000), 0.1)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return action, new_epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgKYPlpJR5yt"
      },
      "source": [
        "## doMakeBatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjnp4s9RgbxZ"
      },
      "outputs": [],
      "source": [
        "### Ask for a batch of experience replays.\n",
        "### Inputs:\n",
        "### - replay_memory: A ReplayMemory object\n",
        "### - batch_size: size of the batch to return\n",
        "### Outputs:\n",
        "### - states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width\n",
        "### - actions_batch: a tensor of shape batch_size x 1 containing action numbers\n",
        "### - next_states_batch: a tensor containing screens. \n",
        "### - rewards_batch: a tensor of shape batch_size x 1 containing reward values.\n",
        "### - non_final_mask: a tensor of bytes of length batch_size containing a 0 if the state is terminal or 1 otherwise\n",
        "def doMakeBatch(replay_memory, batch_size):\n",
        "    states_batch = None\n",
        "    actions_batch = None\n",
        "    next_states_batch = None\n",
        "    rewards_batch = None\n",
        "    non_final_mask = None\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    sample = Transition(*zip(*replay_memory.sample(batch_size)))\n",
        "    states_batch = torch.cat(sample.state)\n",
        "    actions_batch = torch.cat(sample.action)\n",
        "    next_states_batch = torch.cat([state for state in sample.next_state if state is not None])\n",
        "    rewards_batch = torch.cat(sample.reward)\n",
        "    non_final_mask = torch.tensor((tuple(map(lambda x: x is not None, sample.next_state))), device=DEVICE, dtype=(torch.bool))\n",
        "    #print(non_final_mask)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask\n",
        "\n",
        "    #sample = Transition(*zip(*replay_memory.sample(batch_size))) \n",
        "    #state batch - concatenate all elements in sample.state - torch.cat(sample.state)\n",
        "    # same for actions and rewards\n",
        "    # next_states_batch - concatenate all next states that are not none (sample.next_states)\n",
        "    # non_final_mask - 0 when state is none and 1 when the state is not none....append 1 or 0 to list and then convert to tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuhTG2aZR8wJ"
      },
      "source": [
        "## doPredictQValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLulwgrAgcjB"
      },
      "outputs": [],
      "source": [
        "### Ask the policy_net to predict the Q value for a batch of states and a batch of actions.\n",
        "### Inputs:\n",
        "### - policy_net: the DQN\n",
        "### - states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width containing screens\n",
        "### - actions_batch: a tensor of shape batch_size x 1 containing action numbers\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1 containing the Q-value predicted by the DQN in the position indicated by the action\n",
        "def doPredictQValues(policy_net, states_batch, actions_batch):\n",
        "    state_action_values = None\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    state_action_values = policy_net(states_batch)\n",
        "    state_action_values = state_action_values.gather(1, actions_batch)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return state_action_values\n",
        "\n",
        "    #call policy network on given states batch\n",
        "    #gather that based on action_batch (axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVJ5HgVQSNgq"
      },
      "source": [
        "## doPredictNextStateUtilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em2AfgD1gecz"
      },
      "outputs": [],
      "source": [
        "### Ask the policy_net to predict the utility of a next_state.\n",
        "### Inputs:\n",
        "### - policy_net: The DQN\n",
        "### - next_states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width\n",
        "### - non_final_mask: a tensor of length batch_size containing 0 for terminal states and 1 for non-terminal states\n",
        "### - batch_size: the batch size\n",
        "### Note: Only run non-terminal states through the policy_net\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1 containing Q-values\n",
        "def doPredictNextStateUtilities(policy_net, next_states_batch, non_final_mask, batch_size):\n",
        "    next_state_values = torch.zeros(batch_size, device=DEVICE)\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    next_state_values[non_final_mask] = policy_net(next_states_batch).max(1)[0]\n",
        "    next_state_values = next_state_values.unsqueeze(1)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return next_state_values.detach()\n",
        "\n",
        "    #call the policy net on the next_states batch\n",
        "    # for each state, provide the policy's result\n",
        "    # using the mask, set next_state_values[non_final_mask] = policy's best policy (best/max policy across each column -summed) and then unsqueeze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9605CTaSQhR"
      },
      "source": [
        "## doComputeExpectedQValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES5pukXuggdX"
      },
      "outputs": [],
      "source": [
        "### Compute the Q-update equation Q(s_t, a_t) = R(s_t+1) + gamma * argmax_a' Q(s_t+1, a')\n",
        "### Inputs:\n",
        "### - next_state_values: a tensor of shape batch_size x 1 containing Q values for state s_t+1\n",
        "### - rewards_batch: a tensor or shape batch_size x 1 containing reward values for state s_t+1\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1\n",
        "def doComputeExpectedQValues(next_state_values, rewards_batch, gamma):\n",
        "    expected_state_action_values = None\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    expected_state_action_values = torch.add(rewards_batch, torch.mul(gamma, next_state_values))\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return expected_state_action_values\n",
        "\n",
        "    #gamma * next_state values + rewards_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3l9RwRlSS-b"
      },
      "source": [
        "## doComputeLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeePwmIDgja-"
      },
      "outputs": [],
      "source": [
        "### Compute the loss\n",
        "### Inputs:\n",
        "### - state_action_values: a tensor of shape batch_size x 1 containing Q values\n",
        "### - expected_state_action_values: a tensor of shape batch_size x 1 containing updated Q values\n",
        "### Output:\n",
        "### - A tensor scalar value\n",
        "def doComputeLoss(state_action_values, expected_state_action_values):\n",
        "    loss = None\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    loss = torch.nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "    return loss\n",
        "\n",
        "    #call nn.functional.smooth_l1_loss(state_action_values, expected_state_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHsR5t-qSVBk"
      },
      "source": [
        "## doBackprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKoDTe_ChEWd"
      },
      "outputs": [],
      "source": [
        "### Run backpropagation. Make sure gradients are clipped between -1 and +1.\n",
        "### Inputs:\n",
        "### - loss: a tensor scalar\n",
        "### - parameters: the parameters of the DQN\n",
        "### There is no output\n",
        "def doBackprop(loss, parameters):\n",
        "    ### WRITE YOUR CODE BELOW HERE\n",
        "    loss.backward()\n",
        "    for param in parameters:\n",
        "      param.grad.data.clamp_(-1, 1)\n",
        "    ### WRITE YOUR CODE ABOVE HERE\n",
        "\n",
        "    #loss.backward()\n",
        "    #go thru each parameter and clip between -1 and 1, use stack overflow link: https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KivfSnMYHdT7"
      },
      "source": [
        "# Unit testing\n",
        "\n",
        "Run this cell to make sure your functions return valid outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNWu5e1zoc8v",
        "outputId": "e548b15b-3034-4b2b-fb01-29fed833f09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing ReplayMemory...\n",
            "ReplayMemory test passed.\n",
            "Testing doMakeBatch...\n",
            "doMakeBatch test passed.\n",
            "Testing select_action...\n",
            "select_action test passed.\n",
            "Testing doPredictQValues...\n",
            "doPredictQValues test passed.\n",
            "Testing doPredictNextStateUtilities...\n",
            "shape mismatch: value tensor of shape [128] cannot be broadcast to indexing result of shape [64]\n",
            "Will try alternative test.\n",
            "doPredictNextStateUtilities test passed.\n",
            "Testing doComputeExpectedQValues...\n",
            "doComputeExpectedQValues test passed.\n",
            "Testing doComputeLoss...\n",
            "doComputeLoss test passed.\n"
          ]
        }
      ],
      "source": [
        "### RUN UNIT TESTS ON FUNCTIONS\n",
        "unit_test() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5twAqf6HhT8"
      },
      "source": [
        "# Training\n",
        "\n",
        "Edit the globals as necessary.\n",
        "The best model produced during training will be saved to ```SAVE_FILENAME``` with a number appended to the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkAqkDanQa6"
      },
      "source": [
        "###### Train for EASY_LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVeaxxTLZEXp",
        "outputId": "2e0102d6-1d37-4cf9-a5be-392edf5fba14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "duration: 730\n",
            "max reward: [107.00568]\n",
            "result: coin\n",
            "total steps: 43387 \n",
            "\n",
            "episode: 57 epsilon: 0.6445386330969374\n",
            "duration: 312\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 43699 \n",
            "\n",
            "episode: 58 epsilon: 0.6425308064000709\n",
            "duration: 1002\n",
            "max reward: [3.172204]\n",
            "result: timeout\n",
            "total steps: 44701 \n",
            "\n",
            "episode: 59 epsilon: 0.6361247954319745\n",
            "duration: 633\n",
            "max reward: [107.08586]\n",
            "result: coin\n",
            "total steps: 45334 \n",
            "\n",
            "episode: 60 epsilon: 0.6321108430390927\n",
            "duration: 221\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 45555 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.22687292]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.05909991]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.49504018]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.30593014]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.20593023]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.05909991]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.19774604]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.36503005]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.20593023]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4969678]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.26176473]\n",
            " \n",
            "episode: 61 epsilon: 0.6307154205857358\n",
            "duration: 340\n",
            "max reward: [107.63667]\n",
            "result: coin\n",
            "total steps: 45895 \n",
            "\n",
            "episode: 62 epsilon: 0.6285746295627783\n",
            "duration: 706\n",
            "max reward: [107.50042]\n",
            "result: coin\n",
            "total steps: 46601 \n",
            "\n",
            "episode: 63 epsilon: 0.6241525210887803\n",
            "duration: 149\n",
            "max reward: [107.0435]\n",
            "result: coin\n",
            "total steps: 46750 \n",
            "\n",
            "episode: 64 epsilon: 0.6232232263288813\n",
            "duration: 383\n",
            "max reward: [107.74649]\n",
            "result: coin\n",
            "total steps: 47133 \n",
            "\n",
            "episode: 65 epsilon: 0.6208408465415746\n",
            "duration: 953\n",
            "max reward: [107.03261]\n",
            "result: coin\n",
            "total steps: 48086 \n",
            "\n",
            "episode: 66 epsilon: 0.6149523365905305\n",
            "duration: 1002\n",
            "max reward: [2.7207541]\n",
            "result: timeout\n",
            "total steps: 49088 \n",
            "\n",
            "episode: 67 epsilon: 0.6088212820577109\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 50090 \n",
            "\n",
            "episode: 68 epsilon: 0.6027513539365622\n",
            "duration: 985\n",
            "max reward: [107.37146]\n",
            "result: coin\n",
            "total steps: 51075 \n",
            "\n",
            "episode: 69 epsilon: 0.5968433975524597\n",
            "duration: 194\n",
            "max reward: [107.9409]\n",
            "result: coin\n",
            "total steps: 51269 \n",
            "\n",
            "episode: 70 epsilon: 0.5956866437751684\n",
            "duration: 183\n",
            "max reward: [106.943184]\n",
            "result: coin\n",
            "total steps: 51452 \n",
            "\n",
            "Evaluating...\n",
            "duration: 287\n",
            "max reward: [106.95377]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 187\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 103\n",
            "max reward: [107.00775]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 146\n",
            "max reward: [106.94284]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 170\n",
            "max reward: [107.01355]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [107.0475]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 98\n",
            "max reward: [106.90986]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 98\n",
            "max reward: [106.95621]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 158\n",
            "max reward: [106.968994]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 194\n",
            "max reward: [106.99701]\n",
            "result: coin \n",
            "\n",
            "Average duration: 150.7\n",
            "Average max reward: [107.079735]\n",
            " \n",
            "episode: 71 epsilon: 0.5945975340563958\n",
            "duration: 123\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 51575 \n",
            "\n",
            "episode: 72 epsilon: 0.5938666286884566\n",
            "duration: 243\n",
            "max reward: [107.22493]\n",
            "result: coin\n",
            "total steps: 51818 \n",
            "\n",
            "episode: 73 epsilon: 0.5924252847229111\n",
            "duration: 164\n",
            "max reward: [107.967094]\n",
            "result: coin\n",
            "total steps: 51982 \n",
            "\n",
            "episode: 74 epsilon: 0.5914545035141411\n",
            "duration: 114\n",
            "max reward: [107.64349]\n",
            "result: coin\n",
            "total steps: 52096 \n",
            "\n",
            "episode: 75 epsilon: 0.5907806295612686\n",
            "duration: 281\n",
            "max reward: [107.43805]\n",
            "result: coin\n",
            "total steps: 52377 \n",
            "\n",
            "episode: 76 epsilon: 0.5891228662404892\n",
            "duration: 115\n",
            "max reward: [107.02614]\n",
            "result: coin\n",
            "total steps: 52492 \n",
            "\n",
            "episode: 77 epsilon: 0.5884457643525204\n",
            "duration: 190\n",
            "max reward: [107.04506]\n",
            "result: coin\n",
            "total steps: 52682 \n",
            "\n",
            "episode: 78 epsilon: 0.5873287788724831\n",
            "duration: 122\n",
            "max reward: [107.88873]\n",
            "result: coin\n",
            "total steps: 52804 \n",
            "\n",
            "episode: 79 epsilon: 0.5866126746746402\n",
            "duration: 132\n",
            "max reward: [107.074326]\n",
            "result: coin\n",
            "total steps: 52936 \n",
            "\n",
            "episode: 80 epsilon: 0.585838856776241\n",
            "duration: 57\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 52993 \n",
            "\n",
            "Evaluating...\n",
            "duration: 162\n",
            "max reward: [107.102104]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 119\n",
            "max reward: [107.073265]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 212\n",
            "max reward: [107.14842]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [107.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.2911339]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 109\n",
            "max reward: [107.08468]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [107.1027]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 961\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 752\n",
            "max reward: [107.00674]\n",
            "result: coin \n",
            "\n",
            "Average duration: 345.2\n",
            "Average max reward: [96.977905]\n",
            " \n",
            "episode: 81 epsilon: 0.5855050237793211\n",
            "duration: 87\n",
            "max reward: [107.36736]\n",
            "result: coin\n",
            "total steps: 53080 \n",
            "\n",
            "episode: 82 epsilon: 0.5849958559287639\n",
            "duration: 165\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 53245 \n",
            "\n",
            "episode: 83 epsilon: 0.5840314086542918\n",
            "duration: 220\n",
            "max reward: [107.06045]\n",
            "result: coin\n",
            "total steps: 53465 \n",
            "\n",
            "episode: 84 epsilon: 0.5827479518753701\n",
            "duration: 154\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 53619 \n",
            "\n",
            "episode: 85 epsilon: 0.5818512106974149\n",
            "duration: 55\n",
            "max reward: [107.58014]\n",
            "result: coin\n",
            "total steps: 53674 \n",
            "\n",
            "episode: 86 epsilon: 0.581531280520395\n",
            "duration: 70\n",
            "max reward: [107.912674]\n",
            "result: coin\n",
            "total steps: 53744 \n",
            "\n",
            "episode: 87 epsilon: 0.5811243510659561\n",
            "duration: 51\n",
            "max reward: [107.4854]\n",
            "result: coin\n",
            "total steps: 53795 \n",
            "\n",
            "episode: 88 epsilon: 0.5808280532092881\n",
            "duration: 94\n",
            "max reward: [107.16236]\n",
            "result: coin\n",
            "total steps: 53889 \n",
            "\n",
            "episode: 89 epsilon: 0.5802823313687198\n",
            "duration: 79\n",
            "max reward: [107.40912]\n",
            "result: coin\n",
            "total steps: 53968 \n",
            "\n",
            "episode: 90 epsilon: 0.5798240893563658\n",
            "duration: 100\n",
            "max reward: [106.969154]\n",
            "result: coin\n",
            "total steps: 54068 \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.04688]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [107.1203]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [107.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.661705]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 72\n",
            "max reward: [107.67095]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 89\n",
            "max reward: [107.00742]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [107.084816]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.67095]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 98\n",
            "max reward: [107.67095]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.52969]\n",
            "result: coin \n",
            "\n",
            "Average duration: 51.3\n",
            "Average max reward: [107.443375]\n",
            "Saving easy.model.90 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 91 epsilon: 0.5792445550824409\n",
            "duration: 94\n",
            "max reward: [107.858734]\n",
            "result: coin\n",
            "total steps: 54162 \n",
            "\n",
            "episode: 92 epsilon: 0.5787003210307414\n",
            "duration: 86\n",
            "max reward: [107.45294]\n",
            "result: coin\n",
            "total steps: 54248 \n",
            "\n",
            "episode: 93 epsilon: 0.5782028526966994\n",
            "duration: 56\n",
            "max reward: [107.254616]\n",
            "result: coin\n",
            "total steps: 54304 \n",
            "\n",
            "episode: 94 epsilon: 0.5778791497444752\n",
            "duration: 119\n",
            "max reward: [107.910904]\n",
            "result: coin\n",
            "total steps: 54423 \n",
            "\n",
            "episode: 95 epsilon: 0.5771918825613565\n",
            "duration: 77\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 54500 \n",
            "\n",
            "episode: 96 epsilon: 0.5767476158764084\n",
            "duration: 54\n",
            "max reward: [107.9409]\n",
            "result: coin\n",
            "total steps: 54554 \n",
            "\n",
            "episode: 97 epsilon: 0.5764362562385034\n",
            "duration: 79\n",
            "max reward: [107.82358]\n",
            "result: coin\n",
            "total steps: 54633 \n",
            "\n",
            "episode: 98 epsilon: 0.5759810514256506\n",
            "duration: 74\n",
            "max reward: [107.83438]\n",
            "result: coin\n",
            "total steps: 54707 \n",
            "\n",
            "episode: 99 epsilon: 0.5755549831123143\n",
            "duration: 53\n",
            "max reward: [107.83642]\n",
            "result: coin\n",
            "total steps: 54760 \n",
            "\n",
            "episode: 100 epsilon: 0.575250019793683\n",
            "duration: 54\n",
            "max reward: [107.996254]\n",
            "result: coin\n",
            "total steps: 54814 \n",
            "\n",
            "Evaluating...\n",
            "duration: 421\n",
            "max reward: [106.980995]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 218\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 320\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 275\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 901\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 438\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 177\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 172\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 333\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 425.7\n",
            "Average max reward: [97.398094]\n",
            " \n",
            "episode: 101 epsilon: 0.5749394686393525\n",
            "duration: 66\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 54880 \n",
            "\n",
            "episode: 102 epsilon: 0.5745601337843225\n",
            "duration: 99\n",
            "max reward: [107.57788]\n",
            "result: coin\n",
            "total steps: 54979 \n",
            "\n",
            "episode: 103 epsilon: 0.5739916007221767\n",
            "duration: 92\n",
            "max reward: [107.04338]\n",
            "result: coin\n",
            "total steps: 55071 \n",
            "\n",
            "episode: 104 epsilon: 0.5734637712882814\n",
            "duration: 171\n",
            "max reward: [107.54184]\n",
            "result: coin\n",
            "total steps: 55242 \n",
            "\n",
            "episode: 105 epsilon: 0.5724839861943829\n",
            "duration: 185\n",
            "max reward: [107.01198]\n",
            "result: coin\n",
            "total steps: 55427 \n",
            "\n",
            "episode: 106 epsilon: 0.5714258698792983\n",
            "duration: 106\n",
            "max reward: [107.71962]\n",
            "result: coin\n",
            "total steps: 55533 \n",
            "\n",
            "episode: 107 epsilon: 0.5708204793708804\n",
            "duration: 100\n",
            "max reward: [106.91529]\n",
            "result: coin\n",
            "total steps: 55633 \n",
            "\n",
            "episode: 108 epsilon: 0.5702499442066363\n",
            "duration: 55\n",
            "max reward: [107.087105]\n",
            "result: coin\n",
            "total steps: 55688 \n",
            "\n",
            "episode: 109 epsilon: 0.5699363929718163\n",
            "duration: 65\n",
            "max reward: [107.02933]\n",
            "result: coin\n",
            "total steps: 55753 \n",
            "\n",
            "episode: 110 epsilon: 0.5695660546893655\n",
            "duration: 129\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 55882 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7589464]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3229365]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.93692374]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0348759]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8488197]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.0080001]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.813004]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1594374]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6089194]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8683512]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.6360214]\n",
            " \n",
            "episode: 111 epsilon: 0.5688317881825374\n",
            "duration: 120\n",
            "max reward: [107.50686]\n",
            "result: coin\n",
            "total steps: 56002 \n",
            "\n",
            "episode: 112 epsilon: 0.5681495994318315\n",
            "duration: 54\n",
            "max reward: [107.744026]\n",
            "result: coin\n",
            "total steps: 56056 \n",
            "\n",
            "episode: 113 epsilon: 0.5678428814694414\n",
            "duration: 141\n",
            "max reward: [107.2438]\n",
            "result: coin\n",
            "total steps: 56197 \n",
            "\n",
            "episode: 114 epsilon: 0.5670427872055811\n",
            "duration: 106\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 56303 \n",
            "\n",
            "episode: 115 epsilon: 0.5664420403032514\n",
            "duration: 90\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 56393 \n",
            "\n",
            "episode: 116 epsilon: 0.5659324718071975\n",
            "duration: 107\n",
            "max reward: [107.39931]\n",
            "result: coin\n",
            "total steps: 56500 \n",
            "\n",
            "episode: 117 epsilon: 0.5653272479148896\n",
            "duration: 176\n",
            "max reward: [107.36471]\n",
            "result: coin\n",
            "total steps: 56676 \n",
            "\n",
            "episode: 118 epsilon: 0.5643331470239541\n",
            "duration: 136\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 56812 \n",
            "\n",
            "episode: 119 epsilon: 0.5635661756027838\n",
            "duration: 56\n",
            "max reward: [107.03132]\n",
            "result: coin\n",
            "total steps: 56868 \n",
            "\n",
            "episode: 120 epsilon: 0.5632506668951296\n",
            "duration: 82\n",
            "max reward: [107.992516]\n",
            "result: coin\n",
            "total steps: 56950 \n",
            "\n",
            "Evaluating...\n",
            "duration: 172\n",
            "max reward: [106.78284]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 148\n",
            "max reward: [106.84482]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 451\n",
            "max reward: [106.80988]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 348\n",
            "max reward: [106.87032]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 241\n",
            "max reward: [106.89845]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 80\n",
            "max reward: [106.966866]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 595\n",
            "max reward: [106.81715]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 346\n",
            "max reward: [107.091995]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 134\n",
            "max reward: [107.747055]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 142\n",
            "max reward: [107.09327]\n",
            "result: coin \n",
            "\n",
            "Average duration: 265.7\n",
            "Average max reward: [106.99226]\n",
            " \n",
            "episode: 121 epsilon: 0.5627889906614006\n",
            "duration: 115\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 57065 \n",
            "\n",
            "episode: 122 epsilon: 0.5621421553237458\n",
            "duration: 78\n",
            "max reward: [107.07121]\n",
            "result: coin\n",
            "total steps: 57143 \n",
            "\n",
            "episode: 123 epsilon: 0.5617038554017847\n",
            "duration: 135\n",
            "max reward: [107.3947]\n",
            "result: coin\n",
            "total steps: 57278 \n",
            "\n",
            "episode: 124 epsilon: 0.5609460668193745\n",
            "duration: 152\n",
            "max reward: [107.68493]\n",
            "result: coin\n",
            "total steps: 57430 \n",
            "\n",
            "episode: 125 epsilon: 0.5600940764745078\n",
            "duration: 131\n",
            "max reward: [107.04119]\n",
            "result: coin\n",
            "total steps: 57561 \n",
            "\n",
            "episode: 126 epsilon: 0.5593608336132601\n",
            "duration: 76\n",
            "max reward: [107.04602]\n",
            "result: coin\n",
            "total steps: 57637 \n",
            "\n",
            "episode: 127 epsilon: 0.5589358808822062\n",
            "duration: 137\n",
            "max reward: [107.08773]\n",
            "result: coin\n",
            "total steps: 57774 \n",
            "\n",
            "episode: 128 epsilon: 0.5581706630193201\n",
            "duration: 162\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 57936 \n",
            "\n",
            "episode: 129 epsilon: 0.55726715858142\n",
            "duration: 38\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 57974 \n",
            "\n",
            "episode: 130 epsilon: 0.5570554372907519\n",
            "duration: 136\n",
            "max reward: [107.49715]\n",
            "result: coin\n",
            "total steps: 58110 \n",
            "\n",
            "Evaluating...\n",
            "duration: 42\n",
            "max reward: [106.9905]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 65\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [107.13231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.13231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 91\n",
            "max reward: [107.13231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 49\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [107.0157]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 83\n",
            "max reward: [107.13231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [106.869484]\n",
            "result: coin \n",
            "\n",
            "Average duration: 55.1\n",
            "Average max reward: [107.25673]\n",
            " \n",
            "episode: 131 epsilon: 0.5562983568274429\n",
            "duration: 52\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 58162 \n",
            "\n",
            "episode: 132 epsilon: 0.5560091568803954\n",
            "duration: 430\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 58592 \n",
            "\n",
            "episode: 133 epsilon: 0.5536234504506087\n",
            "duration: 118\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 58710 \n",
            "\n",
            "episode: 134 epsilon: 0.5529705600601644\n",
            "duration: 214\n",
            "max reward: [107.735054]\n",
            "result: coin\n",
            "total steps: 58924 \n",
            "\n",
            "episode: 135 epsilon: 0.55178846835089\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 59926 \n",
            "\n",
            "episode: 136 epsilon: 0.5462871555031386\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 60928 \n",
            "\n",
            "episode: 137 epsilon: 0.5408406905632082\n",
            "duration: 1002\n",
            "max reward: [5.0313168]\n",
            "result: timeout\n",
            "total steps: 61930 \n",
            "\n",
            "episode: 138 epsilon: 0.535448526699265\n",
            "duration: 1002\n",
            "max reward: [5.200308]\n",
            "result: timeout\n",
            "total steps: 62932 \n",
            "\n",
            "episode: 139 epsilon: 0.530110122531371\n",
            "duration: 1002\n",
            "max reward: [6.7492275]\n",
            "result: timeout\n",
            "total steps: 63934 \n",
            "\n",
            "episode: 140 epsilon: 0.5248249420771277\n",
            "duration: 1002\n",
            "max reward: [5.072422]\n",
            "result: timeout\n",
            "total steps: 64936 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.0445335]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.055757]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.0774288]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9026685]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.2855911]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4818537]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.8847833]\n",
            " \n",
            "episode: 141 epsilon: 0.519592454697864\n",
            "duration: 1002\n",
            "max reward: [4.0129194]\n",
            "result: timeout\n",
            "total steps: 65938 \n",
            "\n",
            "episode: 142 epsilon: 0.5144121350453582\n",
            "duration: 755\n",
            "max reward: [107.20665]\n",
            "result: coin\n",
            "total steps: 66693 \n",
            "\n",
            "episode: 143 epsilon: 0.5105429479863408\n",
            "duration: 1002\n",
            "max reward: [3.9876099]\n",
            "result: timeout\n",
            "total steps: 67695 \n",
            "\n",
            "episode: 144 epsilon: 0.5054528516175633\n",
            "duration: 1002\n",
            "max reward: [3.44586]\n",
            "result: timeout\n",
            "total steps: 68697 \n",
            "\n",
            "episode: 145 epsilon: 0.5004135033418612\n",
            "duration: 1002\n",
            "max reward: [2.6186948]\n",
            "result: timeout\n",
            "total steps: 69699 \n",
            "\n",
            "episode: 146 epsilon: 0.4954243972024189\n",
            "duration: 614\n",
            "max reward: [107.211716]\n",
            "result: coin\n",
            "total steps: 70313 \n",
            "\n",
            "episode: 147 epsilon: 0.4923918109705957\n",
            "duration: 800\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 71113 \n",
            "\n",
            "episode: 148 epsilon: 0.488468391087248\n",
            "duration: 1002\n",
            "max reward: [3.1526318]\n",
            "result: timeout\n",
            "total steps: 72115 \n",
            "\n",
            "episode: 149 epsilon: 0.48359837732338673\n",
            "duration: 430\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 72545 \n",
            "\n",
            "episode: 150 epsilon: 0.4815233687665348\n",
            "duration: 775\n",
            "max reward: [106.98994]\n",
            "result: coin\n",
            "total steps: 73320 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.70888495]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7555351]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.54530764]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8411422]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.9366064]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.72597647]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7555351]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7555351]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.67568374]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8834138]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.75836205]\n",
            " \n",
            "episode: 151 epsilon: 0.4778059861225953\n",
            "duration: 1002\n",
            "max reward: [4.076582]\n",
            "result: timeout\n",
            "total steps: 74322 \n",
            "\n",
            "episode: 152 epsilon: 0.4730422761849\n",
            "duration: 106\n",
            "max reward: [107.58099]\n",
            "result: coin\n",
            "total steps: 74428 \n",
            "\n",
            "episode: 153 epsilon: 0.47254111703341956\n",
            "duration: 453\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 74881 \n",
            "\n",
            "episode: 154 epsilon: 0.47040534694483455\n",
            "duration: 131\n",
            "max reward: [107.99892]\n",
            "result: coin\n",
            "total steps: 75012 \n",
            "\n",
            "episode: 155 epsilon: 0.4697895193954502\n",
            "duration: 82\n",
            "max reward: [107.88519]\n",
            "result: coin\n",
            "total steps: 75094 \n",
            "\n",
            "episode: 156 epsilon: 0.46940444988962\n",
            "duration: 1002\n",
            "max reward: [2.0949135]\n",
            "result: timeout\n",
            "total steps: 76096 \n",
            "\n",
            "episode: 157 epsilon: 0.4647245029913326\n",
            "duration: 1002\n",
            "max reward: [4.045648]\n",
            "result: timeout\n",
            "total steps: 77098 \n",
            "\n",
            "episode: 158 epsilon: 0.4600912150094145\n",
            "duration: 1002\n",
            "max reward: [3.1861243]\n",
            "result: timeout\n",
            "total steps: 78100 \n",
            "\n",
            "episode: 159 epsilon: 0.4555041207560071\n",
            "duration: 207\n",
            "max reward: [107.62153]\n",
            "result: coin\n",
            "total steps: 78307 \n",
            "\n",
            "episode: 160 epsilon: 0.4545622024478265\n",
            "duration: 462\n",
            "max reward: [106.97999]\n",
            "result: coin\n",
            "total steps: 78769 \n",
            "\n",
            "Evaluating...\n",
            "duration: 115\n",
            "max reward: [107.02416]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.5204487]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.5225282]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 777\n",
            "max reward: [106.98941]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 984\n",
            "max reward: [107.15697]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 947\n",
            "max reward: [106.97554]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 256\n",
            "max reward: [107.06594]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 455\n",
            "max reward: [107.766106]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 191\n",
            "max reward: [107.28449]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 564\n",
            "max reward: [107.374275]\n",
            "result: coin \n",
            "\n",
            "Average duration: 629.3\n",
            "Average max reward: [86.46798]\n",
            " \n",
            "episode: 161 epsilon: 0.4524669687890602\n",
            "duration: 1002\n",
            "max reward: [4.4459443]\n",
            "result: timeout\n",
            "total steps: 79771 \n",
            "\n",
            "episode: 162 epsilon: 0.44795588801924685\n",
            "duration: 739\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 80510 \n",
            "\n",
            "episode: 163 epsilon: 0.4446576958369845\n",
            "duration: 1002\n",
            "max reward: [4.1756573]\n",
            "result: timeout\n",
            "total steps: 81512 \n",
            "\n",
            "episode: 164 epsilon: 0.4402244732611839\n",
            "duration: 1002\n",
            "max reward: [4.3016095]\n",
            "result: timeout\n",
            "total steps: 82514 \n",
            "\n",
            "episode: 165 epsilon: 0.4358354497683872\n",
            "duration: 469\n",
            "max reward: [107.910904]\n",
            "result: coin\n",
            "total steps: 82983 \n",
            "\n",
            "episode: 166 epsilon: 0.43379616736423143\n",
            "duration: 339\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 83322 \n",
            "\n",
            "episode: 167 epsilon: 0.43232808815706536\n",
            "duration: 1002\n",
            "max reward: [3.2749734]\n",
            "result: timeout\n",
            "total steps: 84324 \n",
            "\n",
            "episode: 168 epsilon: 0.4280177913635669\n",
            "duration: 109\n",
            "max reward: [107.01481]\n",
            "result: coin\n",
            "total steps: 84433 \n",
            "\n",
            "episode: 169 epsilon: 0.42755150614259213\n",
            "duration: 131\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 84564 \n",
            "\n",
            "episode: 170 epsilon: 0.4269917803699718\n",
            "duration: 198\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 84762 \n",
            "\n",
            "Evaluating...\n",
            "duration: 103\n",
            "max reward: [107.11872]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [106.90832]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [106.78626]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [107.1646]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 59\n",
            "max reward: [107.21097]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [107.24538]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 74\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.03163]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 62\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Average duration: 48.0\n",
            "Average max reward: [107.17904]\n",
            "Saving easy.model.170 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 171 epsilon: 0.4261471730819876\n",
            "duration: 105\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 84867 \n",
            "\n",
            "episode: 172 epsilon: 0.4256999533816824\n",
            "duration: 132\n",
            "max reward: [107.08396]\n",
            "result: coin\n",
            "total steps: 84999 \n",
            "\n",
            "episode: 173 epsilon: 0.42513840014988913\n",
            "duration: 400\n",
            "max reward: [106.881516]\n",
            "result: coin\n",
            "total steps: 85399 \n",
            "\n",
            "episode: 174 epsilon: 0.42344124312621234\n",
            "duration: 149\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 85548 \n",
            "\n",
            "episode: 175 epsilon: 0.4228107854815395\n",
            "duration: 172\n",
            "max reward: [106.97244]\n",
            "result: coin\n",
            "total steps: 85720 \n",
            "\n",
            "episode: 176 epsilon: 0.42208417599380416\n",
            "duration: 1002\n",
            "max reward: [3.349997]\n",
            "result: timeout\n",
            "total steps: 86722 \n",
            "\n",
            "episode: 177 epsilon: 0.4178760106670313\n",
            "duration: 235\n",
            "max reward: [107.974396]\n",
            "result: coin\n",
            "total steps: 86957 \n",
            "\n",
            "episode: 178 epsilon: 0.4168951549987719\n",
            "duration: 62\n",
            "max reward: [107.19105]\n",
            "result: coin\n",
            "total steps: 87019 \n",
            "\n",
            "episode: 179 epsilon: 0.41663676011336437\n",
            "duration: 54\n",
            "max reward: [107.860504]\n",
            "result: coin\n",
            "total steps: 87073 \n",
            "\n",
            "episode: 180 epsilon: 0.41641183699761003\n",
            "duration: 192\n",
            "max reward: [107.38676]\n",
            "result: coin\n",
            "total steps: 87265 \n",
            "\n",
            "Evaluating...\n",
            "duration: 133\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [107.051315]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [107.002304]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [106.99347]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [106.97979]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 78\n",
            "max reward: [107.04445]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [107.02776]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 77\n",
            "max reward: [107.97521]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 82\n",
            "max reward: [107.133575]\n",
            "result: coin \n",
            "\n",
            "Average duration: 59.8\n",
            "Average max reward: [107.153244]\n",
            " \n",
            "episode: 181 epsilon: 0.4156130933098889\n",
            "duration: 444\n",
            "max reward: [107.883575]\n",
            "result: coin\n",
            "total steps: 87709 \n",
            "\n",
            "episode: 182 epsilon: 0.4137718617344647\n",
            "duration: 221\n",
            "max reward: [107.04967]\n",
            "result: coin\n",
            "total steps: 87930 \n",
            "\n",
            "episode: 183 epsilon: 0.4128584356276515\n",
            "duration: 217\n",
            "max reward: [107.920456]\n",
            "result: coin\n",
            "total steps: 88147 \n",
            "\n",
            "episode: 184 epsilon: 0.4119635041741451\n",
            "duration: 246\n",
            "max reward: [107.74387]\n",
            "result: coin\n",
            "total steps: 88393 \n",
            "\n",
            "episode: 185 epsilon: 0.41095131945153024\n",
            "duration: 88\n",
            "max reward: [106.923004]\n",
            "result: coin\n",
            "total steps: 88481 \n",
            "\n",
            "episode: 186 epsilon: 0.4105898413640987\n",
            "duration: 1002\n",
            "max reward: [5.2694626]\n",
            "result: timeout\n",
            "total steps: 89483 \n",
            "\n",
            "episode: 187 epsilon: 0.40649627417483997\n",
            "duration: 1002\n",
            "max reward: [2.9975262]\n",
            "result: timeout\n",
            "total steps: 90485 \n",
            "\n",
            "episode: 188 epsilon: 0.40244351971557296\n",
            "duration: 1002\n",
            "max reward: [5.449609]\n",
            "result: timeout\n",
            "total steps: 91487 \n",
            "\n",
            "episode: 189 epsilon: 0.3984311710847246\n",
            "duration: 817\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 92304 \n",
            "\n",
            "episode: 190 epsilon: 0.3951892496986948\n",
            "duration: 174\n",
            "max reward: [107.85529]\n",
            "result: coin\n",
            "total steps: 92478 \n",
            "\n",
            "Evaluating...\n",
            "duration: 63\n",
            "max reward: [106.93531]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 130\n",
            "max reward: [107.5676]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 88\n",
            "max reward: [107.06799]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 194\n",
            "max reward: [106.90493]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 82\n",
            "max reward: [106.94912]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 120\n",
            "max reward: [107.06842]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 143\n",
            "max reward: [107.79701]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 67\n",
            "max reward: [106.905304]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 51\n",
            "max reward: [107.01597]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 103\n",
            "max reward: [107.64593]\n",
            "result: coin \n",
            "\n",
            "Average duration: 104.1\n",
            "Average max reward: [107.18575]\n",
            " \n",
            "episode: 191 epsilon: 0.3945022182948784\n",
            "duration: 182\n",
            "max reward: [107.16572]\n",
            "result: coin\n",
            "total steps: 92660 \n",
            "\n",
            "episode: 192 epsilon: 0.39378487723595534\n",
            "duration: 201\n",
            "max reward: [107.631546]\n",
            "result: coin\n",
            "total steps: 92861 \n",
            "\n",
            "episode: 193 epsilon: 0.39299416456515845\n",
            "duration: 49\n",
            "max reward: [106.9595]\n",
            "result: coin\n",
            "total steps: 92910 \n",
            "\n",
            "episode: 194 epsilon: 0.392801644595766\n",
            "duration: 113\n",
            "max reward: [107.94262]\n",
            "result: coin\n",
            "total steps: 93023 \n",
            "\n",
            "episode: 195 epsilon: 0.3923580294271474\n",
            "duration: 204\n",
            "max reward: [107.9709]\n",
            "result: coin\n",
            "total steps: 93227 \n",
            "\n",
            "episode: 196 epsilon: 0.39155843491082204\n",
            "duration: 88\n",
            "max reward: [106.975784]\n",
            "result: coin\n",
            "total steps: 93315 \n",
            "\n",
            "episode: 197 epsilon: 0.39121401505506365\n",
            "duration: 235\n",
            "max reward: [107.57684]\n",
            "result: coin\n",
            "total steps: 93550 \n",
            "\n",
            "episode: 198 epsilon: 0.39029574151369245\n",
            "duration: 1002\n",
            "max reward: [3.784309]\n",
            "result: timeout\n",
            "total steps: 94552 \n",
            "\n",
            "episode: 199 epsilon: 0.3864045058312414\n",
            "duration: 315\n",
            "max reward: [107.287025]\n",
            "result: coin\n",
            "total steps: 94867 \n",
            "\n",
            "episode: 200 epsilon: 0.3851892466759099\n",
            "duration: 323\n",
            "max reward: [107.107414]\n",
            "result: coin\n",
            "total steps: 95190 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.195477]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 347\n",
            "max reward: [106.84167]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2766469]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 758\n",
            "max reward: [106.90496]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.216227]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.8161392]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.8972566]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.024242]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.52909]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.759521]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 912.1\n",
            "Average max reward: [24.146124]\n",
            " \n",
            "episode: 201 epsilon: 0.38394709256797\n",
            "duration: 1002\n",
            "max reward: [3.1852708]\n",
            "result: timeout\n",
            "total steps: 96192 \n",
            "\n",
            "episode: 202 epsilon: 0.3801191527063167\n",
            "duration: 45\n",
            "max reward: [107.089676]\n",
            "result: coin\n",
            "total steps: 96237 \n",
            "\n",
            "episode: 203 epsilon: 0.3799481375688906\n",
            "duration: 136\n",
            "max reward: [107.05918]\n",
            "result: coin\n",
            "total steps: 96373 \n",
            "\n",
            "episode: 204 epsilon: 0.3794317593185982\n",
            "duration: 1002\n",
            "max reward: [3.6927547]\n",
            "result: timeout\n",
            "total steps: 97375 \n",
            "\n",
            "episode: 205 epsilon: 0.3756488371806586\n",
            "duration: 214\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 97589 \n",
            "\n",
            "episode: 206 epsilon: 0.3748458082165462\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 98591 \n",
            "\n",
            "episode: 207 epsilon: 0.37110860785998473\n",
            "duration: 1002\n",
            "max reward: [2.1932838]\n",
            "result: timeout\n",
            "total steps: 99593 \n",
            "\n",
            "episode: 208 epsilon: 0.36740866726783544\n",
            "duration: 481\n",
            "max reward: [107.06411]\n",
            "result: coin\n",
            "total steps: 100074 \n",
            "\n",
            "episode: 209 epsilon: 0.36564567497380684\n",
            "duration: 1002\n",
            "max reward: [3.2574735]\n",
            "result: timeout\n",
            "total steps: 101076 \n",
            "\n",
            "episode: 210 epsilon: 0.36200019964252644\n",
            "duration: 831\n",
            "max reward: [107.02261]\n",
            "result: coin\n",
            "total steps: 101907 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.1460526]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.8666499]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.45752287]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5934706]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.101913]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.0465606]\n",
            " \n",
            "episode: 211 epsilon: 0.3590044425936236\n",
            "duration: 599\n",
            "max reward: [107.75131]\n",
            "result: coin\n",
            "total steps: 102506 \n",
            "\n",
            "episode: 212 epsilon: 0.35686043369972564\n",
            "duration: 855\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 103361 \n",
            "\n",
            "episode: 213 epsilon: 0.35382228359131457\n",
            "duration: 445\n",
            "max reward: [106.99987]\n",
            "result: coin\n",
            "total steps: 103806 \n",
            "\n",
            "episode: 214 epsilon: 0.3522512725214583\n",
            "duration: 144\n",
            "max reward: [107.03884]\n",
            "result: coin\n",
            "total steps: 103950 \n",
            "\n",
            "episode: 215 epsilon: 0.3517443957279071\n",
            "duration: 240\n",
            "max reward: [107.18708]\n",
            "result: coin\n",
            "total steps: 104190 \n",
            "\n",
            "episode: 216 epsilon: 0.3509012213920868\n",
            "duration: 365\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 104555 \n",
            "\n",
            "episode: 217 epsilon: 0.3496227665334735\n",
            "duration: 365\n",
            "max reward: [107.07069]\n",
            "result: coin\n",
            "total steps: 104920 \n",
            "\n",
            "episode: 218 epsilon: 0.34834896952933864\n",
            "duration: 785\n",
            "max reward: [107.06422]\n",
            "result: coin\n",
            "total steps: 105705 \n",
            "\n",
            "episode: 219 epsilon: 0.3456251351558917\n",
            "duration: 142\n",
            "max reward: [106.92083]\n",
            "result: coin\n",
            "total steps: 105847 \n",
            "\n",
            "episode: 220 epsilon: 0.3451346957583528\n",
            "duration: 337\n",
            "max reward: [106.96293]\n",
            "result: coin\n",
            "total steps: 106184 \n",
            "\n",
            "Evaluating...\n",
            "duration: 72\n",
            "max reward: [107.17167]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 71\n",
            "max reward: [107.03144]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 86\n",
            "max reward: [107.82358]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 54\n",
            "max reward: [107.72714]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 71\n",
            "max reward: [106.847916]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [107.03329]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 72\n",
            "max reward: [107.17817]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 84\n",
            "max reward: [107.82358]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 78\n",
            "max reward: [107.113205]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 50\n",
            "max reward: [107.95395]\n",
            "result: coin \n",
            "\n",
            "Average duration: 68.4\n",
            "Average max reward: [107.370384]\n",
            " \n",
            "episode: 221 epsilon: 0.3439735494640713\n",
            "duration: 1002\n",
            "max reward: [6.2149568]\n",
            "result: timeout\n",
            "total steps: 107186 \n",
            "\n",
            "episode: 222 epsilon: 0.3405441445100156\n",
            "duration: 141\n",
            "max reward: [107.8083]\n",
            "result: coin\n",
            "total steps: 107327 \n",
            "\n",
            "episode: 223 epsilon: 0.3400643156251159\n",
            "duration: 208\n",
            "max reward: [106.8942]\n",
            "result: coin\n",
            "total steps: 107535 \n",
            "\n",
            "episode: 224 epsilon: 0.33935771696597355\n",
            "duration: 144\n",
            "max reward: [107.057144]\n",
            "result: coin\n",
            "total steps: 107679 \n",
            "\n",
            "episode: 225 epsilon: 0.3388693935307982\n",
            "duration: 446\n",
            "max reward: [106.84054]\n",
            "result: coin\n",
            "total steps: 108125 \n",
            "\n",
            "episode: 226 epsilon: 0.3373614013578936\n",
            "duration: 260\n",
            "max reward: [106.89348]\n",
            "result: coin\n",
            "total steps: 108385 \n",
            "\n",
            "episode: 227 epsilon: 0.3364854010082977\n",
            "duration: 127\n",
            "max reward: [106.88989]\n",
            "result: coin\n",
            "total steps: 108512 \n",
            "\n",
            "episode: 228 epsilon: 0.3360583357928301\n",
            "duration: 183\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 108695 \n",
            "\n",
            "episode: 229 epsilon: 0.33544391140811175\n",
            "duration: 90\n",
            "max reward: [107.84497]\n",
            "result: coin\n",
            "total steps: 108785 \n",
            "\n",
            "episode: 230 epsilon: 0.3351421477018813\n",
            "duration: 219\n",
            "max reward: [107.71251]\n",
            "result: coin\n",
            "total steps: 109004 \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [106.8685]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 74\n",
            "max reward: [107.912674]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 137\n",
            "max reward: [106.81888]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 175\n",
            "max reward: [106.83946]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 57\n",
            "max reward: [107.134796]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 32\n",
            "max reward: [106.948784]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 120\n",
            "max reward: [107.72066]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 48\n",
            "max reward: [107.192276]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [107.02607]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 90\n",
            "max reward: [107.122116]\n",
            "result: coin \n",
            "\n",
            "Average duration: 78.1\n",
            "Average max reward: [107.158424]\n",
            " \n",
            "episode: 231 epsilon: 0.3344089894996706\n",
            "duration: 514\n",
            "max reward: [107.48273]\n",
            "result: coin\n",
            "total steps: 109518 \n",
            "\n",
            "episode: 232 epsilon: 0.3326945372106188\n",
            "duration: 419\n",
            "max reward: [107.02157]\n",
            "result: coin\n",
            "total steps: 109937 \n",
            "\n",
            "episode: 233 epsilon: 0.33130346343441935\n",
            "duration: 149\n",
            "max reward: [106.86133]\n",
            "result: coin\n",
            "total steps: 110086 \n",
            "\n",
            "episode: 234 epsilon: 0.33081018885472385\n",
            "duration: 141\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 110227 \n",
            "\n",
            "episode: 235 epsilon: 0.3303440751758057\n",
            "duration: 76\n",
            "max reward: [106.98785]\n",
            "result: coin\n",
            "total steps: 110303 \n",
            "\n",
            "episode: 236 epsilon: 0.3300931090578767\n",
            "duration: 67\n",
            "max reward: [107.059204]\n",
            "result: coin\n",
            "total steps: 110370 \n",
            "\n",
            "episode: 237 epsilon: 0.3298720207476624\n",
            "duration: 159\n",
            "max reward: [107.036285]\n",
            "result: coin\n",
            "total steps: 110529 \n",
            "\n",
            "episode: 238 epsilon: 0.32934794098849274\n",
            "duration: 114\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 110643 \n",
            "\n",
            "episode: 239 epsilon: 0.3289726982647571\n",
            "duration: 122\n",
            "max reward: [107.094406]\n",
            "result: coin\n",
            "total steps: 110765 \n",
            "\n",
            "episode: 240 epsilon: 0.3285715962948258\n",
            "duration: 242\n",
            "max reward: [106.987526]\n",
            "result: coin\n",
            "total steps: 111007 \n",
            "\n",
            "Evaluating...\n",
            "duration: 605\n",
            "max reward: [107.13562]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 174\n",
            "max reward: [107.01693]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 647\n",
            "max reward: [107.59754]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 846\n",
            "max reward: [106.93783]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 263\n",
            "max reward: [107.754166]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 531\n",
            "max reward: [107.61086]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [107.01094]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 374\n",
            "max reward: [107.516914]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 201\n",
            "max reward: [107.215645]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 361\n",
            "max reward: [107.82954]\n",
            "result: coin \n",
            "\n",
            "Average duration: 407.0\n",
            "Average max reward: [107.362595]\n",
            " \n",
            "episode: 241 epsilon: 0.3277774143794971\n",
            "duration: 127\n",
            "max reward: [107.82358]\n",
            "result: coin\n",
            "total steps: 111134 \n",
            "\n",
            "episode: 242 epsilon: 0.32736140128746416\n",
            "duration: 306\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 111440 \n",
            "\n",
            "episode: 243 epsilon: 0.3263612064780349\n",
            "duration: 1002\n",
            "max reward: [3.5817456]\n",
            "result: timeout\n",
            "total steps: 112442 \n",
            "\n",
            "episode: 244 epsilon: 0.3231073960032143\n",
            "duration: 1002\n",
            "max reward: [3.3360872]\n",
            "result: timeout\n",
            "total steps: 113444 \n",
            "\n",
            "episode: 245 epsilon: 0.31988602591161314\n",
            "duration: 253\n",
            "max reward: [107.05981]\n",
            "result: coin\n",
            "total steps: 113697 \n",
            "\n",
            "episode: 246 epsilon: 0.31907773718244703\n",
            "duration: 1002\n",
            "max reward: [4.1289287]\n",
            "result: timeout\n",
            "total steps: 114699 \n",
            "\n",
            "episode: 247 epsilon: 0.3158965426565095\n",
            "duration: 325\n",
            "max reward: [107.912674]\n",
            "result: coin\n",
            "total steps: 115024 \n",
            "\n",
            "episode: 248 epsilon: 0.3148715454156033\n",
            "duration: 535\n",
            "max reward: [107.02154]\n",
            "result: coin\n",
            "total steps: 115559 \n",
            "\n",
            "episode: 249 epsilon: 0.31319148082770465\n",
            "duration: 403\n",
            "max reward: [106.90271]\n",
            "result: coin\n",
            "total steps: 115962 \n",
            "\n",
            "episode: 250 epsilon: 0.3119318590027285\n",
            "duration: 353\n",
            "max reward: [107.912674]\n",
            "result: coin\n",
            "total steps: 116315 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4244776]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6521354]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5838966]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3899784]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9835169]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4979959]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6093566]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6305127]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.1790726]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5764353]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.4527379]\n",
            " \n",
            "episode: 251 epsilon: 0.3108326807314932\n",
            "duration: 190\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 116505 \n",
            "\n",
            "episode: 252 epsilon: 0.3102426593359273\n",
            "duration: 280\n",
            "max reward: [107.29309]\n",
            "result: coin\n",
            "total steps: 116785 \n",
            "\n",
            "episode: 253 epsilon: 0.309375194906731\n",
            "duration: 95\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 116880 \n",
            "\n",
            "episode: 254 epsilon: 0.3090814280329283\n",
            "duration: 164\n",
            "max reward: [107.42578]\n",
            "result: coin\n",
            "total steps: 117044 \n",
            "\n",
            "episode: 255 epsilon: 0.30857494991652834\n",
            "duration: 39\n",
            "max reward: [107.614395]\n",
            "result: coin\n",
            "total steps: 117083 \n",
            "\n",
            "episode: 256 epsilon: 0.3084546291501354\n",
            "duration: 958\n",
            "max reward: [106.881546]\n",
            "result: coin\n",
            "total steps: 118041 \n",
            "\n",
            "episode: 257 epsilon: 0.3055137431388311\n",
            "duration: 779\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 118820 \n",
            "\n",
            "episode: 258 epsilon: 0.30314303696897993\n",
            "duration: 163\n",
            "max reward: [107.47798]\n",
            "result: coin\n",
            "total steps: 118983 \n",
            "\n",
            "episode: 259 epsilon: 0.3026493163103711\n",
            "duration: 678\n",
            "max reward: [107.3873]\n",
            "result: coin\n",
            "total steps: 119661 \n",
            "\n",
            "episode: 260 epsilon: 0.3006042944039091\n",
            "duration: 535\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 120196 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6525755]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.687217]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8045683]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.63212943]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8910434]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6632464]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7124491]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.657217]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8144667]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.741755]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.72566676]\n",
            " \n",
            "episode: 261 epsilon: 0.2990003557903654\n",
            "duration: 1002\n",
            "max reward: [4.1828365]\n",
            "result: timeout\n",
            "total steps: 121198 \n",
            "\n",
            "episode: 262 epsilon: 0.2960193320953472\n",
            "duration: 355\n",
            "max reward: [107.45428]\n",
            "result: coin\n",
            "total steps: 121553 \n",
            "\n",
            "episode: 263 epsilon: 0.2949703265529206\n",
            "duration: 104\n",
            "max reward: [106.959274]\n",
            "result: coin\n",
            "total steps: 121657 \n",
            "\n",
            "episode: 264 epsilon: 0.29466371687797227\n",
            "duration: 506\n",
            "max reward: [107.91994]\n",
            "result: coin\n",
            "total steps: 122163 \n",
            "\n",
            "episode: 265 epsilon: 0.2931764843420931\n",
            "duration: 330\n",
            "max reward: [107.56143]\n",
            "result: coin\n",
            "total steps: 122493 \n",
            "\n",
            "episode: 266 epsilon: 0.2922105965351886\n",
            "duration: 244\n",
            "max reward: [107.107155]\n",
            "result: coin\n",
            "total steps: 122737 \n",
            "\n",
            "episode: 267 epsilon: 0.29149847182509786\n",
            "duration: 325\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 123062 \n",
            "\n",
            "episode: 268 epsilon: 0.2905526396015588\n",
            "duration: 133\n",
            "max reward: [107.81089]\n",
            "result: coin\n",
            "total steps: 123195 \n",
            "\n",
            "episode: 269 epsilon: 0.2901664614562812\n",
            "duration: 258\n",
            "max reward: [106.9751]\n",
            "result: coin\n",
            "total steps: 123453 \n",
            "\n",
            "episode: 270 epsilon: 0.2894187968877469\n",
            "duration: 553\n",
            "max reward: [106.994545]\n",
            "result: coin\n",
            "total steps: 124006 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.163382]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.189518]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.2589817]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.669901]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.882856]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.6450434]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.4206343]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.5980406]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.7978745]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.7699013]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [5.639613]\n",
            " \n",
            "episode: 271 epsilon: 0.28782272813848214\n",
            "duration: 985\n",
            "max reward: [107.066666]\n",
            "result: coin\n",
            "total steps: 124991 \n",
            "\n",
            "episode: 272 epsilon: 0.285001591175305\n",
            "duration: 466\n",
            "max reward: [107.99426]\n",
            "result: coin\n",
            "total steps: 125457 \n",
            "\n",
            "episode: 273 epsilon: 0.28367657344952446\n",
            "duration: 235\n",
            "max reward: [107.146965]\n",
            "result: coin\n",
            "total steps: 125692 \n",
            "\n",
            "episode: 274 epsilon: 0.2830107161906303\n",
            "duration: 1002\n",
            "max reward: [4.7241426]\n",
            "result: timeout\n",
            "total steps: 126694 \n",
            "\n",
            "episode: 275 epsilon: 0.28018910867555485\n",
            "duration: 127\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 126821 \n",
            "\n",
            "episode: 276 epsilon: 0.2798334943704182\n",
            "duration: 534\n",
            "max reward: [107.27016]\n",
            "result: coin\n",
            "total steps: 127355 \n",
            "\n",
            "episode: 277 epsilon: 0.27834316622808525\n",
            "duration: 1002\n",
            "max reward: [4.120472]\n",
            "result: timeout\n",
            "total steps: 128357 \n",
            "\n",
            "episode: 278 epsilon: 0.2755680940323383\n",
            "duration: 478\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 128835 \n",
            "\n",
            "episode: 279 epsilon: 0.2742540216778276\n",
            "duration: 508\n",
            "max reward: [106.95626]\n",
            "result: coin\n",
            "total steps: 129343 \n",
            "\n",
            "episode: 280 epsilon: 0.27286434401750825\n",
            "duration: 248\n",
            "max reward: [107.5405]\n",
            "result: coin\n",
            "total steps: 129591 \n",
            "\n",
            "Evaluating...\n",
            "duration: 102\n",
            "max reward: [107.12113]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 282\n",
            "max reward: [107.7229]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 86\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 291\n",
            "max reward: [107.10946]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 478\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 105\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 113\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 279\n",
            "max reward: [107.910904]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 146\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 90\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 197.2\n",
            "Average max reward: [107.78644]\n",
            " \n",
            "episode: 281 epsilon: 0.27218847886353914\n",
            "duration: 1002\n",
            "max reward: [1.8253207]\n",
            "result: timeout\n",
            "total steps: 130593 \n",
            "\n",
            "episode: 282 epsilon: 0.26947476869801656\n",
            "duration: 219\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 130812 \n",
            "\n",
            "episode: 283 epsilon: 0.268885264697059\n",
            "duration: 226\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 131038 \n",
            "\n",
            "episode: 284 epsilon: 0.2682782701610264\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 132040 \n",
            "\n",
            "episode: 285 epsilon: 0.2656035446474246\n",
            "duration: 694\n",
            "max reward: [107.9709]\n",
            "result: coin\n",
            "total steps: 132734 \n",
            "\n",
            "episode: 286 epsilon: 0.26376663748808027\n",
            "duration: 1002\n",
            "max reward: [4.0774913]\n",
            "result: timeout\n",
            "total steps: 133736 \n",
            "\n",
            "episode: 287 epsilon: 0.26113689280356717\n",
            "duration: 861\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 134597 \n",
            "\n",
            "episode: 288 epsilon: 0.25889815574971814\n",
            "duration: 155\n",
            "max reward: [106.962036]\n",
            "result: coin\n",
            "total steps: 134752 \n",
            "\n",
            "episode: 289 epsilon: 0.25849717444909387\n",
            "duration: 160\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 134912 \n",
            "\n",
            "episode: 290 epsilon: 0.2580839096699617\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 135914 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4391992]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.878706]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2925699]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2385247]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.811305]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8367486]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0037763]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.54942155]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.7050251]\n",
            " \n",
            "episode: 291 epsilon: 0.25551082159454647\n",
            "duration: 722\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 136636 \n",
            "\n",
            "episode: 292 epsilon: 0.25367267714903596\n",
            "duration: 620\n",
            "max reward: [107.28461]\n",
            "result: coin\n",
            "total steps: 137256 \n",
            "\n",
            "episode: 293 epsilon: 0.2521047720789486\n",
            "duration: 578\n",
            "max reward: [107.0721]\n",
            "result: coin\n",
            "total steps: 137834 \n",
            "\n",
            "episode: 294 epsilon: 0.25065180960298156\n",
            "duration: 1002\n",
            "max reward: [3.0236301]\n",
            "result: timeout\n",
            "total steps: 138836 \n",
            "\n",
            "episode: 295 epsilon: 0.24815281932034267\n",
            "duration: 1002\n",
            "max reward: [2.7731602]\n",
            "result: timeout\n",
            "total steps: 139838 \n",
            "\n",
            "episode: 296 epsilon: 0.24567874388847874\n",
            "duration: 797\n",
            "max reward: [106.97661]\n",
            "result: coin\n",
            "total steps: 140635 \n",
            "\n",
            "episode: 297 epsilon: 0.243728466478769\n",
            "duration: 111\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 140746 \n",
            "\n",
            "episode: 298 epsilon: 0.2434580779743597\n",
            "duration: 289\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 141035 \n",
            "\n",
            "episode: 299 epsilon: 0.24275549984341319\n",
            "duration: 167\n",
            "max reward: [107.698906]\n",
            "result: coin\n",
            "total steps: 141202 \n",
            "\n",
            "episode: 300 epsilon: 0.24235043648072263\n",
            "duration: 353\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 141555 \n",
            "\n",
            "Evaluating...\n",
            "duration: 447\n",
            "max reward: [107.342186]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.987544]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 260\n",
            "max reward: [107.559265]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 469\n",
            "max reward: [107.03195]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 331\n",
            "max reward: [107.026054]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 573\n",
            "max reward: [107.00328]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 753\n",
            "max reward: [107.0806]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 448\n",
            "max reward: [106.957825]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.4831696]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 155\n",
            "max reward: [107.18103]\n",
            "result: coin \n",
            "\n",
            "Average duration: 544.0\n",
            "Average max reward: [86.86529]\n",
            " \n",
            "episode: 301 epsilon: 0.24149644761707897\n",
            "duration: 242\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 141797 \n",
            "\n",
            "episode: 302 epsilon: 0.24091273279365424\n",
            "duration: 481\n",
            "max reward: [107.08988]\n",
            "result: coin\n",
            "total steps: 142278 \n",
            "\n",
            "episode: 303 epsilon: 0.23975672497650882\n",
            "duration: 1002\n",
            "max reward: [2.6484218]\n",
            "result: timeout\n",
            "total steps: 143280 \n",
            "\n",
            "episode: 304 epsilon: 0.23736635832859748\n",
            "duration: 1002\n",
            "max reward: [3.4032974]\n",
            "result: timeout\n",
            "total steps: 144282 \n",
            "\n",
            "episode: 305 epsilon: 0.2349998235574019\n",
            "duration: 423\n",
            "max reward: [107.03678]\n",
            "result: coin\n",
            "total steps: 144705 \n",
            "\n",
            "episode: 306 epsilon: 0.2340078737566538\n",
            "duration: 1002\n",
            "max reward: [1.8806598]\n",
            "result: timeout\n",
            "total steps: 145707 \n",
            "\n",
            "episode: 307 epsilon: 0.23167482296597683\n",
            "duration: 181\n",
            "max reward: [107.64937]\n",
            "result: coin\n",
            "total steps: 145888 \n",
            "\n",
            "episode: 308 epsilon: 0.23125587080249377\n",
            "duration: 278\n",
            "max reward: [107.54369]\n",
            "result: coin\n",
            "total steps: 146166 \n",
            "\n",
            "episode: 309 epsilon: 0.2306138722730871\n",
            "duration: 408\n",
            "max reward: [107.51007]\n",
            "result: coin\n",
            "total steps: 146574 \n",
            "\n",
            "episode: 310 epsilon: 0.22967488451180934\n",
            "duration: 1002\n",
            "max reward: [2.3403487]\n",
            "result: timeout\n",
            "total steps: 147576 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5673156]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3243275]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6925466]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0713263]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6269729]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.67883253]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.31625676]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.85909915]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.744921]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.0481598]\n",
            " \n",
            "episode: 311 epsilon: 0.22738503348112932\n",
            "duration: 571\n",
            "max reward: [106.966644]\n",
            "result: coin\n",
            "total steps: 148147 \n",
            "\n",
            "episode: 312 epsilon: 0.22609036473684088\n",
            "duration: 1002\n",
            "max reward: [4.587414]\n",
            "result: timeout\n",
            "total steps: 149149 \n",
            "\n",
            "episode: 313 epsilon: 0.22383625125020548\n",
            "duration: 1002\n",
            "max reward: [5.029691]\n",
            "result: timeout\n",
            "total steps: 150151 \n",
            "\n",
            "episode: 314 epsilon: 0.22160461120075775\n",
            "duration: 342\n",
            "max reward: [107.07729]\n",
            "result: coin\n",
            "total steps: 150493 \n",
            "\n",
            "episode: 315 epsilon: 0.2208480179423743\n",
            "duration: 342\n",
            "max reward: [107.54586]\n",
            "result: coin\n",
            "total steps: 150835 \n",
            "\n",
            "episode: 316 epsilon: 0.22009400781326569\n",
            "duration: 1002\n",
            "max reward: [3.0119572]\n",
            "result: timeout\n",
            "total steps: 151837 \n",
            "\n",
            "episode: 317 epsilon: 0.21789967780757538\n",
            "duration: 859\n",
            "max reward: [107.09154]\n",
            "result: coin\n",
            "total steps: 152696 \n",
            "\n",
            "episode: 318 epsilon: 0.21603593580725075\n",
            "duration: 1002\n",
            "max reward: [4.615878]\n",
            "result: timeout\n",
            "total steps: 153698 \n",
            "\n",
            "episode: 319 epsilon: 0.21388206464574488\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 154700 \n",
            "\n",
            "episode: 320 epsilon: 0.21174966750874818\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 155702 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.20593023]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.23885226]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.38382578]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.26707935]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.38178658]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.29445958]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4490235]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4091668]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4091668]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4091668]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.34484577]\n",
            " \n",
            "episode: 321 epsilon: 0.20963853030094373\n",
            "duration: 1002\n",
            "max reward: [3.6520076]\n",
            "result: timeout\n",
            "total steps: 156704 \n",
            "\n",
            "episode: 322 epsilon: 0.20754844106153808\n",
            "duration: 1002\n",
            "max reward: [6.917473]\n",
            "result: timeout\n",
            "total steps: 157706 \n",
            "\n",
            "episode: 323 epsilon: 0.2054791899429798\n",
            "duration: 162\n",
            "max reward: [106.95891]\n",
            "result: coin\n",
            "total steps: 157868 \n",
            "\n",
            "episode: 324 epsilon: 0.2051465831395241\n",
            "duration: 1002\n",
            "max reward: [4.782539]\n",
            "result: timeout\n",
            "total steps: 158870 \n",
            "\n",
            "episode: 325 epsilon: 0.20310127846530585\n",
            "duration: 590\n",
            "max reward: [107.45967]\n",
            "result: coin\n",
            "total steps: 159460 \n",
            "\n",
            "episode: 326 epsilon: 0.20190650895823162\n",
            "duration: 1002\n",
            "max reward: [6.3748236]\n",
            "result: timeout\n",
            "total steps: 160462 \n",
            "\n",
            "episode: 327 epsilon: 0.19989350771683884\n",
            "duration: 577\n",
            "max reward: [107.293976]\n",
            "result: coin\n",
            "total steps: 161039 \n",
            "\n",
            "episode: 328 epsilon: 0.19874344330389054\n",
            "duration: 52\n",
            "max reward: [107.094154]\n",
            "result: coin\n",
            "total steps: 161091 \n",
            "\n",
            "episode: 329 epsilon: 0.1986401235788292\n",
            "duration: 166\n",
            "max reward: [107.901794]\n",
            "result: coin\n",
            "total steps: 161257 \n",
            "\n",
            "episode: 330 epsilon: 0.19831065450867363\n",
            "duration: 67\n",
            "max reward: [106.937126]\n",
            "result: coin\n",
            "total steps: 161324 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.86008096]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9489067]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.104773]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.93856]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0457008]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7084162]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8557999]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.93856]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6880219]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.9993038]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.0088122]\n",
            " \n",
            "episode: 331 epsilon: 0.1981778308710401\n",
            "duration: 536\n",
            "max reward: [107.19046]\n",
            "result: coin\n",
            "total steps: 161860 \n",
            "\n",
            "episode: 332 epsilon: 0.1971184394030288\n",
            "duration: 448\n",
            "max reward: [107.0067]\n",
            "result: coin\n",
            "total steps: 162308 \n",
            "\n",
            "episode: 333 epsilon: 0.19623732396677496\n",
            "duration: 374\n",
            "max reward: [107.797966]\n",
            "result: coin\n",
            "total steps: 162682 \n",
            "\n",
            "episode: 334 epsilon: 0.19550476711035317\n",
            "duration: 1002\n",
            "max reward: [5.2998347]\n",
            "result: timeout\n",
            "total steps: 163684 \n",
            "\n",
            "episode: 335 epsilon: 0.19355559102424316\n",
            "duration: 435\n",
            "max reward: [107.76679]\n",
            "result: coin\n",
            "total steps: 164119 \n",
            "\n",
            "episode: 336 epsilon: 0.19271545282865557\n",
            "duration: 654\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 164773 \n",
            "\n",
            "episode: 337 epsilon: 0.19145920617136228\n",
            "duration: 100\n",
            "max reward: [107.06138]\n",
            "result: coin\n",
            "total steps: 164873 \n",
            "\n",
            "episode: 338 epsilon: 0.1912678426628921\n",
            "duration: 507\n",
            "max reward: [107.797966]\n",
            "result: coin\n",
            "total steps: 165380 \n",
            "\n",
            "episode: 339 epsilon: 0.19030056881177607\n",
            "duration: 283\n",
            "max reward: [107.97359]\n",
            "result: coin\n",
            "total steps: 165663 \n",
            "\n",
            "episode: 340 epsilon: 0.18976277953279352\n",
            "duration: 539\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 166202 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3391993]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.9269726]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7301562]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.31625676]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6892357]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.69999933]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.8701817]\n",
            " \n",
            "episode: 341 epsilon: 0.18874270970888357\n",
            "duration: 369\n",
            "max reward: [107.163574]\n",
            "result: coin\n",
            "total steps: 166571 \n",
            "\n",
            "episode: 342 epsilon: 0.18804753250080666\n",
            "duration: 296\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 166867 \n",
            "\n",
            "episode: 343 epsilon: 0.18749173479102116\n",
            "duration: 1002\n",
            "max reward: [4.9575353]\n",
            "result: timeout\n",
            "total steps: 167869 \n",
            "\n",
            "episode: 344 epsilon: 0.1856224483731014\n",
            "duration: 1002\n",
            "max reward: [4.3579664]\n",
            "result: timeout\n",
            "total steps: 168871 \n",
            "\n",
            "episode: 345 epsilon: 0.1837717986791744\n",
            "duration: 1002\n",
            "max reward: [3.6366062]\n",
            "result: timeout\n",
            "total steps: 169873 \n",
            "\n",
            "episode: 346 epsilon: 0.18193959990171601\n",
            "duration: 1002\n",
            "max reward: [5.323591]\n",
            "result: timeout\n",
            "total steps: 170875 \n",
            "\n",
            "episode: 347 epsilon: 0.18012566808569697\n",
            "duration: 481\n",
            "max reward: [107.118324]\n",
            "result: coin\n",
            "total steps: 171356 \n",
            "\n",
            "episode: 348 epsilon: 0.17926134398808294\n",
            "duration: 394\n",
            "max reward: [107.028244]\n",
            "result: coin\n",
            "total steps: 171750 \n",
            "\n",
            "episode: 349 epsilon: 0.1785564438579084\n",
            "duration: 896\n",
            "max reward: [107.021454]\n",
            "result: coin\n",
            "total steps: 172646 \n",
            "\n",
            "episode: 350 epsilon: 0.17696372416067804\n",
            "duration: 1002\n",
            "max reward: [5.550821]\n",
            "result: timeout\n",
            "total steps: 173648 \n",
            "\n",
            "Evaluating...\n",
            "duration: 429\n",
            "max reward: [106.8867]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.695527]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 357\n",
            "max reward: [106.916245]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 337\n",
            "max reward: [106.841675]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 204\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.68196]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [106.85963]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 637\n",
            "max reward: [106.95581]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [107.02279]\n",
            "result: coin \n",
            "\n",
            "Average duration: 406.4\n",
            "Average max reward: [86.88603]\n",
            " \n",
            "episode: 351 epsilon: 0.17519940166183956\n",
            "duration: 1002\n",
            "max reward: [4.792813]\n",
            "result: timeout\n",
            "total steps: 174650 \n",
            "\n",
            "episode: 352 epsilon: 0.17345266940017923\n",
            "duration: 1002\n",
            "max reward: [5.9359827]\n",
            "result: timeout\n",
            "total steps: 175652 \n",
            "\n",
            "episode: 353 epsilon: 0.17172335200161193\n",
            "duration: 1002\n",
            "max reward: [4.967466]\n",
            "result: timeout\n",
            "total steps: 176654 \n",
            "\n",
            "episode: 354 epsilon: 0.17001127584052644\n",
            "duration: 387\n",
            "max reward: [107.04825]\n",
            "result: coin\n",
            "total steps: 177041 \n",
            "\n",
            "episode: 355 epsilon: 0.1693546036832239\n",
            "duration: 243\n",
            "max reward: [107.07718]\n",
            "result: coin\n",
            "total steps: 177284 \n",
            "\n",
            "episode: 356 epsilon: 0.1689435716025103\n",
            "duration: 1002\n",
            "max reward: [4.9717064]\n",
            "result: timeout\n",
            "total steps: 178286 \n",
            "\n",
            "episode: 357 epsilon: 0.16725920976040876\n",
            "duration: 1002\n",
            "max reward: [5.082366]\n",
            "result: timeout\n",
            "total steps: 179288 \n",
            "\n",
            "episode: 358 epsilon: 0.16559164095037232\n",
            "duration: 208\n",
            "max reward: [107.30347]\n",
            "result: coin\n",
            "total steps: 179496 \n",
            "\n",
            "episode: 359 epsilon: 0.16524756829680493\n",
            "duration: 24\n",
            "max reward: [107.02136]\n",
            "result: coin\n",
            "total steps: 179520 \n",
            "\n",
            "episode: 360 epsilon: 0.16520791363916296\n",
            "duration: 517\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 180037 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.15400004]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.27999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.0999999]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.30078673]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.24168658]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.11642694]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.1447072]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.29707932]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.22597289]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2397523]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.2100412]\n",
            " \n",
            "episode: 361 epsilon: 0.16435599283849284\n",
            "duration: 1002\n",
            "max reward: [6.0237255]\n",
            "result: timeout\n",
            "total steps: 181039 \n",
            "\n",
            "episode: 362 epsilon: 0.1627173690055055\n",
            "duration: 523\n",
            "max reward: [106.93051]\n",
            "result: coin\n",
            "total steps: 181562 \n",
            "\n",
            "episode: 363 epsilon: 0.16186857868702836\n",
            "duration: 1002\n",
            "max reward: [5.3141537]\n",
            "result: timeout\n",
            "total steps: 182564 \n",
            "\n",
            "episode: 364 epsilon: 0.1602547542911696\n",
            "duration: 1002\n",
            "max reward: [5.859969]\n",
            "result: timeout\n",
            "total steps: 183566 \n",
            "\n",
            "episode: 365 epsilon: 0.1586570196713612\n",
            "duration: 1002\n",
            "max reward: [5.294612]\n",
            "result: timeout\n",
            "total steps: 184568 \n",
            "\n",
            "episode: 366 epsilon: 0.15707521441306607\n",
            "duration: 1002\n",
            "max reward: [6.2617283]\n",
            "result: timeout\n",
            "total steps: 185570 \n",
            "\n",
            "episode: 367 epsilon: 0.15550917970107497\n",
            "duration: 459\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 186029 \n",
            "\n",
            "episode: 368 epsilon: 0.15479702820423838\n",
            "duration: 1002\n",
            "max reward: [4.4155517]\n",
            "result: timeout\n",
            "total steps: 187031 \n",
            "\n",
            "episode: 369 epsilon: 0.1532537069336819\n",
            "duration: 87\n",
            "max reward: [106.819916]\n",
            "result: coin\n",
            "total steps: 187118 \n",
            "\n",
            "episode: 370 epsilon: 0.15312043419069898\n",
            "duration: 886\n",
            "max reward: [107.09778]\n",
            "result: coin\n",
            "total steps: 188004 \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [107.16466]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 84\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 75\n",
            "max reward: [107.00496]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 112\n",
            "max reward: [106.98134]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 69\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 52\n",
            "max reward: [106.960205]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 70\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 104\n",
            "max reward: [106.98535]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [107.064156]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 63\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 72.8\n",
            "Average max reward: [107.41606]\n",
            " \n",
            "episode: 371 epsilon: 0.15176977938005748\n",
            "duration: 36\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 188040 \n",
            "\n",
            "episode: 372 epsilon: 0.15171515209298234\n",
            "duration: 1002\n",
            "max reward: [6.2046833]\n",
            "result: timeout\n",
            "total steps: 189042 \n",
            "\n",
            "episode: 373 epsilon: 0.15020255702570562\n",
            "duration: 1002\n",
            "max reward: [5.5028725]\n",
            "result: timeout\n",
            "total steps: 190044 \n",
            "\n",
            "episode: 374 epsilon: 0.14870504248140898\n",
            "duration: 1002\n",
            "max reward: [6.1354938]\n",
            "result: timeout\n",
            "total steps: 191046 \n",
            "\n",
            "episode: 375 epsilon: 0.1472224581077751\n",
            "duration: 1002\n",
            "max reward: [5.5807753]\n",
            "result: timeout\n",
            "total steps: 192048 \n",
            "\n",
            "episode: 376 epsilon: 0.14575465505149443\n",
            "duration: 267\n",
            "max reward: [107.53479]\n",
            "result: coin\n",
            "total steps: 192315 \n",
            "\n",
            "episode: 377 epsilon: 0.14536600919560932\n",
            "duration: 1002\n",
            "max reward: [6.7096033]\n",
            "result: timeout\n",
            "total steps: 193317 \n",
            "\n",
            "episode: 378 epsilon: 0.14391671487381202\n",
            "duration: 597\n",
            "max reward: [107.497986]\n",
            "result: coin\n",
            "total steps: 193914 \n",
            "\n",
            "episode: 379 epsilon: 0.14306009165057046\n",
            "duration: 110\n",
            "max reward: [106.95743]\n",
            "result: coin\n",
            "total steps: 194024 \n",
            "\n",
            "episode: 380 epsilon: 0.1429028120693835\n",
            "duration: 358\n",
            "max reward: [107.15718]\n",
            "result: coin\n",
            "total steps: 194382 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.16746]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4619181]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7072675]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.6068203]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.311438]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8636074]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9912257]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.011962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9593627]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2502439]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.2331307]\n",
            " \n",
            "episode: 381 epsilon: 0.142392134660158\n",
            "duration: 159\n",
            "max reward: [107.08648]\n",
            "result: coin\n",
            "total steps: 194541 \n",
            "\n",
            "episode: 382 epsilon: 0.14216591106146897\n",
            "duration: 333\n",
            "max reward: [107.05901]\n",
            "result: coin\n",
            "total steps: 194874 \n",
            "\n",
            "episode: 383 epsilon: 0.14169328593521047\n",
            "duration: 187\n",
            "max reward: [107.005646]\n",
            "result: coin\n",
            "total steps: 195061 \n",
            "\n",
            "episode: 384 epsilon: 0.1414285670797827\n",
            "duration: 1002\n",
            "max reward: [5.6671133]\n",
            "result: timeout\n",
            "total steps: 196063 \n",
            "\n",
            "episode: 385 epsilon: 0.14001852892613953\n",
            "duration: 1002\n",
            "max reward: [6.9008474]\n",
            "result: timeout\n",
            "total steps: 197065 \n",
            "\n",
            "episode: 386 epsilon: 0.1386225488064267\n",
            "duration: 727\n",
            "max reward: [107.014534]\n",
            "result: coin\n",
            "total steps: 197792 \n",
            "\n",
            "episode: 387 epsilon: 0.13761841731716865\n",
            "duration: 1002\n",
            "max reward: [6.725058]\n",
            "result: timeout\n",
            "total steps: 198794 \n",
            "\n",
            "episode: 388 epsilon: 0.13624636623111233\n",
            "duration: 1002\n",
            "max reward: [5.5984144]\n",
            "result: timeout\n",
            "total steps: 199796 \n",
            "\n",
            "episode: 389 epsilon: 0.1348879944491743\n",
            "duration: 1002\n",
            "max reward: [5.3939877]\n",
            "result: timeout\n",
            "total steps: 200798 \n",
            "\n",
            "episode: 390 epsilon: 0.13354316558914311\n",
            "duration: 1002\n",
            "max reward: [6.7016954]\n",
            "result: timeout\n",
            "total steps: 201800 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0844388]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4179468]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4666045]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7483289]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8504117]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.965101]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2144823]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3073213]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1786413]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.311064]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.4544342]\n",
            " \n",
            "episode: 391 epsilon: 0.13221174462853363\n",
            "duration: 1002\n",
            "max reward: [3.4579744]\n",
            "result: timeout\n",
            "total steps: 202802 \n",
            "\n",
            "episode: 392 epsilon: 0.1308935978910305\n",
            "duration: 1002\n",
            "max reward: [4.177606]\n",
            "result: timeout\n",
            "total steps: 203804 \n",
            "\n",
            "episode: 393 epsilon: 0.12958859303306666\n",
            "duration: 1002\n",
            "max reward: [5.725356]\n",
            "result: timeout\n",
            "total steps: 204806 \n",
            "\n",
            "episode: 394 epsilon: 0.1282965990305361\n",
            "duration: 1002\n",
            "max reward: [5.0639753]\n",
            "result: timeout\n",
            "total steps: 205808 \n",
            "\n",
            "episode: 395 epsilon: 0.12701748616563902\n",
            "duration: 1002\n",
            "max reward: [6.463746]\n",
            "result: timeout\n",
            "total steps: 206810 \n",
            "\n",
            "episode: 396 epsilon: 0.1257511260138575\n",
            "duration: 1002\n",
            "max reward: [6.219202]\n",
            "result: timeout\n",
            "total steps: 207812 \n",
            "\n",
            "episode: 397 epsilon: 0.124497391431062\n",
            "duration: 26\n",
            "max reward: [107.05899]\n",
            "result: coin\n",
            "total steps: 207838 \n",
            "\n",
            "episode: 398 epsilon: 0.12446502631693711\n",
            "duration: 1002\n",
            "max reward: [5.7872543]\n",
            "result: timeout\n",
            "total steps: 208840 \n",
            "\n",
            "episode: 399 epsilon: 0.12322411410574227\n",
            "duration: 642\n",
            "max reward: [107.82619]\n",
            "result: coin\n",
            "total steps: 209482 \n",
            "\n",
            "episode: 400 epsilon: 0.12243554929470854\n",
            "duration: 1002\n",
            "max reward: [6.360066]\n",
            "result: timeout\n",
            "total steps: 210484 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7143569]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.362576]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.95410824]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.48145628]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9461775]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.291331]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.346563]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4367099]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.29931]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7072799]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.353987]\n",
            " \n",
            "episode: 401 epsilon: 0.12121487090255306\n",
            "duration: 1002\n",
            "max reward: [6.0264654]\n",
            "result: timeout\n",
            "total steps: 211486 \n",
            "\n",
            "episode: 402 epsilon: 0.12000636263374542\n",
            "duration: 91\n",
            "max reward: [107.64403]\n",
            "result: coin\n",
            "total steps: 211577 \n",
            "\n",
            "episode: 403 epsilon: 0.11989720651731435\n",
            "duration: 710\n",
            "max reward: [107.35849]\n",
            "result: coin\n",
            "total steps: 212287 \n",
            "\n",
            "episode: 404 epsilon: 0.11904895122072047\n",
            "duration: 1002\n",
            "max reward: [5.9173546]\n",
            "result: timeout\n",
            "total steps: 213289 \n",
            "\n",
            "episode: 405 epsilon: 0.11786203709977257\n",
            "duration: 546\n",
            "max reward: [106.99618]\n",
            "result: coin\n",
            "total steps: 213835 \n",
            "\n",
            "episode: 406 epsilon: 0.11722026401209329\n",
            "duration: 1002\n",
            "max reward: [6.389558]\n",
            "result: timeout\n",
            "total steps: 214837 \n",
            "\n",
            "episode: 407 epsilon: 0.11605158184235918\n",
            "duration: 341\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 215178 \n",
            "\n",
            "episode: 408 epsilon: 0.1156565199116868\n",
            "duration: 883\n",
            "max reward: [107.361176]\n",
            "result: coin\n",
            "total steps: 216061 \n",
            "\n",
            "episode: 409 epsilon: 0.1146397684050097\n",
            "duration: 1002\n",
            "max reward: [6.1766396]\n",
            "result: timeout\n",
            "total steps: 217063 \n",
            "\n",
            "episode: 410 epsilon: 0.11349681369144965\n",
            "duration: 1002\n",
            "max reward: [6.616205]\n",
            "result: timeout\n",
            "total steps: 218065 \n",
            "\n",
            "Evaluating...\n",
            "duration: 80\n",
            "max reward: [107.0643]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 167\n",
            "max reward: [106.961426]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 91\n",
            "max reward: [107.136024]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 51\n",
            "max reward: [107.05745]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 75\n",
            "max reward: [107.02081]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 298\n",
            "max reward: [106.81532]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 161\n",
            "max reward: [106.84744]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 245\n",
            "max reward: [107.15913]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [106.81636]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [107.16632]\n",
            "result: coin \n",
            "\n",
            "Average duration: 126.0\n",
            "Average max reward: [107.00444]\n",
            " \n",
            "episode: 411 epsilon: 0.11236525419872287\n",
            "duration: 1002\n",
            "max reward: [5.868247]\n",
            "result: timeout\n",
            "total steps: 219067 \n",
            "\n",
            "episode: 412 epsilon: 0.11124497631685305\n",
            "duration: 1002\n",
            "max reward: [5.095974]\n",
            "result: timeout\n",
            "total steps: 220069 \n",
            "\n",
            "episode: 413 epsilon: 0.11013586756855182\n",
            "duration: 79\n",
            "max reward: [107.05049]\n",
            "result: coin\n",
            "total steps: 220148 \n",
            "\n",
            "episode: 414 epsilon: 0.11004889459202173\n",
            "duration: 699\n",
            "max reward: [107.1571]\n",
            "result: coin\n",
            "total steps: 220847 \n",
            "\n",
            "episode: 415 epsilon: 0.10928233506554724\n",
            "duration: 1002\n",
            "max reward: [5.7782483]\n",
            "result: timeout\n",
            "total steps: 221849 \n",
            "\n",
            "episode: 416 epsilon: 0.10819279378585152\n",
            "duration: 513\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 222362 \n",
            "\n",
            "episode: 417 epsilon: 0.10763918597187584\n",
            "duration: 585\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 222947 \n",
            "\n",
            "episode: 418 epsilon: 0.10701133498861792\n",
            "duration: 1002\n",
            "max reward: [6.6284504]\n",
            "result: timeout\n",
            "total steps: 223949 \n",
            "\n",
            "episode: 419 epsilon: 0.10594443550485859\n",
            "duration: 1002\n",
            "max reward: [4.021249]\n",
            "result: timeout\n",
            "total steps: 224951 \n",
            "\n",
            "episode: 420 epsilon: 0.10488817297379742\n",
            "duration: 1002\n",
            "max reward: [4.365307]\n",
            "result: timeout\n",
            "total steps: 225953 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5950401]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4927826]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4524169]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4544449]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5261042]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4091668]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.44027996]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6649411]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.41265965]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.3535595]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.48013955]\n",
            " \n",
            "episode: 421 epsilon: 0.10384244134536659\n",
            "duration: 992\n",
            "max reward: [107.655815]\n",
            "result: coin\n",
            "total steps: 226945 \n",
            "\n",
            "episode: 422 epsilon: 0.10281741685442936\n",
            "duration: 1002\n",
            "max reward: [5.8941298]\n",
            "result: timeout\n",
            "total steps: 227947 \n",
            "\n",
            "episode: 423 epsilon: 0.10179233059627613\n",
            "duration: 1002\n",
            "max reward: [3.8762722]\n",
            "result: timeout\n",
            "total steps: 228949 \n",
            "\n",
            "episode: 424 epsilon: 0.10077746441433963\n",
            "duration: 391\n",
            "max reward: [107.65311]\n",
            "result: coin\n",
            "total steps: 229340 \n",
            "\n",
            "episode: 425 epsilon: 0.10038419387341689\n",
            "duration: 1002\n",
            "max reward: [4.74996]\n",
            "result: timeout\n",
            "total steps: 230342 \n",
            "\n",
            "episode: 426 epsilon: 0.1\n",
            "duration: 433\n",
            "max reward: [107.2908]\n",
            "result: coin\n",
            "total steps: 230775 \n",
            "\n",
            "episode: 427 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.8774467]\n",
            "result: timeout\n",
            "total steps: 231777 \n",
            "\n",
            "episode: 428 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.4750676]\n",
            "result: timeout\n",
            "total steps: 232779 \n",
            "\n",
            "episode: 429 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [6.2372694]\n",
            "result: timeout\n",
            "total steps: 233781 \n",
            "\n",
            "episode: 430 epsilon: 0.1\n",
            "duration: 36\n",
            "max reward: [106.99674]\n",
            "result: coin\n",
            "total steps: 233817 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2094016]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7048998]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.192451]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.0299997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4276788]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.1673179]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2588484]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9691033]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7062514]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.2665951]\n",
            " \n",
            "episode: 431 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [6.6821575]\n",
            "result: timeout\n",
            "total steps: 234819 \n",
            "\n",
            "episode: 432 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [5.9307966]\n",
            "result: timeout\n",
            "total steps: 235821 \n",
            "\n",
            "episode: 433 epsilon: 0.1\n",
            "duration: 192\n",
            "max reward: [107.02183]\n",
            "result: coin\n",
            "total steps: 236013 \n",
            "\n",
            "episode: 434 epsilon: 0.1\n",
            "duration: 276\n",
            "max reward: [107.881096]\n",
            "result: coin\n",
            "total steps: 236289 \n",
            "\n",
            "episode: 435 epsilon: 0.1\n",
            "duration: 249\n",
            "max reward: [107.24492]\n",
            "result: coin\n",
            "total steps: 236538 \n",
            "\n",
            "episode: 436 epsilon: 0.1\n",
            "duration: 150\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 236688 \n",
            "\n",
            "episode: 437 epsilon: 0.1\n",
            "duration: 296\n",
            "max reward: [107.96301]\n",
            "result: coin\n",
            "total steps: 236984 \n",
            "\n",
            "episode: 438 epsilon: 0.1\n",
            "duration: 432\n",
            "max reward: [107.83923]\n",
            "result: coin\n",
            "total steps: 237416 \n",
            "\n",
            "episode: 439 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [6.304942]\n",
            "result: timeout\n",
            "total steps: 238418 \n",
            "\n",
            "episode: 440 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.0637302]\n",
            "result: timeout\n",
            "total steps: 239420 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2523117]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.485821]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2934692]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.79999924]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5538177]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.5207434]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.69999933]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7189903]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.6925151]\n",
            " \n",
            "episode: 441 epsilon: 0.1\n",
            "duration: 329\n",
            "max reward: [107.22269]\n",
            "result: coin\n",
            "total steps: 239749 \n",
            "\n",
            "episode: 442 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.9790797]\n",
            "result: timeout\n",
            "total steps: 240751 \n",
            "\n",
            "episode: 443 epsilon: 0.1\n",
            "duration: 511\n",
            "max reward: [107.123245]\n",
            "result: coin\n",
            "total steps: 241262 \n",
            "\n",
            "episode: 444 epsilon: 0.1\n",
            "duration: 23\n",
            "max reward: [107.10366]\n",
            "result: coin\n",
            "total steps: 241285 \n",
            "\n",
            "episode: 445 epsilon: 0.1\n",
            "duration: 156\n",
            "max reward: [107.40271]\n",
            "result: coin\n",
            "total steps: 241441 \n",
            "\n",
            "episode: 446 epsilon: 0.1\n",
            "duration: 104\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 241545 \n",
            "\n",
            "episode: 447 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [6.286594]\n",
            "result: timeout\n",
            "total steps: 242547 \n",
            "\n",
            "episode: 448 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [6.1798673]\n",
            "result: timeout\n",
            "total steps: 243549 \n",
            "\n",
            "episode: 449 epsilon: 0.1\n",
            "duration: 882\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 244431 \n",
            "\n",
            "episode: 450 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.07871]\n",
            "result: timeout\n",
            "total steps: 245433 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3548224]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.64503264]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6858208]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7789679]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7391999]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.48469853]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.93885404]\n",
            " \n",
            "episode: 451 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [5.626841]\n",
            "result: timeout\n",
            "total steps: 246435 \n",
            "\n",
            "episode: 452 epsilon: 0.1\n",
            "duration: 751\n",
            "max reward: [107.68032]\n",
            "result: coin\n",
            "total steps: 247186 \n",
            "\n",
            "episode: 453 epsilon: 0.1\n",
            "duration: 92\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 247278 \n",
            "\n",
            "episode: 454 epsilon: 0.1\n",
            "duration: 25\n",
            "max reward: [107.02771]\n",
            "result: coin\n",
            "total steps: 247303 \n",
            "\n",
            "episode: 455 epsilon: 0.1\n",
            "duration: 270\n",
            "max reward: [107.67805]\n",
            "result: coin\n",
            "total steps: 247573 \n",
            "\n",
            "episode: 456 epsilon: 0.1\n",
            "duration: 493\n",
            "max reward: [107.438034]\n",
            "result: coin\n",
            "total steps: 248066 \n",
            "\n",
            "episode: 457 epsilon: 0.1\n",
            "duration: 403\n",
            "max reward: [107.052765]\n",
            "result: coin\n",
            "total steps: 248469 \n",
            "\n",
            "episode: 458 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.9089756]\n",
            "result: timeout\n",
            "total steps: 249471 \n",
            "\n",
            "episode: 459 epsilon: 0.1\n",
            "duration: 145\n",
            "max reward: [107.93794]\n",
            "result: coin\n",
            "total steps: 249616 \n",
            "\n",
            "episode: 460 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [5.098975]\n",
            "result: timeout\n",
            "total steps: 250618 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7745235]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7294116]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3835711]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.450748]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.46210837]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3187416]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3956182]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6499815]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8244452]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.540478]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.4529626]\n",
            " \n",
            "episode: 461 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.3850298]\n",
            "result: timeout\n",
            "total steps: 251620 \n",
            "\n",
            "episode: 462 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.714594]\n",
            "result: timeout\n",
            "total steps: 252622 \n",
            "\n",
            "episode: 463 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.3989983]\n",
            "result: timeout\n",
            "total steps: 253624 \n",
            "\n",
            "episode: 464 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 254626 \n",
            "\n",
            "episode: 465 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.8722024]\n",
            "result: timeout\n",
            "total steps: 255628 \n",
            "\n",
            "episode: 466 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.0299997]\n",
            "result: timeout\n",
            "total steps: 256630 \n",
            "\n",
            "episode: 467 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.3991423]\n",
            "result: timeout\n",
            "total steps: 257632 \n",
            "\n",
            "episode: 468 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.0443654]\n",
            "result: timeout\n",
            "total steps: 258634 \n",
            "\n",
            "episode: 469 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.8195286]\n",
            "result: timeout\n",
            "total steps: 259636 \n",
            "\n",
            "episode: 470 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.6237621]\n",
            "result: timeout\n",
            "total steps: 260638 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.41372085]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.3831606]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.54748726]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6670742]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.44027996]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.41372085]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6040993]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5513549]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.44027996]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.298167]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.4759345]\n",
            " \n",
            "episode: 471 epsilon: 0.1\n",
            "duration: 904\n",
            "max reward: [106.92122]\n",
            "result: coin\n",
            "total steps: 261542 \n",
            "\n",
            "episode: 472 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.7962518]\n",
            "result: timeout\n",
            "total steps: 262544 \n",
            "\n",
            "episode: 473 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.4609356]\n",
            "result: timeout\n",
            "total steps: 263546 \n",
            "\n",
            "episode: 474 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [4.1603074]\n",
            "result: timeout\n",
            "total steps: 264548 \n",
            "\n",
            "episode: 475 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 265550 \n",
            "\n",
            "episode: 476 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.4687665]\n",
            "result: timeout\n",
            "total steps: 266552 \n",
            "\n",
            "episode: 477 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.9178092]\n",
            "result: timeout\n",
            "total steps: 267554 \n",
            "\n",
            "episode: 478 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.0835707]\n",
            "result: timeout\n",
            "total steps: 268556 \n",
            "\n",
            "episode: 479 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.2143767]\n",
            "result: timeout\n",
            "total steps: 269558 \n",
            "\n",
            "episode: 480 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.5346372]\n",
            "result: timeout\n",
            "total steps: 270560 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.59101]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.414468]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.8157654]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.2515936]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.206889]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.91183]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.923872]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.2591963]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.4059544]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.3722124]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [5.1152797]\n",
            " \n",
            "episode: 481 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.722553]\n",
            "result: timeout\n",
            "total steps: 271562 \n",
            "\n",
            "episode: 482 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.6131997]\n",
            "result: timeout\n",
            "total steps: 272564 \n",
            "\n",
            "episode: 483 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.1361125]\n",
            "result: timeout\n",
            "total steps: 273566 \n",
            "\n",
            "episode: 484 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.94413996]\n",
            "result: timeout\n",
            "total steps: 274568 \n",
            "\n",
            "episode: 485 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.5295622]\n",
            "result: timeout\n",
            "total steps: 275570 \n",
            "\n",
            "episode: 486 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.96880627]\n",
            "result: timeout\n",
            "total steps: 276572 \n",
            "\n",
            "episode: 487 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.7920599]\n",
            "result: timeout\n",
            "total steps: 277574 \n",
            "\n",
            "episode: 488 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.1880078]\n",
            "result: timeout\n",
            "total steps: 278576 \n",
            "\n",
            "episode: 489 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.7610004]\n",
            "result: timeout\n",
            "total steps: 279578 \n",
            "\n",
            "episode: 490 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.71726036]\n",
            "result: timeout\n",
            "total steps: 280580 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4750698]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0144649]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2694912]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3247142]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 769\n",
            "max reward: [107.02607]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.96361613]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.80637]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.0772257]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.8782034]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4061584]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 978.7\n",
            "Average max reward: [12.524138]\n",
            " \n",
            "episode: 491 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.4934573]\n",
            "result: timeout\n",
            "total steps: 281582 \n",
            "\n",
            "episode: 492 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.2198877]\n",
            "result: timeout\n",
            "total steps: 282584 \n",
            "\n",
            "episode: 493 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.31499]\n",
            "result: timeout\n",
            "total steps: 283586 \n",
            "\n",
            "episode: 494 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.0793025]\n",
            "result: timeout\n",
            "total steps: 284588 \n",
            "\n",
            "episode: 495 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.2593641]\n",
            "result: timeout\n",
            "total steps: 285590 \n",
            "\n",
            "episode: 496 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 286592 \n",
            "\n",
            "episode: 497 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.0226214]\n",
            "result: timeout\n",
            "total steps: 287594 \n",
            "\n",
            "episode: 498 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 288596 \n",
            "\n",
            "episode: 499 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.4557037]\n",
            "result: timeout\n",
            "total steps: 289598 \n",
            "\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "SAVE_FILENAME = 'easy.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = EASY_LEVEL\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALs1mZNUnYM3"
      },
      "source": [
        "###### Train for MEDIUM_LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiQi6LpzbJZe",
        "outputId": "645a98b7-e0db-4feb-e39e-021ab6d00729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "result: timeout\n",
            "total steps: 51911 \n",
            "\n",
            "episode: 57 epsilon: 0.5918745853230314\n",
            "duration: 1002\n",
            "max reward: [8.742842]\n",
            "result: timeout\n",
            "total steps: 52913 \n",
            "\n",
            "episode: 58 epsilon: 0.5859736152099253\n",
            "duration: 1002\n",
            "max reward: [9.520224]\n",
            "result: timeout\n",
            "total steps: 53915 \n",
            "\n",
            "episode: 59 epsilon: 0.580131477574407\n",
            "duration: 1002\n",
            "max reward: [9.385599]\n",
            "result: timeout\n",
            "total steps: 54917 \n",
            "\n",
            "episode: 60 epsilon: 0.5743475858586133\n",
            "duration: 1002\n",
            "max reward: [12.22827]\n",
            "result: timeout\n",
            "total steps: 55919 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.260186]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.7580118]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.091881]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2748854]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7917943]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.6477237]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.8295255]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2383852]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.893762]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.7780328]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [4.0564184]\n",
            " \n",
            "episode: 61 epsilon: 0.5686213593526441\n",
            "duration: 1002\n",
            "max reward: [9.687529]\n",
            "result: timeout\n",
            "total steps: 56921 \n",
            "\n",
            "episode: 62 epsilon: 0.5629522231362574\n",
            "duration: 287\n",
            "max reward: [117.94977]\n",
            "result: coin\n",
            "total steps: 57208 \n",
            "\n",
            "episode: 63 epsilon: 0.5613388665300075\n",
            "duration: 377\n",
            "max reward: [117.9279]\n",
            "result: coin\n",
            "total steps: 57585 \n",
            "\n",
            "episode: 64 epsilon: 0.5592266031214962\n",
            "duration: 749\n",
            "max reward: [118.065]\n",
            "result: coin\n",
            "total steps: 58334 \n",
            "\n",
            "episode: 65 epsilon: 0.5550536431079715\n",
            "duration: 166\n",
            "max reward: [118.918526]\n",
            "result: coin\n",
            "total steps: 58500 \n",
            "\n",
            "episode: 66 epsilon: 0.554133018390334\n",
            "duration: 390\n",
            "max reward: [118.21561]\n",
            "result: coin\n",
            "total steps: 58890 \n",
            "\n",
            "episode: 67 epsilon: 0.5519761083271177\n",
            "duration: 882\n",
            "max reward: [118.94681]\n",
            "result: coin\n",
            "total steps: 59772 \n",
            "\n",
            "episode: 68 epsilon: 0.5471290858425812\n",
            "duration: 909\n",
            "max reward: [118.70582]\n",
            "result: coin\n",
            "total steps: 60681 \n",
            "\n",
            "episode: 69 epsilon: 0.5421782182355644\n",
            "duration: 1002\n",
            "max reward: [12.173105]\n",
            "result: timeout\n",
            "total steps: 61683 \n",
            "\n",
            "episode: 70 epsilon: 0.5367727192648003\n",
            "duration: 917\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 62600 \n",
            "\n",
            "Evaluating...\n",
            "duration: 405\n",
            "max reward: [118.544624]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 545\n",
            "max reward: [118.4869]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 117\n",
            "max reward: [117.97961]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 651\n",
            "max reward: [117.844734]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 98\n",
            "max reward: [117.90254]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 99\n",
            "max reward: [118.019936]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 125\n",
            "max reward: [117.97516]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 268\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 207\n",
            "max reward: [117.9047]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.091536]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 351.7\n",
            "Average max reward: [107.67497]\n",
            " \n",
            "episode: 71 epsilon: 0.5318730129169419\n",
            "duration: 1002\n",
            "max reward: [15.914774]\n",
            "result: timeout\n",
            "total steps: 63602 \n",
            "\n",
            "episode: 72 epsilon: 0.5265702565036426\n",
            "duration: 1002\n",
            "max reward: [11.512953]\n",
            "result: timeout\n",
            "total steps: 64604 \n",
            "\n",
            "episode: 73 epsilon: 0.5213203683970555\n",
            "duration: 1002\n",
            "max reward: [3.701983]\n",
            "result: timeout\n",
            "total steps: 65606 \n",
            "\n",
            "episode: 74 epsilon: 0.5161228215019045\n",
            "duration: 1002\n",
            "max reward: [2.6632566]\n",
            "result: timeout\n",
            "total steps: 66608 \n",
            "\n",
            "episode: 75 epsilon: 0.5109770939780365\n",
            "duration: 1002\n",
            "max reward: [4.1190248]\n",
            "result: timeout\n",
            "total steps: 67610 \n",
            "\n",
            "episode: 76 epsilon: 0.505882669188027\n",
            "duration: 352\n",
            "max reward: [118.04196]\n",
            "result: coin\n",
            "total steps: 67962 \n",
            "\n",
            "episode: 77 epsilon: 0.5041050925627524\n",
            "duration: 292\n",
            "max reward: [118.08371]\n",
            "result: coin\n",
            "total steps: 68254 \n",
            "\n",
            "episode: 78 epsilon: 0.5026352527030344\n",
            "duration: 528\n",
            "max reward: [118.80905]\n",
            "result: coin\n",
            "total steps: 68782 \n",
            "\n",
            "episode: 79 epsilon: 0.49998833258719017\n",
            "duration: 1002\n",
            "max reward: [13.6684265]\n",
            "result: timeout\n",
            "total steps: 69784 \n",
            "\n",
            "episode: 80 epsilon: 0.4950034653861623\n",
            "duration: 1002\n",
            "max reward: [14.537193]\n",
            "result: timeout\n",
            "total steps: 70786 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.405575]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.059101]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.972754]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.343189]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.19441]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.880455]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.103424]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.2527485]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [11.621165]\n",
            " \n",
            "episode: 81 epsilon: 0.49006829714687483\n",
            "duration: 1002\n",
            "max reward: [3.5563898]\n",
            "result: timeout\n",
            "total steps: 71788 \n",
            "\n",
            "episode: 82 epsilon: 0.48518233237231667\n",
            "duration: 1002\n",
            "max reward: [13.004169]\n",
            "result: timeout\n",
            "total steps: 72790 \n",
            "\n",
            "episode: 83 epsilon: 0.48034508050556596\n",
            "duration: 480\n",
            "max reward: [118.0121]\n",
            "result: coin\n",
            "total steps: 73270 \n",
            "\n",
            "episode: 84 epsilon: 0.47804494885136045\n",
            "duration: 417\n",
            "max reward: [118.88529]\n",
            "result: coin\n",
            "total steps: 73687 \n",
            "\n",
            "episode: 85 epsilon: 0.4760556519812639\n",
            "duration: 414\n",
            "max reward: [117.96474]\n",
            "result: coin\n",
            "total steps: 74101 \n",
            "\n",
            "episode: 86 epsilon: 0.4740888556596217\n",
            "duration: 342\n",
            "max reward: [118.071945]\n",
            "result: coin\n",
            "total steps: 74443 \n",
            "\n",
            "episode: 87 epsilon: 0.47247024118168623\n",
            "duration: 383\n",
            "max reward: [118.84505]\n",
            "result: coin\n",
            "total steps: 74826 \n",
            "\n",
            "episode: 88 epsilon: 0.47066414104750864\n",
            "duration: 99\n",
            "max reward: [118.502686]\n",
            "result: coin\n",
            "total steps: 74925 \n",
            "\n",
            "episode: 89 epsilon: 0.47019841412073865\n",
            "duration: 609\n",
            "max reward: [118.85358]\n",
            "result: coin\n",
            "total steps: 75534 \n",
            "\n",
            "episode: 90 epsilon: 0.46734360748821346\n",
            "duration: 194\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 75728 \n",
            "\n",
            "Evaluating...\n",
            "duration: 239\n",
            "max reward: [118.46797]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 276\n",
            "max reward: [118.10051]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 178\n",
            "max reward: [118.69972]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 98\n",
            "max reward: [118.55122]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 105\n",
            "max reward: [118.66396]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 297\n",
            "max reward: [118.594475]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 200\n",
            "max reward: [118.89708]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 133\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 301\n",
            "max reward: [118.10051]\n",
            "result: coin \n",
            "\n",
            "Average duration: 189.3\n",
            "Average max reward: [118.60753]\n",
            "Saving medium.model.90 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 91 epsilon: 0.4664378397684534\n",
            "duration: 84\n",
            "max reward: [118.076035]\n",
            "result: coin\n",
            "total steps: 75812 \n",
            "\n",
            "episode: 92 epsilon: 0.4660461964962509\n",
            "duration: 313\n",
            "max reward: [118.25232]\n",
            "result: coin\n",
            "total steps: 76125 \n",
            "\n",
            "episode: 93 epsilon: 0.4645897524252416\n",
            "duration: 405\n",
            "max reward: [118.259995]\n",
            "result: coin\n",
            "total steps: 76530 \n",
            "\n",
            "episode: 94 epsilon: 0.4627119690060378\n",
            "duration: 1002\n",
            "max reward: [16.355556]\n",
            "result: timeout\n",
            "total steps: 77532 \n",
            "\n",
            "episode: 95 epsilon: 0.4580987459216391\n",
            "duration: 1002\n",
            "max reward: [8.15587]\n",
            "result: timeout\n",
            "total steps: 78534 \n",
            "\n",
            "episode: 96 epsilon: 0.45353151651938384\n",
            "duration: 403\n",
            "max reward: [118.94768]\n",
            "result: coin\n",
            "total steps: 78937 \n",
            "\n",
            "episode: 97 epsilon: 0.451707462445459\n",
            "duration: 431\n",
            "max reward: [118.1577]\n",
            "result: coin\n",
            "total steps: 79368 \n",
            "\n",
            "episode: 98 epsilon: 0.4497647927427963\n",
            "duration: 589\n",
            "max reward: [118.637955]\n",
            "result: coin\n",
            "total steps: 79957 \n",
            "\n",
            "episode: 99 epsilon: 0.44712346446142753\n",
            "duration: 241\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 80198 \n",
            "\n",
            "episode: 100 epsilon: 0.4460471943384972\n",
            "duration: 374\n",
            "max reward: [118.486855]\n",
            "result: coin\n",
            "total steps: 80572 \n",
            "\n",
            "Evaluating...\n",
            "duration: 170\n",
            "max reward: [117.93405]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 111\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 323\n",
            "max reward: [117.85393]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 150\n",
            "max reward: [117.9596]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 127\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 201\n",
            "max reward: [118.55284]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 344\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 519\n",
            "max reward: [118.583084]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 629\n",
            "max reward: [118.061485]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 467\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 304.1\n",
            "Average max reward: [118.49449]\n",
            " \n",
            "episode: 101 epsilon: 0.4443820935111151\n",
            "duration: 130\n",
            "max reward: [117.96373]\n",
            "result: coin\n",
            "total steps: 80702 \n",
            "\n",
            "episode: 102 epsilon: 0.44380477212975455\n",
            "duration: 182\n",
            "max reward: [118.94267]\n",
            "result: coin\n",
            "total steps: 80884 \n",
            "\n",
            "episode: 103 epsilon: 0.442997782028227\n",
            "duration: 122\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 81006 \n",
            "\n",
            "episode: 104 epsilon: 0.44245765427907335\n",
            "duration: 272\n",
            "max reward: [117.85603]\n",
            "result: coin\n",
            "total steps: 81278 \n",
            "\n",
            "episode: 105 epsilon: 0.4412558047158205\n",
            "duration: 156\n",
            "max reward: [118.07434]\n",
            "result: coin\n",
            "total steps: 81434 \n",
            "\n",
            "episode: 106 epsilon: 0.44056798230143746\n",
            "duration: 146\n",
            "max reward: [118.24019]\n",
            "result: coin\n",
            "total steps: 81580 \n",
            "\n",
            "episode: 107 epsilon: 0.43992522237619835\n",
            "duration: 439\n",
            "max reward: [118.004715]\n",
            "result: coin\n",
            "total steps: 82019 \n",
            "\n",
            "episode: 108 epsilon: 0.4379981835949311\n",
            "duration: 121\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 82140 \n",
            "\n",
            "episode: 109 epsilon: 0.4374685263000672\n",
            "duration: 154\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 82294 \n",
            "\n",
            "episode: 110 epsilon: 0.43679534325355435\n",
            "duration: 1002\n",
            "max reward: [14.214989]\n",
            "result: timeout\n",
            "total steps: 83296 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8454621]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.86074996]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7621634]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.2586803]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39674902]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.99861145]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.7355874]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7293246]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.9609704]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.4548298]\n",
            " \n",
            "episode: 111 epsilon: 0.4324405080739421\n",
            "duration: 1002\n",
            "max reward: [14.060785]\n",
            "result: timeout\n",
            "total steps: 84298 \n",
            "\n",
            "episode: 112 epsilon: 0.4281290904575766\n",
            "duration: 449\n",
            "max reward: [118.912674]\n",
            "result: coin\n",
            "total steps: 84747 \n",
            "\n",
            "episode: 113 epsilon: 0.4262110999523453\n",
            "duration: 1002\n",
            "max reward: [15.814659]\n",
            "result: timeout\n",
            "total steps: 85749 \n",
            "\n",
            "episode: 114 epsilon: 0.4219617893296899\n",
            "duration: 1002\n",
            "max reward: [3.8624573]\n",
            "result: timeout\n",
            "total steps: 86751 \n",
            "\n",
            "episode: 115 epsilon: 0.41775484419392567\n",
            "duration: 1002\n",
            "max reward: [1.4678438]\n",
            "result: timeout\n",
            "total steps: 87753 \n",
            "\n",
            "episode: 116 epsilon: 0.4135898421625439\n",
            "duration: 1002\n",
            "max reward: [14.995802]\n",
            "result: timeout\n",
            "total steps: 88755 \n",
            "\n",
            "episode: 117 epsilon: 0.40946636506417616\n",
            "duration: 805\n",
            "max reward: [118.083885]\n",
            "result: coin\n",
            "total steps: 89560 \n",
            "\n",
            "episode: 118 epsilon: 0.40618339251862196\n",
            "duration: 576\n",
            "max reward: [118.93815]\n",
            "result: coin\n",
            "total steps: 90136 \n",
            "\n",
            "episode: 119 epsilon: 0.403850501354242\n",
            "duration: 659\n",
            "max reward: [118.00634]\n",
            "result: coin\n",
            "total steps: 90795 \n",
            "\n",
            "episode: 120 epsilon: 0.4011978765489154\n",
            "duration: 1002\n",
            "max reward: [15.233973]\n",
            "result: timeout\n",
            "total steps: 91797 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 359\n",
            "max reward: [118.069824]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.103729]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.030001]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 177\n",
            "max reward: [118.130714]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 674\n",
            "max reward: [118.16281]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 425\n",
            "max reward: [118.02217]\n",
            "result: coin \n",
            "\n",
            "Average duration: 764.7\n",
            "Average max reward: [52.85192]\n",
            " \n",
            "episode: 121 epsilon: 0.39719794693939403\n",
            "duration: 642\n",
            "max reward: [118.017136]\n",
            "result: coin\n",
            "total steps: 92439 \n",
            "\n",
            "episode: 122 epsilon: 0.3946561041658077\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 93441 \n",
            "\n",
            "episode: 123 epsilon: 0.39072139581139126\n",
            "duration: 556\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 93997 \n",
            "\n",
            "episode: 124 epsilon: 0.3885550129758509\n",
            "duration: 1002\n",
            "max reward: [12.116426]\n",
            "result: timeout\n",
            "total steps: 94999 \n",
            "\n",
            "episode: 125 epsilon: 0.3846811322995643\n",
            "duration: 302\n",
            "max reward: [118.86046]\n",
            "result: coin\n",
            "total steps: 95301 \n",
            "\n",
            "episode: 126 epsilon: 0.38352114773833385\n",
            "duration: 423\n",
            "max reward: [117.97915]\n",
            "result: coin\n",
            "total steps: 95724 \n",
            "\n",
            "episode: 127 epsilon: 0.38190227960335926\n",
            "duration: 213\n",
            "max reward: [118.01927]\n",
            "result: coin\n",
            "total steps: 95937 \n",
            "\n",
            "episode: 128 epsilon: 0.3810896934592661\n",
            "duration: 342\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 96279 \n",
            "\n",
            "episode: 129 epsilon: 0.3797885928578463\n",
            "duration: 864\n",
            "max reward: [117.901596]\n",
            "result: coin\n",
            "total steps: 97143 \n",
            "\n",
            "episode: 130 epsilon: 0.3765213542113204\n",
            "duration: 375\n",
            "max reward: [118.91925]\n",
            "result: coin\n",
            "total steps: 97518 \n",
            "\n",
            "Evaluating...\n",
            "duration: 202\n",
            "max reward: [117.9827]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 228\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 111\n",
            "max reward: [118.97177]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 156\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 169\n",
            "max reward: [118.74062]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 239\n",
            "max reward: [118.86484]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 77\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 181\n",
            "max reward: [118.70614]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 252\n",
            "max reward: [117.84016]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 132\n",
            "max reward: [118.829216]\n",
            "result: coin \n",
            "\n",
            "Average duration: 174.7\n",
            "Average max reward: [118.69054]\n",
            "Saving medium.model.130 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 131 epsilon: 0.37511204324263014\n",
            "duration: 300\n",
            "max reward: [118.46417]\n",
            "result: coin\n",
            "total steps: 97818 \n",
            "\n",
            "episode: 132 epsilon: 0.3739883934303579\n",
            "duration: 305\n",
            "max reward: [117.87993]\n",
            "result: coin\n",
            "total steps: 98123 \n",
            "\n",
            "episode: 133 epsilon: 0.3728494665767525\n",
            "duration: 1002\n",
            "max reward: [13.558334]\n",
            "result: timeout\n",
            "total steps: 99125 \n",
            "\n",
            "episode: 134 epsilon: 0.36913216968055923\n",
            "duration: 1002\n",
            "max reward: [14.475771]\n",
            "result: timeout\n",
            "total steps: 100127 \n",
            "\n",
            "episode: 135 epsilon: 0.3654519341119343\n",
            "duration: 1002\n",
            "max reward: [4.6486835]\n",
            "result: timeout\n",
            "total steps: 101129 \n",
            "\n",
            "episode: 136 epsilon: 0.36180839037066287\n",
            "duration: 208\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 101337 \n",
            "\n",
            "episode: 137 epsilon: 0.36105661104023706\n",
            "duration: 82\n",
            "max reward: [118.09129]\n",
            "result: coin\n",
            "total steps: 101419 \n",
            "\n",
            "episode: 138 epsilon: 0.36076066597324424\n",
            "duration: 127\n",
            "max reward: [118.45923]\n",
            "result: coin\n",
            "total steps: 101546 \n",
            "\n",
            "episode: 139 epsilon: 0.3603027907397738\n",
            "duration: 132\n",
            "max reward: [118.20654]\n",
            "result: coin\n",
            "total steps: 101678 \n",
            "\n",
            "episode: 140 epsilon: 0.35982750481371994\n",
            "duration: 1002\n",
            "max reward: [1.9994302]\n",
            "result: timeout\n",
            "total steps: 102680 \n",
            "\n",
            "Evaluating...\n",
            "duration: 251\n",
            "max reward: [118.19316]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7556398]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0732725]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0191128]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.1490643]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.963131]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.125139]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.96492386]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0587368]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 926.9\n",
            "Average max reward: [16.830217]\n",
            " \n",
            "episode: 141 epsilon: 0.356240036447224\n",
            "duration: 607\n",
            "max reward: [117.91241]\n",
            "result: coin\n",
            "total steps: 103287 \n",
            "\n",
            "episode: 142 epsilon: 0.354084208981614\n",
            "duration: 1002\n",
            "max reward: [13.379541]\n",
            "result: timeout\n",
            "total steps: 104289 \n",
            "\n",
            "episode: 143 epsilon: 0.35055400108531964\n",
            "duration: 1002\n",
            "max reward: [9.668022]\n",
            "result: timeout\n",
            "total steps: 105291 \n",
            "\n",
            "episode: 144 epsilon: 0.3470589892454293\n",
            "duration: 1002\n",
            "max reward: [8.079556]\n",
            "result: timeout\n",
            "total steps: 106293 \n",
            "\n",
            "episode: 145 epsilon: 0.3435988225584201\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 107295 \n",
            "\n",
            "episode: 146 epsilon: 0.3401731536192663\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 108297 \n",
            "\n",
            "episode: 147 epsilon: 0.33678163848655834\n",
            "duration: 1002\n",
            "max reward: [7.3521824]\n",
            "result: timeout\n",
            "total steps: 109299 \n",
            "\n",
            "episode: 148 epsilon: 0.33342393664797126\n",
            "duration: 384\n",
            "max reward: [117.98705]\n",
            "result: coin\n",
            "total steps: 109683 \n",
            "\n",
            "episode: 149 epsilon: 0.3321460438556785\n",
            "duration: 155\n",
            "max reward: [118.58543]\n",
            "result: coin\n",
            "total steps: 109838 \n",
            "\n",
            "episode: 150 epsilon: 0.3316316162720722\n",
            "duration: 180\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 110018 \n",
            "\n",
            "Evaluating...\n",
            "duration: 225\n",
            "max reward: [118.910904]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 135\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 677\n",
            "max reward: [118.910904]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 131\n",
            "max reward: [118.01973]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [118.967575]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 237\n",
            "max reward: [118.9409]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 112\n",
            "max reward: [118.910904]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 226\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 99\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 139\n",
            "max reward: [118.910904]\n",
            "result: coin \n",
            "\n",
            "Average duration: 207.5\n",
            "Average max reward: [118.84818]\n",
            " \n",
            "episode: 151 epsilon: 0.3310352162837999\n",
            "duration: 191\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 110209 \n",
            "\n",
            "episode: 152 epsilon: 0.33040354246123244\n",
            "duration: 242\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 110451 \n",
            "\n",
            "episode: 153 epsilon: 0.32960493259616125\n",
            "duration: 129\n",
            "max reward: [118.21466]\n",
            "result: coin\n",
            "total steps: 110580 \n",
            "\n",
            "episode: 154 epsilon: 0.32918001636300787\n",
            "duration: 199\n",
            "max reward: [118.90541]\n",
            "result: coin\n",
            "total steps: 110779 \n",
            "\n",
            "episode: 155 epsilon: 0.328525599491196\n",
            "duration: 163\n",
            "max reward: [118.68454]\n",
            "result: coin\n",
            "total steps: 110942 \n",
            "\n",
            "episode: 156 epsilon: 0.32799053895682767\n",
            "duration: 406\n",
            "max reward: [118.12732]\n",
            "result: coin\n",
            "total steps: 111348 \n",
            "\n",
            "episode: 157 epsilon: 0.32666159694642255\n",
            "duration: 223\n",
            "max reward: [118.04325]\n",
            "result: coin\n",
            "total steps: 111571 \n",
            "\n",
            "episode: 158 epsilon: 0.3259339532095403\n",
            "duration: 356\n",
            "max reward: [118.02954]\n",
            "result: coin\n",
            "total steps: 111927 \n",
            "\n",
            "episode: 159 epsilon: 0.3247756912656532\n",
            "duration: 1002\n",
            "max reward: [15.896212]\n",
            "result: timeout\n",
            "total steps: 112929 \n",
            "\n",
            "episode: 160 epsilon: 0.32153768832525664\n",
            "duration: 191\n",
            "max reward: [117.91548]\n",
            "result: coin\n",
            "total steps: 113120 \n",
            "\n",
            "Evaluating...\n",
            "duration: 150\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 157\n",
            "max reward: [117.962036]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 114\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 549\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 355\n",
            "max reward: [117.98691]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 165\n",
            "max reward: [118.01145]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 85\n",
            "max reward: [118.075806]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 119\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 438\n",
            "max reward: [117.98729]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 193\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 232.5\n",
            "Average max reward: [118.50236]\n",
            " \n",
            "episode: 161 epsilon: 0.3209241374681485\n",
            "duration: 109\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 113229 \n",
            "\n",
            "episode: 162 epsilon: 0.32057452073404324\n",
            "duration: 113\n",
            "max reward: [118.42355]\n",
            "result: coin\n",
            "total steps: 113342 \n",
            "\n",
            "episode: 163 epsilon: 0.32021247611934567\n",
            "duration: 1002\n",
            "max reward: [13.299081]\n",
            "result: timeout\n",
            "total steps: 114344 \n",
            "\n",
            "episode: 164 epsilon: 0.3170199682835975\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 115346 \n",
            "\n",
            "episode: 165 epsilon: 0.3138592896457771\n",
            "duration: 1002\n",
            "max reward: [17.028385]\n",
            "result: timeout\n",
            "total steps: 116348 \n",
            "\n",
            "episode: 166 epsilon: 0.31073012286982976\n",
            "duration: 274\n",
            "max reward: [118.96211]\n",
            "result: coin\n",
            "total steps: 116622 \n",
            "\n",
            "episode: 167 epsilon: 0.30987988768730185\n",
            "duration: 205\n",
            "max reward: [117.96608]\n",
            "result: coin\n",
            "total steps: 116827 \n",
            "\n",
            "episode: 168 epsilon: 0.30924528460794254\n",
            "duration: 1002\n",
            "max reward: [13.812239]\n",
            "result: timeout\n",
            "total steps: 117829 \n",
            "\n",
            "episode: 169 epsilon: 0.3061621193101886\n",
            "duration: 950\n",
            "max reward: [118.35947]\n",
            "result: coin\n",
            "total steps: 118779 \n",
            "\n",
            "episode: 170 epsilon: 0.303267351096792\n",
            "duration: 101\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 118880 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 710\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 741\n",
            "max reward: [118.51822]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.152598]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.7412605]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.31053]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.267704]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9431877]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 976\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.417221]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 944.1\n",
            "Average max reward: [40.83507]\n",
            " \n",
            "episode: 171 epsilon: 0.3029612057016337\n",
            "duration: 270\n",
            "max reward: [118.353775]\n",
            "result: coin\n",
            "total steps: 119150 \n",
            "\n",
            "episode: 172 epsilon: 0.3021443137466403\n",
            "duration: 1002\n",
            "max reward: [16.09822]\n",
            "result: timeout\n",
            "total steps: 120152 \n",
            "\n",
            "episode: 173 epsilon: 0.29913194489439304\n",
            "duration: 830\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 120982 \n",
            "\n",
            "episode: 174 epsilon: 0.29665942490403846\n",
            "duration: 608\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 121590 \n",
            "\n",
            "episode: 175 epsilon: 0.2948612077203249\n",
            "duration: 173\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 121763 \n",
            "\n",
            "episode: 176 epsilon: 0.29435153882168175\n",
            "duration: 1002\n",
            "max reward: [12.674456]\n",
            "result: timeout\n",
            "total steps: 122765 \n",
            "\n",
            "episode: 177 epsilon: 0.2914168636786605\n",
            "duration: 1002\n",
            "max reward: [16.597382]\n",
            "result: timeout\n",
            "total steps: 123767 \n",
            "\n",
            "episode: 178 epsilon: 0.2885114471501162\n",
            "duration: 76\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 123843 \n",
            "\n",
            "episode: 179 epsilon: 0.2882922617512837\n",
            "duration: 164\n",
            "max reward: [117.81862]\n",
            "result: coin\n",
            "total steps: 124007 \n",
            "\n",
            "episode: 180 epsilon: 0.2878198499255919\n",
            "duration: 557\n",
            "max reward: [117.91626]\n",
            "result: coin\n",
            "total steps: 124564 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.419655]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.168211]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.490736]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.262424]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.973064]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.654587]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.711182]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [13.267987]\n",
            " \n",
            "episode: 181 epsilon: 0.2862211498744391\n",
            "duration: 655\n",
            "max reward: [118.747574]\n",
            "result: coin\n",
            "total steps: 125219 \n",
            "\n",
            "episode: 182 epsilon: 0.284352527760892\n",
            "duration: 307\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 125526 \n",
            "\n",
            "episode: 183 epsilon: 0.2834809041275236\n",
            "duration: 361\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 125887 \n",
            "\n",
            "episode: 184 epsilon: 0.282459383018605\n",
            "duration: 219\n",
            "max reward: [118.49805]\n",
            "result: coin\n",
            "total steps: 126106 \n",
            "\n",
            "episode: 185 epsilon: 0.2818414738273216\n",
            "duration: 119\n",
            "max reward: [118.811775]\n",
            "result: coin\n",
            "total steps: 126225 \n",
            "\n",
            "episode: 186 epsilon: 0.28150628195218824\n",
            "duration: 365\n",
            "max reward: [117.97798]\n",
            "result: coin\n",
            "total steps: 126590 \n",
            "\n",
            "episode: 187 epsilon: 0.28048065692739016\n",
            "duration: 359\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 126949 \n",
            "\n",
            "episode: 188 epsilon: 0.279475536639445\n",
            "duration: 1002\n",
            "max reward: [17.805813]\n",
            "result: timeout\n",
            "total steps: 127951 \n",
            "\n",
            "episode: 189 epsilon: 0.2766891747480089\n",
            "duration: 275\n",
            "max reward: [118.40645]\n",
            "result: coin\n",
            "total steps: 128226 \n",
            "\n",
            "episode: 190 epsilon: 0.27592932479000787\n",
            "duration: 183\n",
            "max reward: [118.30858]\n",
            "result: coin\n",
            "total steps: 128409 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.094413]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.016197]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6518154]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.892803]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4881322]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.500672]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.147967]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.484111]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.3891387]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.6377635]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [3.7303014]\n",
            " \n",
            "episode: 191 epsilon: 0.2754248358737907\n",
            "duration: 274\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 128683 \n",
            "\n",
            "episode: 192 epsilon: 0.27467120476960594\n",
            "duration: 837\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 129520 \n",
            "\n",
            "episode: 193 epsilon: 0.27238180130487777\n",
            "duration: 203\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 129723 \n",
            "\n",
            "episode: 194 epsilon: 0.271829427097739\n",
            "duration: 262\n",
            "max reward: [118.99763]\n",
            "result: coin\n",
            "total steps: 129985 \n",
            "\n",
            "episode: 195 epsilon: 0.2711181661574396\n",
            "duration: 1002\n",
            "max reward: [15.763334]\n",
            "result: timeout\n",
            "total steps: 130987 \n",
            "\n",
            "episode: 196 epsilon: 0.26841512697432957\n",
            "duration: 672\n",
            "max reward: [118.43057]\n",
            "result: coin\n",
            "total steps: 131659 \n",
            "\n",
            "episode: 197 epsilon: 0.26661742436693225\n",
            "duration: 798\n",
            "max reward: [118.14371]\n",
            "result: coin\n",
            "total steps: 132457 \n",
            "\n",
            "episode: 198 epsilon: 0.26449828393643243\n",
            "duration: 919\n",
            "max reward: [118.07437]\n",
            "result: coin\n",
            "total steps: 133376 \n",
            "\n",
            "episode: 199 epsilon: 0.2620786798171547\n",
            "duration: 756\n",
            "max reward: [118.58115]\n",
            "result: coin\n",
            "total steps: 134132 \n",
            "\n",
            "episode: 200 epsilon: 0.26010483553015845\n",
            "duration: 300\n",
            "max reward: [118.353874]\n",
            "result: coin\n",
            "total steps: 134432 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.715498]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.509885]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.715498]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.562886]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.052385]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.927618]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.248726]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [9.87325]\n",
            " \n",
            "episode: 201 epsilon: 0.25932569032573344\n",
            "duration: 654\n",
            "max reward: [118.13661]\n",
            "result: coin\n",
            "total steps: 135086 \n",
            "\n",
            "episode: 202 epsilon: 0.2576352341280583\n",
            "duration: 846\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 135932 \n",
            "\n",
            "episode: 203 epsilon: 0.25546483378568646\n",
            "duration: 457\n",
            "max reward: [118.97344]\n",
            "result: coin\n",
            "total steps: 136389 \n",
            "\n",
            "episode: 204 epsilon: 0.2543000231149141\n",
            "duration: 176\n",
            "max reward: [118.897446]\n",
            "result: coin\n",
            "total steps: 136565 \n",
            "\n",
            "episode: 205 epsilon: 0.2538528487031448\n",
            "duration: 141\n",
            "max reward: [118.73887]\n",
            "result: coin\n",
            "total steps: 136706 \n",
            "\n",
            "episode: 206 epsilon: 0.2534951684103385\n",
            "duration: 194\n",
            "max reward: [118.85358]\n",
            "result: coin\n",
            "total steps: 136900 \n",
            "\n",
            "episode: 207 epsilon: 0.2530038645025023\n",
            "duration: 535\n",
            "max reward: [118.479904]\n",
            "result: coin\n",
            "total steps: 137435 \n",
            "\n",
            "episode: 208 epsilon: 0.2516539081805008\n",
            "duration: 227\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 137662 \n",
            "\n",
            "episode: 209 epsilon: 0.2510833016923183\n",
            "duration: 286\n",
            "max reward: [118.05316]\n",
            "result: coin\n",
            "total steps: 137948 \n",
            "\n",
            "episode: 210 epsilon: 0.25036622935170566\n",
            "duration: 309\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 138257 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.425703]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1234567]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5483975]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.406423]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.2846856]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.0361586]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.9591064]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.082858]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.058712]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.044112]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [5.0969615]\n",
            " \n",
            "episode: 211 epsilon: 0.2495937917337379\n",
            "duration: 204\n",
            "max reward: [118.01776]\n",
            "result: coin\n",
            "total steps: 138461 \n",
            "\n",
            "episode: 212 epsilon: 0.2490851394003817\n",
            "duration: 259\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 138720 \n",
            "\n",
            "episode: 213 epsilon: 0.24844084361254662\n",
            "duration: 198\n",
            "max reward: [118.52371]\n",
            "result: coin\n",
            "total steps: 138918 \n",
            "\n",
            "episode: 214 epsilon: 0.24794941741467863\n",
            "duration: 150\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 139068 \n",
            "\n",
            "episode: 215 epsilon: 0.24757777209223192\n",
            "duration: 1002\n",
            "max reward: [17.045343]\n",
            "result: timeout\n",
            "total steps: 140070 \n",
            "\n",
            "episode: 216 epsilon: 0.2451094298622842\n",
            "duration: 360\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 140430 \n",
            "\n",
            "episode: 217 epsilon: 0.2442286223196287\n",
            "duration: 1002\n",
            "max reward: [17.221416]\n",
            "result: timeout\n",
            "total steps: 141432 \n",
            "\n",
            "episode: 218 epsilon: 0.24179367100255758\n",
            "duration: 593\n",
            "max reward: [118.82358]\n",
            "result: coin\n",
            "total steps: 142025 \n",
            "\n",
            "episode: 219 epsilon: 0.24036407746763386\n",
            "duration: 156\n",
            "max reward: [118.52292]\n",
            "result: coin\n",
            "total steps: 142181 \n",
            "\n",
            "episode: 220 epsilon: 0.23998940182976608\n",
            "duration: 255\n",
            "max reward: [118.05864]\n",
            "result: coin\n",
            "total steps: 142436 \n",
            "\n",
            "Evaluating...\n",
            "duration: 769\n",
            "max reward: [117.78038]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.000746]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 503\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.36576]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.114708]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 928.8\n",
            "Average max reward: [33.72616]\n",
            " \n",
            "episode: 221 epsilon: 0.23937820845783978\n",
            "duration: 1002\n",
            "max reward: [17.111662]\n",
            "result: timeout\n",
            "total steps: 143438 \n",
            "\n",
            "episode: 222 epsilon: 0.23699161560714713\n",
            "duration: 836\n",
            "max reward: [117.982544]\n",
            "result: coin\n",
            "total steps: 144274 \n",
            "\n",
            "episode: 223 epsilon: 0.23501862429530598\n",
            "duration: 1002\n",
            "max reward: [16.723248]\n",
            "result: timeout\n",
            "total steps: 145276 \n",
            "\n",
            "episode: 224 epsilon: 0.23267549635506343\n",
            "duration: 447\n",
            "max reward: [118.910904]\n",
            "result: coin\n",
            "total steps: 145723 \n",
            "\n",
            "episode: 225 epsilon: 0.23163775795958183\n",
            "duration: 339\n",
            "max reward: [118.76797]\n",
            "result: coin\n",
            "total steps: 146062 \n",
            "\n",
            "episode: 226 epsilon: 0.23085383545947932\n",
            "duration: 72\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 146134 \n",
            "\n",
            "episode: 227 epsilon: 0.23068768052090433\n",
            "duration: 85\n",
            "max reward: [117.99216]\n",
            "result: coin\n",
            "total steps: 146219 \n",
            "\n",
            "episode: 228 epsilon: 0.23049167930477935\n",
            "duration: 230\n",
            "max reward: [118.7509]\n",
            "result: coin\n",
            "total steps: 146449 \n",
            "\n",
            "episode: 229 epsilon: 0.22996215762574\n",
            "duration: 111\n",
            "max reward: [118.82358]\n",
            "result: coin\n",
            "total steps: 146560 \n",
            "\n",
            "episode: 230 epsilon: 0.22970704124655994\n",
            "duration: 104\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 146664 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.7739162]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.9485703]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.173286]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.5820446]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.6017246]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.55766]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.9719076]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.3859396]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.415077]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [6.5906553]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [6.0000787]\n",
            " \n",
            "episode: 231 epsilon: 0.22946827010617776\n",
            "duration: 320\n",
            "max reward: [118.1371]\n",
            "result: coin\n",
            "total steps: 146984 \n",
            "\n",
            "episode: 232 epsilon: 0.22873514526718014\n",
            "duration: 180\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 147164 \n",
            "\n",
            "episode: 233 epsilon: 0.22832379233440403\n",
            "duration: 275\n",
            "max reward: [118.82358]\n",
            "result: coin\n",
            "total steps: 147439 \n",
            "\n",
            "episode: 234 epsilon: 0.22769676446396442\n",
            "duration: 83\n",
            "max reward: [118.52014]\n",
            "result: coin\n",
            "total steps: 147522 \n",
            "\n",
            "episode: 235 epsilon: 0.22750785455791528\n",
            "duration: 398\n",
            "max reward: [118.70886]\n",
            "result: coin\n",
            "total steps: 147920 \n",
            "\n",
            "episode: 236 epsilon: 0.22660417281633027\n",
            "duration: 1002\n",
            "max reward: [16.25739]\n",
            "result: timeout\n",
            "total steps: 148922 \n",
            "\n",
            "episode: 237 epsilon: 0.2243449366800726\n",
            "duration: 1002\n",
            "max reward: [17.340572]\n",
            "result: timeout\n",
            "total steps: 149924 \n",
            "\n",
            "episode: 238 epsilon: 0.22210822505365052\n",
            "duration: 1002\n",
            "max reward: [10.352694]\n",
            "result: timeout\n",
            "total steps: 150926 \n",
            "\n",
            "episode: 239 epsilon: 0.21989381336844313\n",
            "duration: 1002\n",
            "max reward: [10.629538]\n",
            "result: timeout\n",
            "total steps: 151928 \n",
            "\n",
            "episode: 240 epsilon: 0.21770147929477118\n",
            "duration: 665\n",
            "max reward: [118.503525]\n",
            "result: coin\n",
            "total steps: 152593 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.610883]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.1711142]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6603374]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.288165]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.19999981]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4558215]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3520436]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.504364]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.49999952]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.53920007]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.2281929]\n",
            " \n",
            "episode: 241 epsilon: 0.21625856745674918\n",
            "duration: 1002\n",
            "max reward: [16.379875]\n",
            "result: timeout\n",
            "total steps: 153595 \n",
            "\n",
            "episode: 242 epsilon: 0.2141024766650337\n",
            "duration: 1002\n",
            "max reward: [16.638222]\n",
            "result: timeout\n",
            "total steps: 154597 \n",
            "\n",
            "episode: 243 epsilon: 0.21196788202746733\n",
            "duration: 1002\n",
            "max reward: [16.435322]\n",
            "result: timeout\n",
            "total steps: 155599 \n",
            "\n",
            "episode: 244 epsilon: 0.20985456922810153\n",
            "duration: 1002\n",
            "max reward: [15.8328705]\n",
            "result: timeout\n",
            "total steps: 156601 \n",
            "\n",
            "episode: 245 epsilon: 0.20776232608771072\n",
            "duration: 690\n",
            "max reward: [118.78373]\n",
            "result: coin\n",
            "total steps: 157291 \n",
            "\n",
            "episode: 246 epsilon: 0.20633370046417435\n",
            "duration: 211\n",
            "max reward: [118.22384]\n",
            "result: coin\n",
            "total steps: 157502 \n",
            "\n",
            "episode: 247 epsilon: 0.2058987953424518\n",
            "duration: 1002\n",
            "max reward: [17.698792]\n",
            "result: timeout\n",
            "total steps: 158504 \n",
            "\n",
            "episode: 248 epsilon: 0.20384599113735613\n",
            "duration: 595\n",
            "max reward: [117.97874]\n",
            "result: coin\n",
            "total steps: 159099 \n",
            "\n",
            "episode: 249 epsilon: 0.20263670867305328\n",
            "duration: 561\n",
            "max reward: [118.02084]\n",
            "result: coin\n",
            "total steps: 159660 \n",
            "\n",
            "episode: 250 epsilon: 0.20150309948425893\n",
            "duration: 498\n",
            "max reward: [118.17872]\n",
            "result: coin\n",
            "total steps: 160158 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2612777]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.273471]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.143383]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.173471]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4593875]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [4.231099]\n",
            " \n",
            "episode: 251 epsilon: 0.20050210858491374\n",
            "duration: 168\n",
            "max reward: [118.38097]\n",
            "result: coin\n",
            "total steps: 160326 \n",
            "\n",
            "episode: 252 epsilon: 0.20016554783268206\n",
            "duration: 326\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 160652 \n",
            "\n",
            "episode: 253 epsilon: 0.19951407063155516\n",
            "duration: 213\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 160865 \n",
            "\n",
            "episode: 254 epsilon: 0.19908955792763727\n",
            "duration: 1002\n",
            "max reward: [7.840164]\n",
            "result: timeout\n",
            "total steps: 161867 \n",
            "\n",
            "episode: 255 epsilon: 0.19710464159519955\n",
            "duration: 1002\n",
            "max reward: [2.253006]\n",
            "result: timeout\n",
            "total steps: 162869 \n",
            "\n",
            "episode: 256 epsilon: 0.19513951481319222\n",
            "duration: 1002\n",
            "max reward: [2.5040934]\n",
            "result: timeout\n",
            "total steps: 163871 \n",
            "\n",
            "episode: 257 epsilon: 0.19319398028044962\n",
            "duration: 1002\n",
            "max reward: [2.2359202]\n",
            "result: timeout\n",
            "total steps: 164873 \n",
            "\n",
            "episode: 258 epsilon: 0.1912678426628921\n",
            "duration: 1002\n",
            "max reward: [7.776045]\n",
            "result: timeout\n",
            "total steps: 165875 \n",
            "\n",
            "episode: 259 epsilon: 0.18936090857391447\n",
            "duration: 1002\n",
            "max reward: [15.173443]\n",
            "result: timeout\n",
            "total steps: 166877 \n",
            "\n",
            "episode: 260 epsilon: 0.1874729865549695\n",
            "duration: 1002\n",
            "max reward: [15.676556]\n",
            "result: timeout\n",
            "total steps: 167879 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.55405]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.62583]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.970327]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.708643]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.78376]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.402412]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.175425]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.621075]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.511879]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [15.035339]\n",
            " \n",
            "episode: 261 epsilon: 0.18560388705634537\n",
            "duration: 367\n",
            "max reward: [118.910904]\n",
            "result: coin\n",
            "total steps: 168246 \n",
            "\n",
            "episode: 262 epsilon: 0.18492396920325435\n",
            "duration: 495\n",
            "max reward: [118.64803]\n",
            "result: coin\n",
            "total steps: 168741 \n",
            "\n",
            "episode: 263 epsilon: 0.1840108573719402\n",
            "duration: 137\n",
            "max reward: [118.21124]\n",
            "result: coin\n",
            "total steps: 168878 \n",
            "\n",
            "episode: 264 epsilon: 0.18375893510349725\n",
            "duration: 314\n",
            "max reward: [117.99577]\n",
            "result: coin\n",
            "total steps: 169192 \n",
            "\n",
            "episode: 265 epsilon: 0.1831828369946445\n",
            "duration: 87\n",
            "max reward: [118.06263]\n",
            "result: coin\n",
            "total steps: 169279 \n",
            "\n",
            "episode: 266 epsilon: 0.18302353723190376\n",
            "duration: 140\n",
            "max reward: [118.66194]\n",
            "result: coin\n",
            "total steps: 169419 \n",
            "\n",
            "episode: 267 epsilon: 0.18276748355917213\n",
            "duration: 219\n",
            "max reward: [118.1278]\n",
            "result: coin\n",
            "total steps: 169638 \n",
            "\n",
            "episode: 268 epsilon: 0.1823676607359681\n",
            "duration: 106\n",
            "max reward: [118.08316]\n",
            "result: coin\n",
            "total steps: 169744 \n",
            "\n",
            "episode: 269 epsilon: 0.18217445343354893\n",
            "duration: 103\n",
            "max reward: [118.9709]\n",
            "result: coin\n",
            "total steps: 169847 \n",
            "\n",
            "episode: 270 epsilon: 0.1819869103477819\n",
            "duration: 1002\n",
            "max reward: [11.610935]\n",
            "result: timeout\n",
            "total steps: 170849 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.922928]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [10.501455]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.990427]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.055483]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.800124]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.923724]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.547894]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [8.834676]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.80715]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [10.160249]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [9.4544115]\n",
            " \n",
            "episode: 271 epsilon: 0.18017250684817454\n",
            "duration: 1002\n",
            "max reward: [15.422682]\n",
            "result: timeout\n",
            "total steps: 171851 \n",
            "\n",
            "episode: 272 epsilon: 0.1783761928916727\n",
            "duration: 1002\n",
            "max reward: [14.627239]\n",
            "result: timeout\n",
            "total steps: 172853 \n",
            "\n",
            "episode: 273 epsilon: 0.17659778812612778\n",
            "duration: 1002\n",
            "max reward: [15.633228]\n",
            "result: timeout\n",
            "total steps: 173855 \n",
            "\n",
            "episode: 274 epsilon: 0.17483711399749602\n",
            "duration: 405\n",
            "max reward: [118.80536]\n",
            "result: coin\n",
            "total steps: 174260 \n",
            "\n",
            "episode: 275 epsilon: 0.17413045563490379\n",
            "duration: 416\n",
            "max reward: [118.102646]\n",
            "result: coin\n",
            "total steps: 174676 \n",
            "\n",
            "episode: 276 epsilon: 0.17340757756832734\n",
            "duration: 294\n",
            "max reward: [118.3844]\n",
            "result: coin\n",
            "total steps: 174970 \n",
            "\n",
            "episode: 277 epsilon: 0.17289850798924045\n",
            "duration: 328\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 175298 \n",
            "\n",
            "episode: 278 epsilon: 0.17233232992266234\n",
            "duration: 266\n",
            "max reward: [118.82564]\n",
            "result: coin\n",
            "total steps: 175564 \n",
            "\n",
            "episode: 279 epsilon: 0.17187453506216363\n",
            "duration: 129\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 175693 \n",
            "\n",
            "episode: 280 epsilon: 0.17165295985866663\n",
            "duration: 361\n",
            "max reward: [118.20523]\n",
            "result: coin\n",
            "total steps: 176054 \n",
            "\n",
            "Evaluating...\n",
            "duration: 267\n",
            "max reward: [117.81763]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 141\n",
            "max reward: [118.06254]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 198\n",
            "max reward: [117.95776]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 219\n",
            "max reward: [117.84354]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 264\n",
            "max reward: [118.06254]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 218\n",
            "max reward: [117.88254]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.1272144]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 132\n",
            "max reward: [118.69385]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 280\n",
            "max reward: [118.86731]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 144\n",
            "max reward: [118.06254]\n",
            "result: coin \n",
            "\n",
            "Average duration: 286.5\n",
            "Average max reward: [106.73775]\n",
            " \n",
            "episode: 281 epsilon: 0.17103440982813228\n",
            "duration: 327\n",
            "max reward: [118.73887]\n",
            "result: coin\n",
            "total steps: 176381 \n",
            "\n",
            "episode: 282 epsilon: 0.1704760407390037\n",
            "duration: 190\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 176571 \n",
            "\n",
            "episode: 283 epsilon: 0.17015244377606312\n",
            "duration: 294\n",
            "max reward: [117.984955]\n",
            "result: coin\n",
            "total steps: 176865 \n",
            "\n",
            "episode: 284 epsilon: 0.16965293023606484\n",
            "duration: 250\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 177115 \n",
            "\n",
            "episode: 285 epsilon: 0.16922932763435314\n",
            "duration: 179\n",
            "max reward: [118.77182]\n",
            "result: coin\n",
            "total steps: 177294 \n",
            "\n",
            "episode: 286 epsilon: 0.16892667809003975\n",
            "duration: 185\n",
            "max reward: [118.189354]\n",
            "result: coin\n",
            "total steps: 177479 \n",
            "\n",
            "episode: 287 epsilon: 0.1686144526331701\n",
            "duration: 159\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 177638 \n",
            "\n",
            "episode: 288 epsilon: 0.16834656867766443\n",
            "duration: 116\n",
            "max reward: [118.039764]\n",
            "result: coin\n",
            "total steps: 177754 \n",
            "\n",
            "episode: 289 epsilon: 0.16815139987778718\n",
            "duration: 254\n",
            "max reward: [118.04346]\n",
            "result: coin\n",
            "total steps: 178008 \n",
            "\n",
            "episode: 290 epsilon: 0.16772483728592352\n",
            "duration: 471\n",
            "max reward: [118.420456]\n",
            "result: coin\n",
            "total steps: 178479 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3058286]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.0780516]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.300924]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2852483]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4181297]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7738731]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.5137262]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4434195]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.277001]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3580098]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.2754211]\n",
            " \n",
            "episode: 291 epsilon: 0.16693671079717703\n",
            "duration: 1002\n",
            "max reward: [12.746046]\n",
            "result: timeout\n",
            "total steps: 179481 \n",
            "\n",
            "episode: 292 epsilon: 0.1652723572911775\n",
            "duration: 636\n",
            "max reward: [118.318436]\n",
            "result: coin\n",
            "total steps: 180117 \n",
            "\n",
            "episode: 293 epsilon: 0.16422456062411753\n",
            "duration: 268\n",
            "max reward: [118.037544]\n",
            "result: coin\n",
            "total steps: 180385 \n",
            "\n",
            "episode: 294 epsilon: 0.16378502803838463\n",
            "duration: 402\n",
            "max reward: [118.27728]\n",
            "result: coin\n",
            "total steps: 180787 \n",
            "\n",
            "episode: 295 epsilon: 0.16312793386985755\n",
            "duration: 365\n",
            "max reward: [118.33876]\n",
            "result: coin\n",
            "total steps: 181152 \n",
            "\n",
            "episode: 296 epsilon: 0.16253360222631383\n",
            "duration: 260\n",
            "max reward: [118.042435]\n",
            "result: coin\n",
            "total steps: 181412 \n",
            "\n",
            "episode: 297 epsilon: 0.16211156374829513\n",
            "duration: 216\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 181628 \n",
            "\n",
            "episode: 298 epsilon: 0.16176178067231656\n",
            "duration: 363\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 181991 \n",
            "\n",
            "episode: 299 epsilon: 0.16117564987948046\n",
            "duration: 243\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 182234 \n",
            "\n",
            "episode: 300 epsilon: 0.16078446852810582\n",
            "duration: 275\n",
            "max reward: [118.73547]\n",
            "result: coin\n",
            "total steps: 182509 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.902112]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [9.585899]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.140115]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.442103]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.30126]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.107353]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.296075]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.906938]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.335827]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.0240836]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [5.404176]\n",
            " \n",
            "episode: 301 epsilon: 0.16034291864900566\n",
            "duration: 183\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 182692 \n",
            "\n",
            "episode: 302 epsilon: 0.16004975943037641\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 183694 \n",
            "\n",
            "episode: 303 epsilon: 0.1584540686025754\n",
            "duration: 1002\n",
            "max reward: [9.014596]\n",
            "result: timeout\n",
            "total steps: 184696 \n",
            "\n",
            "episode: 304 epsilon: 0.15687428675974882\n",
            "duration: 664\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 185360 \n",
            "\n",
            "episode: 305 epsilon: 0.15583609211634294\n",
            "duration: 251\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 185611 \n",
            "\n",
            "episode: 306 epsilon: 0.15544543400615793\n",
            "duration: 504\n",
            "max reward: [118.89348]\n",
            "result: coin\n",
            "total steps: 186115 \n",
            "\n",
            "episode: 307 epsilon: 0.1546639599875174\n",
            "duration: 426\n",
            "max reward: [118.35343]\n",
            "result: coin\n",
            "total steps: 186541 \n",
            "\n",
            "episode: 308 epsilon: 0.15400649291711763\n",
            "duration: 327\n",
            "max reward: [118.88529]\n",
            "result: coin\n",
            "total steps: 186868 \n",
            "\n",
            "episode: 309 epsilon: 0.15350371417653302\n",
            "duration: 183\n",
            "max reward: [118.1956]\n",
            "result: coin\n",
            "total steps: 187051 \n",
            "\n",
            "episode: 310 epsilon: 0.15322305925716495\n",
            "duration: 977\n",
            "max reward: [118.04171]\n",
            "result: coin\n",
            "total steps: 188028 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.5437143]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.937661]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.253255]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.937661]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2251403]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.937661]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1517742]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.937661]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3888083]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.6313336]\n",
            " \n",
            "episode: 311 epsilon: 0.15173335900362625\n",
            "duration: 776\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 188804 \n",
            "\n",
            "episode: 312 epsilon: 0.15056046485279664\n",
            "duration: 238\n",
            "max reward: [117.97048]\n",
            "result: coin\n",
            "total steps: 189042 \n",
            "\n",
            "episode: 313 epsilon: 0.15020255702570562\n",
            "duration: 785\n",
            "max reward: [118.912674]\n",
            "result: coin\n",
            "total steps: 189827 \n",
            "\n",
            "episode: 314 epsilon: 0.1490280827955708\n",
            "duration: 1002\n",
            "max reward: [8.903032]\n",
            "result: timeout\n",
            "total steps: 190829 \n",
            "\n",
            "episode: 315 epsilon: 0.14754227772064904\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 191831 \n",
            "\n",
            "episode: 316 epsilon: 0.14607128607336625\n",
            "duration: 467\n",
            "max reward: [118.06557]\n",
            "result: coin\n",
            "total steps: 192298 \n",
            "\n",
            "episode: 317 epsilon: 0.14539072351783042\n",
            "duration: 1002\n",
            "max reward: [8.430655]\n",
            "result: timeout\n",
            "total steps: 193300 \n",
            "\n",
            "episode: 318 epsilon: 0.14394118279505497\n",
            "duration: 328\n",
            "max reward: [118.88256]\n",
            "result: coin\n",
            "total steps: 193628 \n",
            "\n",
            "episode: 319 epsilon: 0.14346982915803602\n",
            "duration: 105\n",
            "max reward: [117.82066]\n",
            "result: coin\n",
            "total steps: 193733 \n",
            "\n",
            "episode: 320 epsilon: 0.14331926489748997\n",
            "duration: 209\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 193942 \n",
            "\n",
            "Evaluating...\n",
            "duration: 268\n",
            "max reward: [118.02592]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 581\n",
            "max reward: [118.05611]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 606\n",
            "max reward: [117.93894]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 680\n",
            "max reward: [118.81623]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.638443]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 299\n",
            "max reward: [117.906586]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 457\n",
            "max reward: [118.26257]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 196\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.76176]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 386\n",
            "max reward: [118.045586]\n",
            "result: coin \n",
            "\n",
            "Average duration: 547.7\n",
            "Average max reward: [97.94521]\n",
            " \n",
            "episode: 321 epsilon: 0.1430200404323405\n",
            "duration: 93\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 194035 \n",
            "\n",
            "episode: 322 epsilon: 0.14288709362458618\n",
            "duration: 228\n",
            "max reward: [118.88898]\n",
            "result: coin\n",
            "total steps: 194263 \n",
            "\n",
            "episode: 323 epsilon: 0.14256168216115866\n",
            "duration: 191\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 194454 \n",
            "\n",
            "episode: 324 epsilon: 0.14228964922238763\n",
            "duration: 621\n",
            "max reward: [118.01826]\n",
            "result: coin\n",
            "total steps: 195075 \n",
            "\n",
            "episode: 325 epsilon: 0.14140876846632683\n",
            "duration: 314\n",
            "max reward: [118.04416]\n",
            "result: coin\n",
            "total steps: 195389 \n",
            "\n",
            "episode: 326 epsilon: 0.14096544132121272\n",
            "duration: 1002\n",
            "max reward: [16.863083]\n",
            "result: timeout\n",
            "total steps: 196391 \n",
            "\n",
            "episode: 327 epsilon: 0.13956002051612224\n",
            "duration: 1002\n",
            "max reward: [16.57807]\n",
            "result: timeout\n",
            "total steps: 197393 \n",
            "\n",
            "episode: 328 epsilon: 0.1381686117101492\n",
            "duration: 1002\n",
            "max reward: [16.333225]\n",
            "result: timeout\n",
            "total steps: 198395 \n",
            "\n",
            "episode: 329 epsilon: 0.13679107520412406\n",
            "duration: 102\n",
            "max reward: [118.119316]\n",
            "result: coin\n",
            "total steps: 198497 \n",
            "\n",
            "episode: 330 epsilon: 0.1366516194419454\n",
            "duration: 1002\n",
            "max reward: [16.153893]\n",
            "result: timeout\n",
            "total steps: 199499 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2062178]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7742987]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9955125]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3140678]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3546944]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3546944]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.0397134]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.2947474]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8476338]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.225065]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.6406645]\n",
            " \n",
            "episode: 331 epsilon: 0.1352892072988486\n",
            "duration: 906\n",
            "max reward: [118.9409]\n",
            "result: coin\n",
            "total steps: 200405 \n",
            "\n",
            "episode: 332 epsilon: 0.13406902286263328\n",
            "duration: 810\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 201215 \n",
            "\n",
            "episode: 333 epsilon: 0.13298745006078627\n",
            "duration: 186\n",
            "max reward: [118.61778]\n",
            "result: coin\n",
            "total steps: 201401 \n",
            "\n",
            "episode: 334 epsilon: 0.13274032330280475\n",
            "duration: 165\n",
            "max reward: [118.04266]\n",
            "result: coin\n",
            "total steps: 201566 \n",
            "\n",
            "episode: 335 epsilon: 0.1325214823627802\n",
            "duration: 189\n",
            "max reward: [118.08649]\n",
            "result: coin\n",
            "total steps: 201755 \n",
            "\n",
            "episode: 336 epsilon: 0.13227125330206382\n",
            "duration: 436\n",
            "max reward: [118.17583]\n",
            "result: coin\n",
            "total steps: 202191 \n",
            "\n",
            "episode: 337 epsilon: 0.13169580602431727\n",
            "duration: 189\n",
            "max reward: [118.910904]\n",
            "result: coin\n",
            "total steps: 202380 \n",
            "\n",
            "episode: 338 epsilon: 0.13144713601811003\n",
            "duration: 205\n",
            "max reward: [118.09201]\n",
            "result: coin\n",
            "total steps: 202585 \n",
            "\n",
            "episode: 339 epsilon: 0.13117794540392522\n",
            "duration: 145\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 202730 \n",
            "\n",
            "episode: 340 epsilon: 0.13098787521727673\n",
            "duration: 212\n",
            "max reward: [118.34054]\n",
            "result: coin\n",
            "total steps: 202942 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.157214]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.9664211]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8649921]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.552849]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.7317348]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1258316]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5649924]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5656064]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4489925]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.3373528]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.9315987]\n",
            " \n",
            "episode: 341 epsilon: 0.1307104750698679\n",
            "duration: 311\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 203253 \n",
            "\n",
            "episode: 342 epsilon: 0.13030459696000257\n",
            "duration: 199\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 203452 \n",
            "\n",
            "episode: 343 epsilon: 0.1300455486506081\n",
            "duration: 146\n",
            "max reward: [118.02905]\n",
            "result: coin\n",
            "total steps: 203598 \n",
            "\n",
            "episode: 344 epsilon: 0.12985582068469534\n",
            "duration: 1002\n",
            "max reward: [13.8831215]\n",
            "result: timeout\n",
            "total steps: 204600 \n",
            "\n",
            "episode: 345 epsilon: 0.12856116243128335\n",
            "duration: 1002\n",
            "max reward: [8.397549]\n",
            "result: timeout\n",
            "total steps: 205602 \n",
            "\n",
            "episode: 346 epsilon: 0.1272794118779983\n",
            "duration: 1002\n",
            "max reward: [11.760401]\n",
            "result: timeout\n",
            "total steps: 206604 \n",
            "\n",
            "episode: 347 epsilon: 0.12601044033549513\n",
            "duration: 1002\n",
            "max reward: [12.895908]\n",
            "result: timeout\n",
            "total steps: 207606 \n",
            "\n",
            "episode: 348 epsilon: 0.12475412039745745\n",
            "duration: 1002\n",
            "max reward: [15.974705]\n",
            "result: timeout\n",
            "total steps: 208608 \n",
            "\n",
            "episode: 349 epsilon: 0.12351032592780568\n",
            "duration: 1002\n",
            "max reward: [11.191035]\n",
            "result: timeout\n",
            "total steps: 209610 \n",
            "\n",
            "episode: 350 epsilon: 0.12227893204803267\n",
            "duration: 1002\n",
            "max reward: [16.821259]\n",
            "result: timeout\n",
            "total steps: 210612 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.211563]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.341551]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.146629]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.145168]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.186474]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.404928]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.273502]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.281803]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.83802]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.62462]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [13.845427]\n",
            " \n",
            "episode: 351 epsilon: 0.12105981512466593\n",
            "duration: 939\n",
            "max reward: [117.84914]\n",
            "result: coin\n",
            "total steps: 211551 \n",
            "\n",
            "episode: 352 epsilon: 0.11992838384388572\n",
            "duration: 119\n",
            "max reward: [118.17847]\n",
            "result: coin\n",
            "total steps: 211670 \n",
            "\n",
            "episode: 353 epsilon: 0.11978575394873059\n",
            "duration: 85\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 211755 \n",
            "\n",
            "episode: 354 epsilon: 0.11968397931821984\n",
            "duration: 878\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 212633 \n",
            "\n",
            "episode: 355 epsilon: 0.11863775363155038\n",
            "duration: 303\n",
            "max reward: [118.78554]\n",
            "result: coin\n",
            "total steps: 212936 \n",
            "\n",
            "episode: 356 epsilon: 0.11827882528909267\n",
            "duration: 78\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 213014 \n",
            "\n",
            "episode: 357 epsilon: 0.11818660377643274\n",
            "duration: 52\n",
            "max reward: [118.00383]\n",
            "result: coin\n",
            "total steps: 213066 \n",
            "\n",
            "episode: 358 epsilon: 0.11812516271852853\n",
            "duration: 391\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 213457 \n",
            "\n",
            "episode: 359 epsilon: 0.11766419511124777\n",
            "duration: 302\n",
            "max reward: [118.00554]\n",
            "result: coin\n",
            "total steps: 213759 \n",
            "\n",
            "episode: 360 epsilon: 0.11730938527453245\n",
            "duration: 333\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 214092 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [7.0458927]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.7245407]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.193638]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.052372]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.335905]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.4488797]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.7144012]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.1777515]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.678343]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [5.2713113]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [4.664304]\n",
            " \n",
            "episode: 361 epsilon: 0.11691939471622835\n",
            "duration: 188\n",
            "max reward: [118.97501]\n",
            "result: coin\n",
            "total steps: 214280 \n",
            "\n",
            "episode: 362 epsilon: 0.11669979274469515\n",
            "duration: 299\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 214579 \n",
            "\n",
            "episode: 363 epsilon: 0.1163513814987704\n",
            "duration: 1002\n",
            "max reward: [16.526863]\n",
            "result: timeout\n",
            "total steps: 215581 \n",
            "\n",
            "episode: 364 epsilon: 0.11519136205906408\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 216583 \n",
            "\n",
            "episode: 365 epsilon: 0.11404290797494837\n",
            "duration: 1002\n",
            "max reward: [12.469431]\n",
            "result: timeout\n",
            "total steps: 217585 \n",
            "\n",
            "episode: 366 epsilon: 0.11290590394020919\n",
            "duration: 1002\n",
            "max reward: [12.522123]\n",
            "result: timeout\n",
            "total steps: 218587 \n",
            "\n",
            "episode: 367 epsilon: 0.1117802357982315\n",
            "duration: 1002\n",
            "max reward: [14.401562]\n",
            "result: timeout\n",
            "total steps: 219589 \n",
            "\n",
            "episode: 368 epsilon: 0.11066579053053793\n",
            "duration: 285\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 219874 \n",
            "\n",
            "episode: 369 epsilon: 0.11035084204230236\n",
            "duration: 232\n",
            "max reward: [117.937515]\n",
            "result: coin\n",
            "total steps: 220106 \n",
            "\n",
            "episode: 370 epsilon: 0.11009512483542191\n",
            "duration: 1002\n",
            "max reward: [13.674883]\n",
            "result: timeout\n",
            "total steps: 221108 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.6035028]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.7084022]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.6317296]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.6035028]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.8515978]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.834962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.3367186]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.4432306]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9241378]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.716981]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [3.4654763]\n",
            " \n",
            "episode: 371 epsilon: 0.10899748006850238\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 222110 \n",
            "\n",
            "episode: 372 epsilon: 0.10791077878374102\n",
            "duration: 1002\n",
            "max reward: [12.204868]\n",
            "result: timeout\n",
            "total steps: 223112 \n",
            "\n",
            "episode: 373 epsilon: 0.10683491187498144\n",
            "duration: 413\n",
            "max reward: [118.162056]\n",
            "result: coin\n",
            "total steps: 223525 \n",
            "\n",
            "episode: 374 epsilon: 0.10639459357210514\n",
            "duration: 359\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 223884 \n",
            "\n",
            "episode: 375 epsilon: 0.10601332177354869\n",
            "duration: 363\n",
            "max reward: [118.07403]\n",
            "result: coin\n",
            "total steps: 224247 \n",
            "\n",
            "episode: 376 epsilon: 0.10562919103460618\n",
            "duration: 1002\n",
            "max reward: [11.892975]\n",
            "result: timeout\n",
            "total steps: 225249 \n",
            "\n",
            "episode: 377 epsilon: 0.10457607148052595\n",
            "duration: 352\n",
            "max reward: [118.0854]\n",
            "result: coin\n",
            "total steps: 225601 \n",
            "\n",
            "episode: 378 epsilon: 0.10420861081909392\n",
            "duration: 316\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 225917 \n",
            "\n",
            "episode: 379 epsilon: 0.10387983135404869\n",
            "duration: 116\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 226033 \n",
            "\n",
            "episode: 380 epsilon: 0.10375940061301209\n",
            "duration: 134\n",
            "max reward: [117.94095]\n",
            "result: coin\n",
            "total steps: 226167 \n",
            "\n",
            "Evaluating...\n",
            "duration: 159\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 146\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 84\n",
            "max reward: [118.03812]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 116\n",
            "max reward: [117.978714]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 65\n",
            "max reward: [117.93637]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 124\n",
            "max reward: [118.10943]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 91\n",
            "max reward: [117.93614]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 376\n",
            "max reward: [118.77666]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 78\n",
            "max reward: [118.08127]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 120\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 135.9\n",
            "Average max reward: [118.38568]\n",
            "Saving medium.model.380 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 381 epsilon: 0.10362045612978515\n",
            "duration: 89\n",
            "max reward: [118.88646]\n",
            "result: coin\n",
            "total steps: 226256 \n",
            "\n",
            "episode: 382 epsilon: 0.10352827495053912\n",
            "duration: 76\n",
            "max reward: [117.96248]\n",
            "result: coin\n",
            "total steps: 226332 \n",
            "\n",
            "episode: 383 epsilon: 0.10344962335296953\n",
            "duration: 214\n",
            "max reward: [117.982834]\n",
            "result: coin\n",
            "total steps: 226546 \n",
            "\n",
            "episode: 384 epsilon: 0.10322847786905848\n",
            "duration: 287\n",
            "max reward: [118.305954]\n",
            "result: coin\n",
            "total steps: 226833 \n",
            "\n",
            "episode: 385 epsilon: 0.10293263687247206\n",
            "duration: 111\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 226944 \n",
            "\n",
            "episode: 386 epsilon: 0.10281844503373874\n",
            "duration: 364\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 227308 \n",
            "\n",
            "episode: 387 epsilon: 0.10244486621973815\n",
            "duration: 184\n",
            "max reward: [117.96563]\n",
            "result: coin\n",
            "total steps: 227492 \n",
            "\n",
            "episode: 388 epsilon: 0.10225654097824884\n",
            "duration: 218\n",
            "max reward: [118.02081]\n",
            "result: coin\n",
            "total steps: 227710 \n",
            "\n",
            "episode: 389 epsilon: 0.10203386452443819\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 228712 \n",
            "\n",
            "episode: 390 epsilon: 0.10101659025719657\n",
            "duration: 1002\n",
            "max reward: [13.2565365]\n",
            "result: timeout\n",
            "total steps: 229714 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.0966015]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 560\n",
            "max reward: [118.014366]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.976999]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.976505]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.083851]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.992771]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.837971]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.080364]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 957.8\n",
            "Average max reward: [22.605942]\n",
            " \n",
            "episode: 391 epsilon: 0.10000945818087967\n",
            "duration: 476\n",
            "max reward: [117.948204]\n",
            "result: coin\n",
            "total steps: 230190 \n",
            "\n",
            "episode: 392 epsilon: 0.1\n",
            "duration: 404\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 230594 \n",
            "\n",
            "episode: 393 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.306211]\n",
            "result: timeout\n",
            "total steps: 231596 \n",
            "\n",
            "episode: 394 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.132818]\n",
            "result: timeout\n",
            "total steps: 232598 \n",
            "\n",
            "episode: 395 epsilon: 0.1\n",
            "duration: 84\n",
            "max reward: [118.12015]\n",
            "result: coin\n",
            "total steps: 232682 \n",
            "\n",
            "episode: 396 epsilon: 0.1\n",
            "duration: 274\n",
            "max reward: [117.95854]\n",
            "result: coin\n",
            "total steps: 232956 \n",
            "\n",
            "episode: 397 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 233958 \n",
            "\n",
            "episode: 398 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.433479]\n",
            "result: timeout\n",
            "total steps: 234960 \n",
            "\n",
            "episode: 399 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.065433]\n",
            "result: timeout\n",
            "total steps: 235962 \n",
            "\n",
            "episode: 400 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.7732735]\n",
            "result: timeout\n",
            "total steps: 236964 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.480926]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.535968]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.071346]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.210625]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.024088]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.34993]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.841538]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.932337]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.616329]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.322096]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [12.738518]\n",
            " \n",
            "episode: 401 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 237966 \n",
            "\n",
            "episode: 402 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [7.980157]\n",
            "result: timeout\n",
            "total steps: 238968 \n",
            "\n",
            "episode: 403 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [17.139904]\n",
            "result: timeout\n",
            "total steps: 239970 \n",
            "\n",
            "episode: 404 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [14.706957]\n",
            "result: timeout\n",
            "total steps: 240972 \n",
            "\n",
            "episode: 405 epsilon: 0.1\n",
            "duration: 622\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 241594 \n",
            "\n",
            "episode: 406 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.212878]\n",
            "result: timeout\n",
            "total steps: 242596 \n",
            "\n",
            "episode: 407 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [16.191484]\n",
            "result: timeout\n",
            "total steps: 243598 \n",
            "\n",
            "episode: 408 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.752928]\n",
            "result: timeout\n",
            "total steps: 244600 \n",
            "\n",
            "episode: 409 epsilon: 0.1\n",
            "duration: 526\n",
            "max reward: [118.9709]\n",
            "result: coin\n",
            "total steps: 245126 \n",
            "\n",
            "episode: 410 epsilon: 0.1\n",
            "duration: 980\n",
            "max reward: [118.16319]\n",
            "result: coin\n",
            "total steps: 246106 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.524868]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.0728035]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.756144]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.982552]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.675896]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.030878]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.098295]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.765339]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [14.3906765]\n",
            " \n",
            "episode: 411 epsilon: 0.1\n",
            "duration: 179\n",
            "max reward: [117.94974]\n",
            "result: coin\n",
            "total steps: 246285 \n",
            "\n",
            "episode: 412 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [7.3011503]\n",
            "result: timeout\n",
            "total steps: 247287 \n",
            "\n",
            "episode: 413 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 248289 \n",
            "\n",
            "episode: 414 epsilon: 0.1\n",
            "duration: 545\n",
            "max reward: [118.0991]\n",
            "result: coin\n",
            "total steps: 248834 \n",
            "\n",
            "episode: 415 epsilon: 0.1\n",
            "duration: 430\n",
            "max reward: [117.99892]\n",
            "result: coin\n",
            "total steps: 249264 \n",
            "\n",
            "episode: 416 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 250266 \n",
            "\n",
            "episode: 417 epsilon: 0.1\n",
            "duration: 610\n",
            "max reward: [117.88515]\n",
            "result: coin\n",
            "total steps: 250876 \n",
            "\n",
            "episode: 418 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 251878 \n",
            "\n",
            "episode: 419 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [10.970554]\n",
            "result: timeout\n",
            "total steps: 252880 \n",
            "\n",
            "episode: 420 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [11.881607]\n",
            "result: timeout\n",
            "total steps: 253882 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.033623]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.908932]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.305979]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.651857]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [11.7219715]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.931673]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.151737]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [12.070577]\n",
            " \n",
            "episode: 421 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.53545]\n",
            "result: timeout\n",
            "total steps: 254884 \n",
            "\n",
            "episode: 422 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [11.670387]\n",
            "result: timeout\n",
            "total steps: 255886 \n",
            "\n",
            "episode: 423 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.145248]\n",
            "result: timeout\n",
            "total steps: 256888 \n",
            "\n",
            "episode: 424 epsilon: 0.1\n",
            "duration: 626\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 257514 \n",
            "\n",
            "episode: 425 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 258516 \n",
            "\n",
            "episode: 426 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [14.843279]\n",
            "result: timeout\n",
            "total steps: 259518 \n",
            "\n",
            "episode: 427 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.86973]\n",
            "result: timeout\n",
            "total steps: 260520 \n",
            "\n",
            "episode: 428 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 261522 \n",
            "\n",
            "episode: 429 epsilon: 0.1\n",
            "duration: 931\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 262453 \n",
            "\n",
            "episode: 430 epsilon: 0.1\n",
            "duration: 182\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 262635 \n",
            "\n",
            "Evaluating...\n",
            "duration: 154\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 891\n",
            "max reward: [118.93303]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.41107]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 373\n",
            "max reward: [118.9153]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 399\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 558\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 872\n",
            "max reward: [118.94267]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 363\n",
            "max reward: [118.883575]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 463\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 564\n",
            "max reward: [118.389755]\n",
            "result: coin \n",
            "\n",
            "Average duration: 563.9\n",
            "Average max reward: [108.54755]\n",
            " \n",
            "episode: 431 epsilon: 0.1\n",
            "duration: 102\n",
            "max reward: [118.044304]\n",
            "result: coin\n",
            "total steps: 262737 \n",
            "\n",
            "episode: 432 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 263739 \n",
            "\n",
            "episode: 433 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.551481]\n",
            "result: timeout\n",
            "total steps: 264741 \n",
            "\n",
            "episode: 434 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.036873]\n",
            "result: timeout\n",
            "total steps: 265743 \n",
            "\n",
            "episode: 435 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [14.68858]\n",
            "result: timeout\n",
            "total steps: 266745 \n",
            "\n",
            "episode: 436 epsilon: 0.1\n",
            "duration: 839\n",
            "max reward: [118.13597]\n",
            "result: coin\n",
            "total steps: 267584 \n",
            "\n",
            "episode: 437 epsilon: 0.1\n",
            "duration: 832\n",
            "max reward: [118.97]\n",
            "result: coin\n",
            "total steps: 268416 \n",
            "\n",
            "episode: 438 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [14.045622]\n",
            "result: timeout\n",
            "total steps: 269418 \n",
            "\n",
            "episode: 439 epsilon: 0.1\n",
            "duration: 945\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 270363 \n",
            "\n",
            "episode: 440 epsilon: 0.1\n",
            "duration: 580\n",
            "max reward: [118.21443]\n",
            "result: coin\n",
            "total steps: 270943 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.698212]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 943\n",
            "max reward: [118.02606]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 274\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 218\n",
            "max reward: [118.11835]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.597155]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.533337]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 844.9\n",
            "Average max reward: [44.097313]\n",
            " \n",
            "episode: 441 epsilon: 0.1\n",
            "duration: 80\n",
            "max reward: [118.04806]\n",
            "result: coin\n",
            "total steps: 271023 \n",
            "\n",
            "episode: 442 epsilon: 0.1\n",
            "duration: 416\n",
            "max reward: [118.02615]\n",
            "result: coin\n",
            "total steps: 271439 \n",
            "\n",
            "episode: 443 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.252815]\n",
            "result: timeout\n",
            "total steps: 272441 \n",
            "\n",
            "episode: 444 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [15.134071]\n",
            "result: timeout\n",
            "total steps: 273443 \n",
            "\n",
            "episode: 445 epsilon: 0.1\n",
            "duration: 763\n",
            "max reward: [117.946465]\n",
            "result: coin\n",
            "total steps: 274206 \n",
            "\n",
            "episode: 446 epsilon: 0.1\n",
            "duration: 320\n",
            "max reward: [118.04208]\n",
            "result: coin\n",
            "total steps: 274526 \n",
            "\n",
            "episode: 447 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.030001]\n",
            "result: timeout\n",
            "total steps: 275528 \n",
            "\n",
            "episode: 448 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.6714115]\n",
            "result: timeout\n",
            "total steps: 276530 \n",
            "\n",
            "episode: 449 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.496582]\n",
            "result: timeout\n",
            "total steps: 277532 \n",
            "\n",
            "episode: 450 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.929345]\n",
            "result: timeout\n",
            "total steps: 278534 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.66372156]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.66372156]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6852968]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.66372156]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.49371028]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.64147925]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.76289034]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.73821473]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6185484]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.70622516]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.6637529]\n",
            " \n",
            "episode: 451 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.219616]\n",
            "result: timeout\n",
            "total steps: 279536 \n",
            "\n",
            "episode: 452 epsilon: 0.1\n",
            "duration: 602\n",
            "max reward: [117.89456]\n",
            "result: coin\n",
            "total steps: 280138 \n",
            "\n",
            "episode: 453 epsilon: 0.1\n",
            "duration: 204\n",
            "max reward: [117.91222]\n",
            "result: coin\n",
            "total steps: 280342 \n",
            "\n",
            "episode: 454 epsilon: 0.1\n",
            "duration: 521\n",
            "max reward: [118.97177]\n",
            "result: coin\n",
            "total steps: 280863 \n",
            "\n",
            "episode: 455 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [16.690746]\n",
            "result: timeout\n",
            "total steps: 281865 \n",
            "\n",
            "episode: 456 epsilon: 0.1\n",
            "duration: 788\n",
            "max reward: [118.7271]\n",
            "result: coin\n",
            "total steps: 282653 \n",
            "\n",
            "episode: 457 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.337162]\n",
            "result: timeout\n",
            "total steps: 283655 \n",
            "\n",
            "episode: 458 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.028452]\n",
            "result: timeout\n",
            "total steps: 284657 \n",
            "\n",
            "episode: 459 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.060127]\n",
            "result: timeout\n",
            "total steps: 285659 \n",
            "\n",
            "episode: 460 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 286661 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.434292]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.43944]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.099371]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.950785]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.371334]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.53536]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.068649]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [12.589924]\n",
            " \n",
            "episode: 461 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 287663 \n",
            "\n",
            "episode: 462 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [15.157709]\n",
            "result: timeout\n",
            "total steps: 288665 \n",
            "\n",
            "episode: 463 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.097122]\n",
            "result: timeout\n",
            "total steps: 289667 \n",
            "\n",
            "episode: 464 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.949806]\n",
            "result: timeout\n",
            "total steps: 290669 \n",
            "\n",
            "episode: 465 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.902109]\n",
            "result: timeout\n",
            "total steps: 291671 \n",
            "\n",
            "episode: 466 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 292673 \n",
            "\n",
            "episode: 467 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.19999981]\n",
            "result: timeout\n",
            "total steps: 293675 \n",
            "\n",
            "episode: 468 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.11470723]\n",
            "result: timeout\n",
            "total steps: 294677 \n",
            "\n",
            "episode: 469 epsilon: 0.1\n",
            "duration: 262\n",
            "max reward: [118.01978]\n",
            "result: coin\n",
            "total steps: 294939 \n",
            "\n",
            "episode: 470 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 295941 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 495\n",
            "max reward: [117.88339]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 705\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 136\n",
            "max reward: [118.14953]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 754\n",
            "max reward: [118.28216]\n",
            "result: coin \n",
            "\n",
            "Average duration: 810.2\n",
            "Average max reward: [54.531506]\n",
            " \n",
            "episode: 471 epsilon: 0.1\n",
            "duration: 864\n",
            "max reward: [119.]\n",
            "result: coin\n",
            "total steps: 296805 \n",
            "\n",
            "episode: 472 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [2.0315497]\n",
            "result: timeout\n",
            "total steps: 297807 \n",
            "\n",
            "episode: 473 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [15.290674]\n",
            "result: timeout\n",
            "total steps: 298809 \n",
            "\n",
            "episode: 474 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [15.112999]\n",
            "result: timeout\n",
            "total steps: 299811 \n",
            "\n",
            "episode: 475 epsilon: 0.1\n",
            "duration: 591\n",
            "max reward: [118.640335]\n",
            "result: coin\n",
            "total steps: 300402 \n",
            "\n",
            "episode: 476 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [14.315315]\n",
            "result: timeout\n",
            "total steps: 301404 \n",
            "\n",
            "episode: 477 epsilon: 0.1\n",
            "duration: 531\n",
            "max reward: [118.09813]\n",
            "result: coin\n",
            "total steps: 301935 \n",
            "\n",
            "episode: 478 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.385508]\n",
            "result: timeout\n",
            "total steps: 302937 \n",
            "\n",
            "episode: 479 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.521201]\n",
            "result: timeout\n",
            "total steps: 303939 \n",
            "\n",
            "episode: 480 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.781645]\n",
            "result: timeout\n",
            "total steps: 304941 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.849997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [15.557602]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.669916]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [14.706287]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.85873]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [16.485811]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.880276]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [12.6118965]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.154789]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [13.824524]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [13.8599825]\n",
            " \n",
            "episode: 481 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [13.271076]\n",
            "result: timeout\n",
            "total steps: 305943 \n",
            "\n",
            "episode: 482 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.49999952]\n",
            "result: timeout\n",
            "total steps: 306945 \n",
            "\n",
            "episode: 483 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 307947 \n",
            "\n",
            "episode: 484 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [16.740162]\n",
            "result: timeout\n",
            "total steps: 308949 \n",
            "\n",
            "episode: 485 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [12.]\n",
            "result: timeout\n",
            "total steps: 309951 \n",
            "\n",
            "episode: 486 epsilon: 0.1\n",
            "duration: 904\n",
            "max reward: [117.85005]\n",
            "result: coin\n",
            "total steps: 310855 \n",
            "\n",
            "episode: 487 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [15.850052]\n",
            "result: timeout\n",
            "total steps: 311857 \n",
            "\n",
            "episode: 488 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [11.319637]\n",
            "result: timeout\n",
            "total steps: 312859 \n",
            "\n",
            "episode: 489 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.3023775]\n",
            "result: timeout\n",
            "total steps: 313861 \n",
            "\n",
            "episode: 490 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.9501357]\n",
            "result: timeout\n",
            "total steps: 314863 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4175162]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.5524144]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.922703]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.2761102]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.234493]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4175162]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1107345]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1644144]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1303194]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.6117754]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.2837996]\n",
            " \n",
            "episode: 491 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.7425592]\n",
            "result: timeout\n",
            "total steps: 315865 \n",
            "\n",
            "episode: 492 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout\n",
            "total steps: 316867 \n",
            "\n",
            "episode: 493 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.69999933]\n",
            "result: timeout\n",
            "total steps: 317869 \n",
            "\n",
            "episode: 494 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.8411751]\n",
            "result: timeout\n",
            "total steps: 318871 \n",
            "\n",
            "episode: 495 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [1.0651336]\n",
            "result: timeout\n",
            "total steps: 319873 \n",
            "\n",
            "episode: 496 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [0.657217]\n",
            "result: timeout\n",
            "total steps: 320875 \n",
            "\n",
            "episode: 497 epsilon: 0.1\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 321877 \n",
            "\n",
            "episode: 498 epsilon: 0.1\n",
            "duration: 362\n",
            "max reward: [118.818825]\n",
            "result: coin\n",
            "total steps: 322239 \n",
            "\n",
            "episode: 499 epsilon: 0.1\n",
            "duration: 383\n",
            "max reward: [117.89697]\n",
            "result: coin\n",
            "total steps: 322622 \n",
            "\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "SAVE_FILENAME = 'medium.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = MEDIUM_LEVEL\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQnvWYw6nbNs"
      },
      "source": [
        "###### Train for ONE_MONSTER Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20M6QJ90bWzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0501788b-9a8e-43db-d404-7a7ef0ac7ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "screen size:  40 40\n",
            "Making new model.\n",
            "training...\n",
            "episode: 0 epsilon: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-bfb86c3d4208>:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  reward = torch.tensor([reward], device=DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "result: died\n",
            "total steps: 34902 \n",
            "\n",
            "episode: 57 epsilon: 0.7016149598615509\n",
            "duration: 356\n",
            "max reward: [6.0186205]\n",
            "result: died\n",
            "total steps: 35258 \n",
            "\n",
            "episode: 58 epsilon: 0.6991216513269012\n",
            "duration: 27\n",
            "max reward: [5.2530313]\n",
            "result: died\n",
            "total steps: 35285 \n",
            "\n",
            "episode: 59 epsilon: 0.6989329139617337\n",
            "duration: 1002\n",
            "max reward: [3.3073854]\n",
            "result: timeout\n",
            "total steps: 36287 \n",
            "\n",
            "episode: 60 epsilon: 0.6919645758397255\n",
            "duration: 125\n",
            "max reward: [5.3029003]\n",
            "result: died\n",
            "total steps: 36412 \n",
            "\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [5.6376033]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 154\n",
            "max reward: [6.0324373]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 57\n",
            "max reward: [5.6562333]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [5.584092]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [6.2698717]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 65\n",
            "max reward: [5.364823]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 88\n",
            "max reward: [5.887126]\n",
            "result: died \n",
            "\n",
            "Average duration: 711.4\n",
            "Average max reward: [36.44322]\n",
            " \n",
            "episode: 61 epsilon: 0.6911001604920721\n",
            "duration: 231\n",
            "max reward: [5.9788322]\n",
            "result: died\n",
            "total steps: 36643 \n",
            "\n",
            "episode: 62 epsilon: 0.6895055615921432\n",
            "duration: 146\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 36789 \n",
            "\n",
            "episode: 63 epsilon: 0.6884996179897374\n",
            "duration: 135\n",
            "max reward: [7.2860556]\n",
            "result: died\n",
            "total steps: 36924 \n",
            "\n",
            "episode: 64 epsilon: 0.6875707706184956\n",
            "duration: 179\n",
            "max reward: [5.231189]\n",
            "result: died\n",
            "total steps: 37103 \n",
            "\n",
            "episode: 65 epsilon: 0.6863411198048937\n",
            "duration: 66\n",
            "max reward: [5.251707]\n",
            "result: died\n",
            "total steps: 37169 \n",
            "\n",
            "episode: 66 epsilon: 0.685888284118037\n",
            "duration: 180\n",
            "max reward: [5.9838004]\n",
            "result: died\n",
            "total steps: 37349 \n",
            "\n",
            "episode: 67 epsilon: 0.6846547956792612\n",
            "duration: 135\n",
            "max reward: [5.2079635]\n",
            "result: died\n",
            "total steps: 37484 \n",
            "\n",
            "episode: 68 epsilon: 0.6837311353161203\n",
            "duration: 45\n",
            "max reward: [5.431664]\n",
            "result: died\n",
            "total steps: 37529 \n",
            "\n",
            "episode: 69 epsilon: 0.6834235255226224\n",
            "duration: 246\n",
            "max reward: [5.8243995]\n",
            "result: died\n",
            "total steps: 37775 \n",
            "\n",
            "episode: 70 epsilon: 0.6817443698581024\n",
            "duration: 43\n",
            "max reward: [6.0636835]\n",
            "result: died\n",
            "total steps: 37818 \n",
            "\n",
            "Evaluating...\n",
            "duration: 483\n",
            "max reward: [5.689933]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 274\n",
            "max reward: [5.3648233]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 646\n",
            "max reward: [5.364908]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [5.4648232]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [5.9006796]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 643\n",
            "max reward: [6.159258]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 208\n",
            "max reward: [5.959257]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 265\n",
            "max reward: [5.4648232]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 64\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 251\n",
            "max reward: [5.84935]\n",
            "result: died \n",
            "\n",
            "Average duration: 906.4\n",
            "Average max reward: [15.921786]\n",
            " \n",
            "episode: 71 epsilon: 0.6814512827972974\n",
            "duration: 687\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 38505 \n",
            "\n",
            "episode: 72 epsilon: 0.676785756915732\n",
            "duration: 518\n",
            "max reward: [5.668272]\n",
            "result: died\n",
            "total steps: 39023 \n",
            "\n",
            "episode: 73 epsilon: 0.6732890709303136\n",
            "duration: 45\n",
            "max reward: [5.7893343]\n",
            "result: died\n",
            "total steps: 39068 \n",
            "\n",
            "episode: 74 epsilon: 0.672986159008689\n",
            "duration: 100\n",
            "max reward: [5.582605]\n",
            "result: died\n",
            "total steps: 39168 \n",
            "\n",
            "episode: 75 epsilon: 0.6723135092306235\n",
            "duration: 112\n",
            "max reward: [5.859824]\n",
            "result: died\n",
            "total steps: 39280 \n",
            "\n",
            "episode: 76 epsilon: 0.6715609396179368\n",
            "duration: 180\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 39460 \n",
            "\n",
            "episode: 77 epsilon: 0.6703532172028831\n",
            "duration: 174\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 39634 \n",
            "\n",
            "episode: 78 epsilon: 0.6691878167973335\n",
            "duration: 122\n",
            "max reward: [5.23575]\n",
            "result: died\n",
            "total steps: 39756 \n",
            "\n",
            "episode: 79 epsilon: 0.6683719054679519\n",
            "duration: 142\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 39898 \n",
            "\n",
            "episode: 80 epsilon: 0.6674234908958988\n",
            "duration: 137\n",
            "max reward: [5.2663536]\n",
            "result: died\n",
            "total steps: 40035 \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [5.376874]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [5.9007673]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [5.7900887]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [6.408662]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 105\n",
            "max reward: [5.7604837]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 42\n",
            "max reward: [5.558076]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.6992793]\n",
            "result: died \n",
            "\n",
            "Average duration: 716.8\n",
            "Average max reward: [36.449425]\n",
            " \n",
            "episode: 81 epsilon: 0.6665097467710142\n",
            "duration: 212\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 40247 \n",
            "\n",
            "episode: 82 epsilon: 0.6650982428306915\n",
            "duration: 148\n",
            "max reward: [6.1727495]\n",
            "result: died\n",
            "total steps: 40395 \n",
            "\n",
            "episode: 83 epsilon: 0.6641146254876789\n",
            "duration: 74\n",
            "max reward: [6.034216]\n",
            "result: died\n",
            "total steps: 40469 \n",
            "\n",
            "episode: 84 epsilon: 0.6636233624545583\n",
            "duration: 846\n",
            "max reward: [107.03122]\n",
            "result: coin\n",
            "total steps: 41315 \n",
            "\n",
            "episode: 85 epsilon: 0.6580327902723334\n",
            "duration: 40\n",
            "max reward: [6.2253933]\n",
            "result: died\n",
            "total steps: 41355 \n",
            "\n",
            "episode: 86 epsilon: 0.6577696297918293\n",
            "duration: 324\n",
            "max reward: [5.9267616]\n",
            "result: died\n",
            "total steps: 41679 \n",
            "\n",
            "episode: 87 epsilon: 0.6556419049668536\n",
            "duration: 189\n",
            "max reward: [5.871416]\n",
            "result: died\n",
            "total steps: 41868 \n",
            "\n",
            "episode: 88 epsilon: 0.6544039120383033\n",
            "duration: 325\n",
            "max reward: [5.6739545]\n",
            "result: died\n",
            "total steps: 42193 \n",
            "\n",
            "episode: 89 epsilon: 0.6522805516538028\n",
            "duration: 109\n",
            "max reward: [5.3613358]\n",
            "result: died\n",
            "total steps: 42302 \n",
            "\n",
            "episode: 90 epsilon: 0.6515699531990132\n",
            "duration: 508\n",
            "max reward: [5.171264]\n",
            "result: died\n",
            "total steps: 42810 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.9641995]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 971\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 442\n",
            "max reward: [5.021702]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 116\n",
            "max reward: [5.1928983]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 939\n",
            "max reward: [5.2843313]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.9048028]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 677\n",
            "max reward: [5.346423]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.1700895]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 480\n",
            "max reward: [6.241954]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 909\n",
            "max reward: [5.284276]\n",
            "result: died \n",
            "\n",
            "Average duration: 997.7\n",
            "Average max reward: [15.041067]\n",
            " \n",
            "episode: 91 epsilon: 0.6482683709558195\n",
            "duration: 106\n",
            "max reward: [5.2887397]\n",
            "result: died\n",
            "total steps: 42916 \n",
            "\n",
            "episode: 92 epsilon: 0.6475815705511283\n",
            "duration: 169\n",
            "max reward: [5.4582014]\n",
            "result: died\n",
            "total steps: 43085 \n",
            "\n",
            "episode: 93 epsilon: 0.6464880819550199\n",
            "duration: 141\n",
            "max reward: [5.146276]\n",
            "result: died\n",
            "total steps: 43226 \n",
            "\n",
            "episode: 94 epsilon: 0.6455771760990062\n",
            "duration: 92\n",
            "max reward: [5.268803]\n",
            "result: died\n",
            "total steps: 43318 \n",
            "\n",
            "episode: 95 epsilon: 0.6449835182214915\n",
            "duration: 59\n",
            "max reward: [6.171113]\n",
            "result: died\n",
            "total steps: 43377 \n",
            "\n",
            "episode: 96 epsilon: 0.6446030901830476\n",
            "duration: 224\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 43601 \n",
            "\n",
            "episode: 97 epsilon: 0.6431607952344516\n",
            "duration: 192\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 43793 \n",
            "\n",
            "episode: 98 epsilon: 0.6419271112232399\n",
            "duration: 162\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 43955 \n",
            "\n",
            "episode: 99 epsilon: 0.6408880311851359\n",
            "duration: 153\n",
            "max reward: [5.447554]\n",
            "result: died\n",
            "total steps: 44108 \n",
            "\n",
            "episode: 100 epsilon: 0.6399082222424001\n",
            "duration: 213\n",
            "max reward: [5.576866]\n",
            "result: died\n",
            "total steps: 44321 \n",
            "\n",
            "Evaluating...\n",
            "duration: 144\n",
            "max reward: [5.422328]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 125\n",
            "max reward: [5.371401]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 631\n",
            "max reward: [5.137924]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 152\n",
            "max reward: [5.0450416]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 404\n",
            "max reward: [5.191199]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 409\n",
            "max reward: [5.0726495]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 196\n",
            "max reward: [5.3740487]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 695\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 289\n",
            "max reward: [5.212071]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 252\n",
            "max reward: [5.0768523]\n",
            "result: died \n",
            "\n",
            "Average duration: 969.5\n",
            "Average max reward: [15.490352]\n",
            " \n",
            "episode: 101 epsilon: 0.6385466682987433\n",
            "duration: 752\n",
            "max reward: [6.058628]\n",
            "result: died\n",
            "total steps: 45073 \n",
            "\n",
            "episode: 102 epsilon: 0.6337628072148955\n",
            "duration: 84\n",
            "max reward: [5.3694096]\n",
            "result: died\n",
            "total steps: 45157 \n",
            "\n",
            "episode: 103 epsilon: 0.6332306699857609\n",
            "duration: 124\n",
            "max reward: [5.6150317]\n",
            "result: died\n",
            "total steps: 45281 \n",
            "\n",
            "episode: 104 epsilon: 0.6324459505815578\n",
            "duration: 109\n",
            "max reward: [5.1898947]\n",
            "result: died\n",
            "total steps: 45390 \n",
            "\n",
            "episode: 105 epsilon: 0.6317569600634723\n",
            "duration: 147\n",
            "max reward: [5.498992]\n",
            "result: died\n",
            "total steps: 45537 \n",
            "\n",
            "episode: 106 epsilon: 0.6308289595796441\n",
            "duration: 157\n",
            "max reward: [107.95083]\n",
            "result: coin\n",
            "total steps: 45694 \n",
            "\n",
            "episode: 107 epsilon: 0.6298393351715416\n",
            "duration: 37\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 45731 \n",
            "\n",
            "episode: 108 epsilon: 0.6296063377247139\n",
            "duration: 343\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 46074 \n",
            "\n",
            "episode: 109 epsilon: 0.6274504873832669\n",
            "duration: 198\n",
            "max reward: [5.013971]\n",
            "result: died\n",
            "total steps: 46272 \n",
            "\n",
            "episode: 110 epsilon: 0.6262093645353424\n",
            "duration: 43\n",
            "max reward: [5.072608]\n",
            "result: died\n",
            "total steps: 46315 \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [5.41197]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [5.6537857]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [5.41197]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 122\n",
            "max reward: [5.421442]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [5.599786]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [5.41197]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 63\n",
            "max reward: [5.41197]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [5.3304625]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.204972]\n",
            "result: died \n",
            "\n",
            "Average duration: 909.4\n",
            "Average max reward: [15.785833]\n",
            " \n",
            "episode: 111 epsilon: 0.6259401523933507\n",
            "duration: 37\n",
            "max reward: [5.109475]\n",
            "result: died\n",
            "total steps: 46352 \n",
            "\n",
            "episode: 112 epsilon: 0.6257085973772849\n",
            "duration: 205\n",
            "max reward: [6.107606]\n",
            "result: died\n",
            "total steps: 46557 \n",
            "\n",
            "episode: 113 epsilon: 0.6244272086248857\n",
            "duration: 149\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 46706 \n",
            "\n",
            "episode: 114 epsilon: 0.6234975048853234\n",
            "duration: 1002\n",
            "max reward: [7.393343]\n",
            "result: timeout\n",
            "total steps: 47708 \n",
            "\n",
            "episode: 115 epsilon: 0.6172812553061723\n",
            "duration: 493\n",
            "max reward: [5.6065617]\n",
            "result: died\n",
            "total steps: 48201 \n",
            "\n",
            "episode: 116 epsilon: 0.6142455478848511\n",
            "duration: 1002\n",
            "max reward: [4.097965]\n",
            "result: timeout\n",
            "total steps: 49203 \n",
            "\n",
            "episode: 117 epsilon: 0.6081215400121381\n",
            "duration: 103\n",
            "max reward: [5.0763927]\n",
            "result: died\n",
            "total steps: 49306 \n",
            "\n",
            "episode: 118 epsilon: 0.6074954972932732\n",
            "duration: 186\n",
            "max reward: [5.679779]\n",
            "result: died\n",
            "total steps: 49492 \n",
            "\n",
            "episode: 119 epsilon: 0.6063666058627974\n",
            "duration: 257\n",
            "max reward: [5.2347465]\n",
            "result: died\n",
            "total steps: 49749 \n",
            "\n",
            "episode: 120 epsilon: 0.6048102444667581\n",
            "duration: 175\n",
            "max reward: [5.5120316]\n",
            "result: died\n",
            "total steps: 49924 \n",
            "\n",
            "Evaluating...\n",
            "duration: 183\n",
            "max reward: [5.7812424]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 49\n",
            "max reward: [5.4615936]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 384\n",
            "max reward: [5.578044]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.807395]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 49\n",
            "max reward: [5.1581593]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 135\n",
            "max reward: [6.0410023]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [6.183524]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.7048016]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 76\n",
            "max reward: [5.892649]\n",
            "result: died \n",
            "\n",
            "Average duration: 904.3\n",
            "Average max reward: [15.860842]\n",
            " \n",
            "episode: 121 epsilon: 0.6037527521146303\n",
            "duration: 643\n",
            "max reward: [7.8428116]\n",
            "result: died\n",
            "total steps: 50567 \n",
            "\n",
            "episode: 122 epsilon: 0.5998830762590133\n",
            "duration: 223\n",
            "max reward: [5.4156165]\n",
            "result: died\n",
            "total steps: 50790 \n",
            "\n",
            "episode: 123 epsilon: 0.5985468274701079\n",
            "duration: 545\n",
            "max reward: [5.3663273]\n",
            "result: died\n",
            "total steps: 51335 \n",
            "\n",
            "episode: 124 epsilon: 0.5952936203022896\n",
            "duration: 652\n",
            "max reward: [7.1632032]\n",
            "result: died\n",
            "total steps: 51987 \n",
            "\n",
            "episode: 125 epsilon: 0.5914249315282711\n",
            "duration: 63\n",
            "max reward: [5.2856216]\n",
            "result: died\n",
            "total steps: 52050 \n",
            "\n",
            "episode: 126 epsilon: 0.5910524511650427\n",
            "duration: 58\n",
            "max reward: [5.2955046]\n",
            "result: died\n",
            "total steps: 52108 \n",
            "\n",
            "episode: 127 epsilon: 0.5907097401391718\n",
            "duration: 141\n",
            "max reward: [5.4474936]\n",
            "result: died\n",
            "total steps: 52249 \n",
            "\n",
            "episode: 128 epsilon: 0.5898774263247083\n",
            "duration: 30\n",
            "max reward: [5.5681562]\n",
            "result: died\n",
            "total steps: 52279 \n",
            "\n",
            "episode: 129 epsilon: 0.5897004896386407\n",
            "duration: 99\n",
            "max reward: [5.126533]\n",
            "result: died\n",
            "total steps: 52378 \n",
            "\n",
            "episode: 130 epsilon: 0.5891169750412828\n",
            "duration: 920\n",
            "max reward: [6.178537]\n",
            "result: died\n",
            "total steps: 53298 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5237999]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.49999952]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.49999952]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.4378138]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.39999962]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.71013]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8872757]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5999994]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5057867]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.84648037]\n",
            " \n",
            "episode: 131 epsilon: 0.5837219540204268\n",
            "duration: 113\n",
            "max reward: [5.3319073]\n",
            "result: died\n",
            "total steps: 53411 \n",
            "\n",
            "episode: 132 epsilon: 0.5830627207493299\n",
            "duration: 154\n",
            "max reward: [5.127228]\n",
            "result: died\n",
            "total steps: 53565 \n",
            "\n",
            "episode: 133 epsilon: 0.5821654952003703\n",
            "duration: 248\n",
            "max reward: [6.3077073]\n",
            "result: died\n",
            "total steps: 53813 \n",
            "\n",
            "episode: 134 epsilon: 0.5807235135685603\n",
            "duration: 361\n",
            "max reward: [6.0617466]\n",
            "result: died\n",
            "total steps: 54174 \n",
            "\n",
            "episode: 135 epsilon: 0.5786308811586934\n",
            "duration: 109\n",
            "max reward: [5.9308114]\n",
            "result: died\n",
            "total steps: 54283 \n",
            "\n",
            "episode: 136 epsilon: 0.5780005171090489\n",
            "duration: 236\n",
            "max reward: [107.65961]\n",
            "result: coin\n",
            "total steps: 54519 \n",
            "\n",
            "episode: 137 epsilon: 0.5766380442390271\n",
            "duration: 284\n",
            "max reward: [6.1116366]\n",
            "result: died\n",
            "total steps: 54803 \n",
            "\n",
            "episode: 138 epsilon: 0.5750027154594141\n",
            "duration: 40\n",
            "max reward: [5.737652]\n",
            "result: died\n",
            "total steps: 54843 \n",
            "\n",
            "episode: 139 epsilon: 0.5747727603673148\n",
            "duration: 195\n",
            "max reward: [6.2018867]\n",
            "result: died\n",
            "total steps: 55038 \n",
            "\n",
            "episode: 140 epsilon: 0.573653045561344\n",
            "duration: 234\n",
            "max reward: [5.6058254]\n",
            "result: died\n",
            "total steps: 55272 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.5339346]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.8326654]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.7032466]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.5736275]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.397946]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.1082025]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.240441]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.175116]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.375737]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.3630967]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [4.4304013]\n",
            " \n",
            "episode: 141 epsilon: 0.572312266757728\n",
            "duration: 894\n",
            "max reward: [8.]\n",
            "result: died\n",
            "total steps: 56166 \n",
            "\n",
            "episode: 142 epsilon: 0.5672185977188364\n",
            "duration: 218\n",
            "max reward: [5.3576217]\n",
            "result: died\n",
            "total steps: 56384 \n",
            "\n",
            "episode: 143 epsilon: 0.5659834080217554\n",
            "duration: 261\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 56645 \n",
            "\n",
            "episode: 144 epsilon: 0.5645081174185413\n",
            "duration: 174\n",
            "max reward: [107.0994]\n",
            "result: coin\n",
            "total steps: 56819 \n",
            "\n",
            "episode: 145 epsilon: 0.5635267273511965\n",
            "duration: 135\n",
            "max reward: [5.5096636]\n",
            "result: died\n",
            "total steps: 56954 \n",
            "\n",
            "episode: 146 epsilon: 0.5627664795519993\n",
            "duration: 200\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 57154 \n",
            "\n",
            "episode: 147 epsilon: 0.5616420713758742\n",
            "duration: 132\n",
            "max reward: [6.181684]\n",
            "result: died\n",
            "total steps: 57286 \n",
            "\n",
            "episode: 148 epsilon: 0.5609011929290085\n",
            "duration: 487\n",
            "max reward: [107.91156]\n",
            "result: coin\n",
            "total steps: 57773 \n",
            "\n",
            "episode: 149 epsilon: 0.558176244753859\n",
            "duration: 255\n",
            "max reward: [5.830331]\n",
            "result: died\n",
            "total steps: 58028 \n",
            "\n",
            "episode: 150 epsilon: 0.5567547085586803\n",
            "duration: 132\n",
            "max reward: [5.239206]\n",
            "result: died\n",
            "total steps: 58160 \n",
            "\n",
            "Evaluating...\n",
            "duration: 79\n",
            "max reward: [5.9918065]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 307\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 687\n",
            "max reward: [5.657622]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 148\n",
            "max reward: [6.119863]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 229\n",
            "max reward: [107.19005]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 97\n",
            "max reward: [5.2462163]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 459\n",
            "max reward: [8.]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 72\n",
            "max reward: [5.7432117]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.4599886]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 188\n",
            "max reward: [6.0582533]\n",
            "result: died \n",
            "\n",
            "Average duration: 853.8\n",
            "Average max reward: [26.2467]\n",
            " \n",
            "episode: 151 epsilon: 0.5560202771747356\n",
            "duration: 185\n",
            "max reward: [5.002811]\n",
            "result: died\n",
            "total steps: 58345 \n",
            "\n",
            "episode: 152 epsilon: 0.5549925905651809\n",
            "duration: 203\n",
            "max reward: [5.082586]\n",
            "result: died\n",
            "total steps: 58548 \n",
            "\n",
            "episode: 153 epsilon: 0.5538670983674178\n",
            "duration: 227\n",
            "max reward: [5.627467]\n",
            "result: died\n",
            "total steps: 58775 \n",
            "\n",
            "episode: 154 epsilon: 0.5526112459858503\n",
            "duration: 876\n",
            "max reward: [5.651604]\n",
            "result: died\n",
            "total steps: 59651 \n",
            "\n",
            "episode: 155 epsilon: 0.5477915127238924\n",
            "duration: 441\n",
            "max reward: [5.267352]\n",
            "result: died\n",
            "total steps: 60092 \n",
            "\n",
            "episode: 156 epsilon: 0.5453810710831392\n",
            "duration: 60\n",
            "max reward: [7.3377]\n",
            "result: died\n",
            "total steps: 60152 \n",
            "\n",
            "episode: 157 epsilon: 0.5450539405894512\n",
            "duration: 305\n",
            "max reward: [5.418435]\n",
            "result: died\n",
            "total steps: 60457 \n",
            "\n",
            "episode: 158 epsilon: 0.5433940586773236\n",
            "duration: 35\n",
            "max reward: [6.166398]\n",
            "result: died\n",
            "total steps: 60492 \n",
            "\n",
            "episode: 159 epsilon: 0.5432039040357899\n",
            "duration: 135\n",
            "max reward: [6.1875744]\n",
            "result: died\n",
            "total steps: 60627 \n",
            "\n",
            "episode: 160 epsilon: 0.5424710735372267\n",
            "duration: 297\n",
            "max reward: [5.8670363]\n",
            "result: died\n",
            "total steps: 60924 \n",
            "\n",
            "Evaluating...\n",
            "duration: 468\n",
            "max reward: [5.6997957]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.726203]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 873\n",
            "max reward: [6.1074]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.031843]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 537\n",
            "max reward: [5.9797297]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.337735]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 208\n",
            "max reward: [6.137202]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 957\n",
            "max reward: [6.3575153]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.1447325]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.6995554]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1001.0\n",
            "Average max reward: [5.0221715]\n",
            " \n",
            "episode: 161 epsilon: 0.540862324623509\n",
            "duration: 173\n",
            "max reward: [8.]\n",
            "result: died\n",
            "total steps: 61097 \n",
            "\n",
            "episode: 162 epsilon: 0.5399274417087991\n",
            "duration: 70\n",
            "max reward: [5.2822294]\n",
            "result: died\n",
            "total steps: 61167 \n",
            "\n",
            "episode: 163 epsilon: 0.5395496247509657\n",
            "duration: 65\n",
            "max reward: [6.9771986]\n",
            "result: died\n",
            "total steps: 61232 \n",
            "\n",
            "episode: 164 epsilon: 0.5391990314500442\n",
            "duration: 91\n",
            "max reward: [6.111253]\n",
            "result: died\n",
            "total steps: 61323 \n",
            "\n",
            "episode: 165 epsilon: 0.5387085835190782\n",
            "duration: 97\n",
            "max reward: [6.758914]\n",
            "result: died\n",
            "total steps: 61420 \n",
            "\n",
            "episode: 166 epsilon: 0.5381862895465936\n",
            "duration: 89\n",
            "max reward: [5.2453322]\n",
            "result: died\n",
            "total steps: 61509 \n",
            "\n",
            "episode: 167 epsilon: 0.537707516834357\n",
            "duration: 234\n",
            "max reward: [5.0703936]\n",
            "result: died\n",
            "total steps: 61743 \n",
            "\n",
            "episode: 168 epsilon: 0.5364507522330099\n",
            "duration: 124\n",
            "max reward: [5.0651655]\n",
            "result: died\n",
            "total steps: 61867 \n",
            "\n",
            "episode: 169 epsilon: 0.5357859655531638\n",
            "duration: 188\n",
            "max reward: [5.8648705]\n",
            "result: died\n",
            "total steps: 62055 \n",
            "\n",
            "episode: 170 epsilon: 0.5347796341858072\n",
            "duration: 189\n",
            "max reward: [5.027673]\n",
            "result: died\n",
            "total steps: 62244 \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [6.065736]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 92\n",
            "max reward: [5.824462]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [107.912674]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 45\n",
            "max reward: [107.528564]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.1531596]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 32\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 92\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 195\n",
            "max reward: [5.7062607]\n",
            "result: died \n",
            "\n",
            "Average duration: 427.7\n",
            "Average max reward: [67.01909]\n",
            "Saving monster.model.170 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 171 epsilon: 0.5337698552189057\n",
            "duration: 40\n",
            "max reward: [5.371792]\n",
            "result: died\n",
            "total steps: 62284 \n",
            "\n",
            "episode: 172 epsilon: 0.5335563899727137\n",
            "duration: 1002\n",
            "max reward: [4.0558643]\n",
            "result: timeout\n",
            "total steps: 63286 \n",
            "\n",
            "episode: 173 epsilon: 0.5282368503456364\n",
            "duration: 173\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 63459 \n",
            "\n",
            "episode: 174 epsilon: 0.5273237906189268\n",
            "duration: 104\n",
            "max reward: [5.1103077]\n",
            "result: died\n",
            "total steps: 63563 \n",
            "\n",
            "episode: 175 epsilon: 0.5267756589545535\n",
            "duration: 698\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 64261 \n",
            "\n",
            "episode: 176 epsilon: 0.52311156741086\n",
            "duration: 374\n",
            "max reward: [5.1993065]\n",
            "result: died\n",
            "total steps: 64635 \n",
            "\n",
            "episode: 177 epsilon: 0.5211587841297078\n",
            "duration: 80\n",
            "max reward: [5.326908]\n",
            "result: died\n",
            "total steps: 64715 \n",
            "\n",
            "episode: 178 epsilon: 0.5207420238287517\n",
            "duration: 41\n",
            "max reward: [107.92755]\n",
            "result: coin\n",
            "total steps: 64756 \n",
            "\n",
            "episode: 179 epsilon: 0.5205285633613679\n",
            "duration: 404\n",
            "max reward: [5.3872614]\n",
            "result: died\n",
            "total steps: 65160 \n",
            "\n",
            "episode: 180 epsilon: 0.5184298701801159\n",
            "duration: 37\n",
            "max reward: [5.845389]\n",
            "result: died\n",
            "total steps: 65197 \n",
            "\n",
            "Evaluating...\n",
            "duration: 95\n",
            "max reward: [5.491763]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.507597]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [5.5658917]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [5.3962936]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 109\n",
            "max reward: [5.471349]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 59\n",
            "max reward: [5.655403]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 88\n",
            "max reward: [5.2702627]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.682718]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 256\n",
            "max reward: [5.654051]\n",
            "result: died \n",
            "\n",
            "Average duration: 903.8\n",
            "Average max reward: [15.769533]\n",
            " \n",
            "episode: 181 epsilon: 0.5182380866102976\n",
            "duration: 629\n",
            "max reward: [6.256404]\n",
            "result: died\n",
            "total steps: 65826 \n",
            "\n",
            "episode: 182 epsilon: 0.514988599396386\n",
            "duration: 211\n",
            "max reward: [5.369238]\n",
            "result: died\n",
            "total steps: 66037 \n",
            "\n",
            "episode: 183 epsilon: 0.5139031190361618\n",
            "duration: 121\n",
            "max reward: [5.8989735]\n",
            "result: died\n",
            "total steps: 66158 \n",
            "\n",
            "episode: 184 epsilon: 0.5132816723132171\n",
            "duration: 160\n",
            "max reward: [5.8618174]\n",
            "result: died\n",
            "total steps: 66318 \n",
            "\n",
            "episode: 185 epsilon: 0.5124610782877964\n",
            "duration: 204\n",
            "max reward: [5.496843]\n",
            "result: died\n",
            "total steps: 66522 \n",
            "\n",
            "episode: 186 epsilon: 0.5114167232923669\n",
            "duration: 390\n",
            "max reward: [5.5827827]\n",
            "result: died\n",
            "total steps: 66912 \n",
            "\n",
            "episode: 187 epsilon: 0.5094260823445117\n",
            "duration: 36\n",
            "max reward: [5.6216803]\n",
            "result: died\n",
            "total steps: 66948 \n",
            "\n",
            "episode: 188 epsilon: 0.5092427219617168\n",
            "duration: 254\n",
            "max reward: [8.]\n",
            "result: died\n",
            "total steps: 67202 \n",
            "\n",
            "episode: 189 epsilon: 0.5079508867731571\n",
            "duration: 102\n",
            "max reward: [6.0743365]\n",
            "result: died\n",
            "total steps: 67304 \n",
            "\n",
            "episode: 190 epsilon: 0.5074330410148825\n",
            "duration: 500\n",
            "max reward: [6.1356106]\n",
            "result: died\n",
            "total steps: 67804 \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 42\n",
            "max reward: [6.3556757]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [6.443824]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [6.5850887]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.964244]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [6.5415206]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 166\n",
            "max reward: [6.5850887]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [6.1525764]\n",
            "result: died \n",
            "\n",
            "Average duration: 708.7\n",
            "Average max reward: [36.8628]\n",
            " \n",
            "episode: 191 epsilon: 0.5049022081645003\n",
            "duration: 39\n",
            "max reward: [5.31785]\n",
            "result: died\n",
            "total steps: 67843 \n",
            "\n",
            "episode: 192 epsilon: 0.5047053346961378\n",
            "duration: 324\n",
            "max reward: [5.631876]\n",
            "result: died\n",
            "total steps: 68167 \n",
            "\n",
            "episode: 193 epsilon: 0.5030727356503739\n",
            "duration: 32\n",
            "max reward: [6.3460197]\n",
            "result: died\n",
            "total steps: 68199 \n",
            "\n",
            "episode: 194 epsilon: 0.5029117781295426\n",
            "duration: 281\n",
            "max reward: [5.584546]\n",
            "result: died\n",
            "total steps: 68480 \n",
            "\n",
            "episode: 195 epsilon: 0.5015005796953788\n",
            "duration: 66\n",
            "max reward: [5.22381]\n",
            "result: died\n",
            "total steps: 68546 \n",
            "\n",
            "episode: 196 epsilon: 0.5011696985155801\n",
            "duration: 89\n",
            "max reward: [5.3764315]\n",
            "result: died\n",
            "total steps: 68635 \n",
            "\n",
            "episode: 197 epsilon: 0.5007238559132886\n",
            "duration: 41\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 68676 \n",
            "\n",
            "episode: 198 epsilon: 0.5005186012124532\n",
            "duration: 175\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 68851 \n",
            "\n",
            "episode: 199 epsilon: 0.4996434596325572\n",
            "duration: 58\n",
            "max reward: [5.010989]\n",
            "result: died\n",
            "total steps: 68909 \n",
            "\n",
            "episode: 200 epsilon: 0.49935375044975483\n",
            "duration: 84\n",
            "max reward: [6.04359]\n",
            "result: died\n",
            "total steps: 68993 \n",
            "\n",
            "Evaluating...\n",
            "duration: 196\n",
            "max reward: [106.77735]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 181\n",
            "max reward: [6.00983]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 236\n",
            "max reward: [6.4529266]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 412\n",
            "max reward: [6.3492513]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 651\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [6.4511046]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 130\n",
            "max reward: [6.174078]\n",
            "result: died \n",
            "\n",
            "Average duration: 594.3\n",
            "Average max reward: [57.021454]\n",
            " \n",
            "episode: 201 epsilon: 0.4989344694220624\n",
            "duration: 45\n",
            "max reward: [5.7994337]\n",
            "result: died\n",
            "total steps: 69038 \n",
            "\n",
            "episode: 202 epsilon: 0.4987099994203608\n",
            "duration: 35\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 69073 \n",
            "\n",
            "episode: 203 epsilon: 0.4985354814629877\n",
            "duration: 26\n",
            "max reward: [5.6745577]\n",
            "result: died\n",
            "total steps: 69099 \n",
            "\n",
            "episode: 204 epsilon: 0.4984058790868464\n",
            "duration: 129\n",
            "max reward: [6.146058]\n",
            "result: died\n",
            "total steps: 69228 \n",
            "\n",
            "episode: 205 epsilon: 0.4977633500231731\n",
            "duration: 180\n",
            "max reward: [6.4712048]\n",
            "result: died\n",
            "total steps: 69408 \n",
            "\n",
            "episode: 206 epsilon: 0.49686818188615006\n",
            "duration: 180\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 69588 \n",
            "\n",
            "episode: 207 epsilon: 0.495974623602471\n",
            "duration: 42\n",
            "max reward: [5.9582677]\n",
            "result: died\n",
            "total steps: 69630 \n",
            "\n",
            "episode: 208 epsilon: 0.4957663579993962\n",
            "duration: 48\n",
            "max reward: [107.008064]\n",
            "result: coin\n",
            "total steps: 69678 \n",
            "\n",
            "episode: 209 epsilon: 0.495528447250704\n",
            "duration: 187\n",
            "max reward: [5.0246315]\n",
            "result: died\n",
            "total steps: 69865 \n",
            "\n",
            "episode: 210 epsilon: 0.49460267492125104\n",
            "duration: 116\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 69981 \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.588077]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 95\n",
            "max reward: [5.7634134]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [6.079552]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [6.2956753]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [5.4753857]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [6.338133]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 48\n",
            "max reward: [6.1347094]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [6.0227013]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 100\n",
            "max reward: [6.1569805]\n",
            "result: died \n",
            "\n",
            "Average duration: 903.8\n",
            "Average max reward: [16.185463]\n",
            " \n",
            "episode: 211 epsilon: 0.4940292684583888\n",
            "duration: 21\n",
            "max reward: [5.277356]\n",
            "result: died\n",
            "total steps: 70002 \n",
            "\n",
            "episode: 212 epsilon: 0.49392553320459537\n",
            "duration: 180\n",
            "max reward: [107.912674]\n",
            "result: coin\n",
            "total steps: 70182 \n",
            "\n",
            "episode: 213 epsilon: 0.4930372669243112\n",
            "duration: 87\n",
            "max reward: [5.852775]\n",
            "result: died\n",
            "total steps: 70269 \n",
            "\n",
            "episode: 214 epsilon: 0.49260851103794145\n",
            "duration: 146\n",
            "max reward: [6.0257244]\n",
            "result: died\n",
            "total steps: 70415 \n",
            "\n",
            "episode: 215 epsilon: 0.49188982737855963\n",
            "duration: 101\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 70516 \n",
            "\n",
            "episode: 216 epsilon: 0.49139326945686923\n",
            "duration: 500\n",
            "max reward: [5.354624]\n",
            "result: died\n",
            "total steps: 71016 \n",
            "\n",
            "episode: 217 epsilon: 0.4889424353008773\n",
            "duration: 171\n",
            "max reward: [5.730278]\n",
            "result: died\n",
            "total steps: 71187 \n",
            "\n",
            "episode: 218 epsilon: 0.48810705818750516\n",
            "duration: 50\n",
            "max reward: [8.]\n",
            "result: died\n",
            "total steps: 71237 \n",
            "\n",
            "episode: 219 epsilon: 0.48786306566162607\n",
            "duration: 284\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 71521 \n",
            "\n",
            "episode: 220 epsilon: 0.4864795001481166\n",
            "duration: 815\n",
            "max reward: [6.160201]\n",
            "result: died\n",
            "total steps: 72336 \n",
            "\n",
            "Evaluating...\n",
            "duration: 461\n",
            "max reward: [5.379409]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 551\n",
            "max reward: [5.5753255]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [5.755567]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 100\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 575\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 157\n",
            "max reward: [5.5794096]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 310\n",
            "max reward: [5.0697184]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 242\n",
            "max reward: [5.362734]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 756\n",
            "max reward: [5.3775177]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 110\n",
            "max reward: [5.505755]\n",
            "result: died \n",
            "\n",
            "Average duration: 867.5\n",
            "Average max reward: [25.960545]\n",
            " \n",
            "episode: 221 epsilon: 0.4825308050114176\n",
            "duration: 96\n",
            "max reward: [5.3860707]\n",
            "result: died\n",
            "total steps: 72432 \n",
            "\n",
            "episode: 222 epsilon: 0.48206779771766656\n",
            "duration: 152\n",
            "max reward: [5.4408903]\n",
            "result: died\n",
            "total steps: 72584 \n",
            "\n",
            "episode: 223 epsilon: 0.48133561126780794\n",
            "duration: 22\n",
            "max reward: [5.1667547]\n",
            "result: died\n",
            "total steps: 72606 \n",
            "\n",
            "episode: 224 epsilon: 0.4812297290807966\n",
            "duration: 143\n",
            "max reward: [6.334253]\n",
            "result: died\n",
            "total steps: 72749 \n",
            "\n",
            "episode: 225 epsilon: 0.48054206236709546\n",
            "duration: 335\n",
            "max reward: [5.9393263]\n",
            "result: died\n",
            "total steps: 73084 \n",
            "\n",
            "episode: 226 epsilon: 0.4789349398913067\n",
            "duration: 126\n",
            "max reward: [107.09755]\n",
            "result: coin\n",
            "total steps: 73210 \n",
            "\n",
            "episode: 227 epsilon: 0.47833186188597426\n",
            "duration: 226\n",
            "max reward: [6.470352]\n",
            "result: died\n",
            "total steps: 73436 \n",
            "\n",
            "episode: 228 epsilon: 0.4772520525222957\n",
            "duration: 87\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 73523 \n",
            "\n",
            "episode: 229 epsilon: 0.4768370238002733\n",
            "duration: 61\n",
            "max reward: [5.1999903]\n",
            "result: died\n",
            "total steps: 73584 \n",
            "\n",
            "episode: 230 epsilon: 0.4765462419132474\n",
            "duration: 98\n",
            "max reward: [5.276618]\n",
            "result: died\n",
            "total steps: 73682 \n",
            "\n",
            "Evaluating...\n",
            "duration: 222\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 122\n",
            "max reward: [5.7597404]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 241\n",
            "max reward: [5.672632]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 59\n",
            "max reward: [5.659595]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 477\n",
            "max reward: [5.631166]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 414\n",
            "max reward: [5.4777627]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 359\n",
            "max reward: [6.467763]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 512\n",
            "max reward: [6.467763]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 163\n",
            "max reward: [5.5325217]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 275\n",
            "max reward: [5.4660835]\n",
            "result: died \n",
            "\n",
            "Average duration: 922.2\n",
            "Average max reward: [16.0135]\n",
            " \n",
            "episode: 231 epsilon: 0.47607945535894247\n",
            "duration: 207\n",
            "max reward: [5.7887306]\n",
            "result: died\n",
            "total steps: 73889 \n",
            "\n",
            "episode: 232 epsilon: 0.47509499015935897\n",
            "duration: 449\n",
            "max reward: [6.4907856]\n",
            "result: died\n",
            "total steps: 74338 \n",
            "\n",
            "episode: 233 epsilon: 0.47296659547532865\n",
            "duration: 132\n",
            "max reward: [5.165141]\n",
            "result: died\n",
            "total steps: 74470 \n",
            "\n",
            "episode: 234 epsilon: 0.4723426914365577\n",
            "duration: 171\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 74641 \n",
            "\n",
            "episode: 235 epsilon: 0.4715356756293659\n",
            "duration: 146\n",
            "max reward: [6.0487156]\n",
            "result: died\n",
            "total steps: 74787 \n",
            "\n",
            "episode: 236 epsilon: 0.4708477358611788\n",
            "duration: 123\n",
            "max reward: [106.95367]\n",
            "result: coin\n",
            "total steps: 74910 \n",
            "\n",
            "episode: 237 epsilon: 0.4702689491728534\n",
            "duration: 76\n",
            "max reward: [5.239548]\n",
            "result: died\n",
            "total steps: 74986 \n",
            "\n",
            "episode: 238 epsilon: 0.469911680550755\n",
            "duration: 119\n",
            "max reward: [5.3956995]\n",
            "result: died\n",
            "total steps: 75105 \n",
            "\n",
            "episode: 239 epsilon: 0.4693528182399249\n",
            "duration: 135\n",
            "max reward: [5.2573347]\n",
            "result: died\n",
            "total steps: 75240 \n",
            "\n",
            "episode: 240 epsilon: 0.4687196194406576\n",
            "duration: 56\n",
            "max reward: [5.3458805]\n",
            "result: died\n",
            "total steps: 75296 \n",
            "\n",
            "Evaluating...\n",
            "duration: 83\n",
            "max reward: [6.201804]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.8007674]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 34\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 63\n",
            "max reward: [5.4618783]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.8007674]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [5.5086336]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 88\n",
            "max reward: [5.7031]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [5.7199416]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [5.588077]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.8007674]\n",
            "result: died \n",
            "\n",
            "Average duration: 903.4\n",
            "Average max reward: [15.958572]\n",
            " \n",
            "episode: 241 epsilon: 0.46845720993529\n",
            "duration: 137\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 75433 \n",
            "\n",
            "episode: 242 epsilon: 0.46781586298065453\n",
            "duration: 62\n",
            "max reward: [5.484212]\n",
            "result: died\n",
            "total steps: 75495 \n",
            "\n",
            "episode: 243 epsilon: 0.46752590704123603\n",
            "duration: 1002\n",
            "max reward: [4.1768966]\n",
            "result: timeout\n",
            "total steps: 76497 \n",
            "\n",
            "episode: 244 epsilon: 0.46286468915324813\n",
            "duration: 1002\n",
            "max reward: [3.0102773]\n",
            "result: timeout\n",
            "total steps: 77499 \n",
            "\n",
            "episode: 245 epsilon: 0.45824994345401393\n",
            "duration: 678\n",
            "max reward: [6.42338]\n",
            "result: died\n",
            "total steps: 78177 \n",
            "\n",
            "episode: 246 epsilon: 0.4551535175825694\n",
            "duration: 162\n",
            "max reward: [107.279015]\n",
            "result: coin\n",
            "total steps: 78339 \n",
            "\n",
            "episode: 247 epsilon: 0.45441676581414564\n",
            "duration: 52\n",
            "max reward: [5.1631804]\n",
            "result: died\n",
            "total steps: 78391 \n",
            "\n",
            "episode: 248 epsilon: 0.4541805305224214\n",
            "duration: 89\n",
            "max reward: [6.3809633]\n",
            "result: died\n",
            "total steps: 78480 \n",
            "\n",
            "episode: 249 epsilon: 0.45377648967510353\n",
            "duration: 86\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 78566 \n",
            "\n",
            "episode: 250 epsilon: 0.4533864096524346\n",
            "duration: 59\n",
            "max reward: [5.4384527]\n",
            "result: died\n",
            "total steps: 78625 \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [5.4571114]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 48\n",
            "max reward: [5.223344]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [5.5791607]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 65\n",
            "max reward: [5.5017138]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.9007673]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [5.3586855]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 54\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 82\n",
            "max reward: [5.3953066]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.4571114]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 808.4\n",
            "Average max reward: [25.98732]\n",
            " \n",
            "episode: 251 epsilon: 0.4531189905671273\n",
            "duration: 58\n",
            "max reward: [6.340748]\n",
            "result: died\n",
            "total steps: 78683 \n",
            "\n",
            "episode: 252 epsilon: 0.4528562577524798\n",
            "duration: 127\n",
            "max reward: [5.682226]\n",
            "result: died\n",
            "total steps: 78810 \n",
            "\n",
            "episode: 253 epsilon: 0.45228149535650847\n",
            "duration: 1002\n",
            "max reward: [3.4183202]\n",
            "result: timeout\n",
            "total steps: 79812 \n",
            "\n",
            "episode: 254 epsilon: 0.4477722637507063\n",
            "duration: 944\n",
            "max reward: [6.092978]\n",
            "result: died\n",
            "total steps: 80756 \n",
            "\n",
            "episode: 255 epsilon: 0.44356518224789465\n",
            "duration: 351\n",
            "max reward: [6.2723308]\n",
            "result: died\n",
            "total steps: 81107 \n",
            "\n",
            "episode: 256 epsilon: 0.4420109976478198\n",
            "duration: 177\n",
            "max reward: [5.3637924]\n",
            "result: died\n",
            "total steps: 81284 \n",
            "\n",
            "episode: 257 epsilon: 0.4412293301617822\n",
            "duration: 326\n",
            "max reward: [107.7558]\n",
            "result: coin\n",
            "total steps: 81610 \n",
            "\n",
            "episode: 258 epsilon: 0.439793264604141\n",
            "duration: 56\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 81666 \n",
            "\n",
            "episode: 259 epsilon: 0.4395470493226759\n",
            "duration: 884\n",
            "max reward: [5.973231]\n",
            "result: died\n",
            "total steps: 82550 \n",
            "\n",
            "episode: 260 epsilon: 0.43567857724521897\n",
            "duration: 215\n",
            "max reward: [6.082926]\n",
            "result: died\n",
            "total steps: 82765 \n",
            "\n",
            "Evaluating...\n",
            "duration: 316\n",
            "max reward: [107.86714]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.3270836]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.9922204]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.9145145]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.3106613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 737\n",
            "max reward: [5.771504]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [107.81703]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 208\n",
            "max reward: [5.092286]\n",
            "result: died \n",
            "\n",
            "Average duration: 835.0\n",
            "Average max reward: [25.309244]\n",
            " \n",
            "episode: 261 epsilon: 0.43474287454498495\n",
            "duration: 201\n",
            "max reward: [5.517099]\n",
            "result: died\n",
            "total steps: 82966 \n",
            "\n",
            "episode: 262 epsilon: 0.4338699189813932\n",
            "duration: 553\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 83519 \n",
            "\n",
            "episode: 263 epsilon: 0.4314772401838209\n",
            "duration: 1002\n",
            "max reward: [4.029478]\n",
            "result: timeout\n",
            "total steps: 84521 \n",
            "\n",
            "episode: 264 epsilon: 0.42717542631657973\n",
            "duration: 142\n",
            "max reward: [107.94768]\n",
            "result: coin\n",
            "total steps: 84663 \n",
            "\n",
            "episode: 265 epsilon: 0.426569267685693\n",
            "duration: 185\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 84848 \n",
            "\n",
            "episode: 266 epsilon: 0.4257808440571958\n",
            "duration: 24\n",
            "max reward: [5.5992684]\n",
            "result: died\n",
            "total steps: 84872 \n",
            "\n",
            "episode: 267 epsilon: 0.42567866891612943\n",
            "duration: 380\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 85252 \n",
            "\n",
            "episode: 268 epsilon: 0.4240641594849599\n",
            "duration: 110\n",
            "max reward: [6.129287]\n",
            "result: died\n",
            "total steps: 85362 \n",
            "\n",
            "episode: 269 epsilon: 0.42359794537429724\n",
            "duration: 25\n",
            "max reward: [5.3970613]\n",
            "result: died\n",
            "total steps: 85387 \n",
            "\n",
            "episode: 270 epsilon: 0.42349205912428645\n",
            "duration: 206\n",
            "max reward: [107.89774]\n",
            "result: coin\n",
            "total steps: 85593 \n",
            "\n",
            "Evaluating...\n",
            "duration: 500\n",
            "max reward: [5.5727997]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 784\n",
            "max reward: [5.539706]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 222\n",
            "max reward: [6.2027025]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 750\n",
            "max reward: [5.5499177]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 164\n",
            "max reward: [5.445607]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [107.457924]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 132\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [5.5416913]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 574\n",
            "max reward: [5.4079]\n",
            "result: died \n",
            "\n",
            "Average duration: 815.8\n",
            "Average max reward: [25.771826]\n",
            " \n",
            "episode: 271 epsilon: 0.4226205634312441\n",
            "duration: 197\n",
            "max reward: [107.81374]\n",
            "result: coin\n",
            "total steps: 85790 \n",
            "\n",
            "episode: 272 epsilon: 0.4217888204571067\n",
            "duration: 224\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 86014 \n",
            "\n",
            "episode: 273 epsilon: 0.4208450708934072\n",
            "duration: 161\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 86175 \n",
            "\n",
            "episode: 274 epsilon: 0.4201680554729233\n",
            "duration: 179\n",
            "max reward: [5.3739643]\n",
            "result: died\n",
            "total steps: 86354 \n",
            "\n",
            "episode: 275 epsilon: 0.4194166273824053\n",
            "duration: 38\n",
            "max reward: [6.3062077]\n",
            "result: died\n",
            "total steps: 86392 \n",
            "\n",
            "episode: 276 epsilon: 0.4192572793420452\n",
            "duration: 176\n",
            "max reward: [5.445424]\n",
            "result: died\n",
            "total steps: 86568 \n",
            "\n",
            "episode: 277 epsilon: 0.41852003549529554\n",
            "duration: 163\n",
            "max reward: [107.959114]\n",
            "result: coin\n",
            "total steps: 86731 \n",
            "\n",
            "episode: 278 epsilon: 0.41783840351841833\n",
            "duration: 156\n",
            "max reward: [5.750984]\n",
            "result: died\n",
            "total steps: 86887 \n",
            "\n",
            "episode: 279 epsilon: 0.4171870837704207\n",
            "duration: 141\n",
            "max reward: [6.0760984]\n",
            "result: died\n",
            "total steps: 87028 \n",
            "\n",
            "episode: 280 epsilon: 0.4165992644922824\n",
            "duration: 54\n",
            "max reward: [6.357737]\n",
            "result: died\n",
            "total steps: 87082 \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [107.84169]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 83\n",
            "max reward: [5.2079315]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [107.77967]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [107.883194]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 105\n",
            "max reward: [5.1168566]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 114\n",
            "max reward: [5.7062426]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [107.22928]\n",
            "result: coin \n",
            "\n",
            "Average duration: 328.2\n",
            "Average max reward: [77.07649]\n",
            "Saving monster.model.280 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 281 epsilon: 0.41637436161869756\n",
            "duration: 117\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 87199 \n",
            "\n",
            "episode: 282 epsilon: 0.4158874884919229\n",
            "duration: 172\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 87371 \n",
            "\n",
            "episode: 283 epsilon: 0.41517277683993775\n",
            "duration: 78\n",
            "max reward: [107.74945]\n",
            "result: coin\n",
            "total steps: 87449 \n",
            "\n",
            "episode: 284 epsilon: 0.4148490683367309\n",
            "duration: 110\n",
            "max reward: [6.4302845]\n",
            "result: died\n",
            "total steps: 87559 \n",
            "\n",
            "episode: 285 epsilon: 0.4143929852532448\n",
            "duration: 22\n",
            "max reward: [6.0371957]\n",
            "result: died\n",
            "total steps: 87581 \n",
            "\n",
            "episode: 286 epsilon: 0.4143018288240639\n",
            "duration: 50\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 87631 \n",
            "\n",
            "episode: 287 epsilon: 0.41409472968875033\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 88633 \n",
            "\n",
            "episode: 288 epsilon: 0.4099662188783825\n",
            "duration: 1002\n",
            "max reward: [0.6555188]\n",
            "result: timeout\n",
            "total steps: 89635 \n",
            "\n",
            "episode: 289 epsilon: 0.4058788691847577\n",
            "duration: 1002\n",
            "max reward: [1.2904179]\n",
            "result: timeout\n",
            "total steps: 90637 \n",
            "\n",
            "episode: 290 epsilon: 0.4018322702328981\n",
            "duration: 328\n",
            "max reward: [5.487076]\n",
            "result: died\n",
            "total steps: 90965 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.537548]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 537\n",
            "max reward: [6.2395496]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 187\n",
            "max reward: [5.5609794]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 514\n",
            "max reward: [5.862116]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 884\n",
            "max reward: [5.515663]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.212846]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 225\n",
            "max reward: [5.7837524]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.4898167]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 79\n",
            "max reward: [5.6655984]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 433\n",
            "max reward: [5.9545975]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.6\n",
            "Average max reward: [5.382247]\n",
            " \n",
            "episode: 291 epsilon: 0.40051641956133927\n",
            "duration: 56\n",
            "max reward: [5.0749173]\n",
            "result: died\n",
            "total steps: 91021 \n",
            "\n",
            "episode: 292 epsilon: 0.4002921931556383\n",
            "duration: 71\n",
            "max reward: [5.9435887]\n",
            "result: died\n",
            "total steps: 91092 \n",
            "\n",
            "episode: 293 epsilon: 0.40000808656827114\n",
            "duration: 193\n",
            "max reward: [6.1097465]\n",
            "result: died\n",
            "total steps: 91285 \n",
            "\n",
            "episode: 294 epsilon: 0.39923681547720624\n",
            "duration: 39\n",
            "max reward: [5.0329175]\n",
            "result: died\n",
            "total steps: 91324 \n",
            "\n",
            "episode: 295 epsilon: 0.39908114347718326\n",
            "duration: 116\n",
            "max reward: [5.158764]\n",
            "result: died\n",
            "total steps: 91440 \n",
            "\n",
            "episode: 296 epsilon: 0.3986184777487525\n",
            "duration: 258\n",
            "max reward: [5.281908]\n",
            "result: died\n",
            "total steps: 91698 \n",
            "\n",
            "episode: 297 epsilon: 0.3975913676179673\n",
            "duration: 77\n",
            "max reward: [5.3857355]\n",
            "result: died\n",
            "total steps: 91775 \n",
            "\n",
            "episode: 298 epsilon: 0.39728534010061595\n",
            "duration: 56\n",
            "max reward: [5.3539586]\n",
            "result: died\n",
            "total steps: 91831 \n",
            "\n",
            "episode: 299 epsilon: 0.39706292259287435\n",
            "duration: 622\n",
            "max reward: [6.1753235]\n",
            "result: died\n",
            "total steps: 92453 \n",
            "\n",
            "episode: 300 epsilon: 0.39460085617867385\n",
            "duration: 128\n",
            "max reward: [5.702156]\n",
            "result: died\n",
            "total steps: 92581 \n",
            "\n",
            "Evaluating...\n",
            "duration: 80\n",
            "max reward: [5.4510326]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 48\n",
            "max reward: [5.5282373]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [5.4210973]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [5.3282375]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 67\n",
            "max reward: [5.531377]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 167\n",
            "max reward: [5.3600826]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 55\n",
            "max reward: [5.5282373]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [5.5282373]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 80\n",
            "max reward: [5.335128]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 65\n",
            "max reward: [5.59482]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.4606485]\n",
            " \n",
            "episode: 301 epsilon: 0.39409609020190767\n",
            "duration: 1002\n",
            "max reward: [3.1359882]\n",
            "result: timeout\n",
            "total steps: 93583 \n",
            "\n",
            "episode: 302 epsilon: 0.39016696516825855\n",
            "duration: 1002\n",
            "max reward: [2.1110215]\n",
            "result: timeout\n",
            "total steps: 94585 \n",
            "\n",
            "episode: 303 epsilon: 0.38627701338172826\n",
            "duration: 523\n",
            "max reward: [107.91124]\n",
            "result: coin\n",
            "total steps: 95108 \n",
            "\n",
            "episode: 304 epsilon: 0.38426205830217813\n",
            "duration: 153\n",
            "max reward: [5.9393497]\n",
            "result: died\n",
            "total steps: 95261 \n",
            "\n",
            "episode: 305 epsilon: 0.3836745868832123\n",
            "duration: 128\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 95389 \n",
            "\n",
            "episode: 306 epsilon: 0.38318379758416227\n",
            "duration: 27\n",
            "max reward: [5.716129]\n",
            "result: died\n",
            "total steps: 95416 \n",
            "\n",
            "episode: 307 epsilon: 0.38308035192460704\n",
            "duration: 40\n",
            "max reward: [107.97]\n",
            "result: coin\n",
            "total steps: 95456 \n",
            "\n",
            "episode: 308 epsilon: 0.3829271504261796\n",
            "duration: 88\n",
            "max reward: [5.1785507]\n",
            "result: died\n",
            "total steps: 95544 \n",
            "\n",
            "episode: 309 epsilon: 0.3825903227597144\n",
            "duration: 217\n",
            "max reward: [5.46494]\n",
            "result: died\n",
            "total steps: 95761 \n",
            "\n",
            "episode: 310 epsilon: 0.38176100189789325\n",
            "duration: 364\n",
            "max reward: [5.1295624]\n",
            "result: died\n",
            "total steps: 96125 \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [6.075652]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 54\n",
            "max reward: [5.555708]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 57\n",
            "max reward: [5.8007593]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [6.237749]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 214\n",
            "max reward: [5.815007]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [6.30969]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [5.608392]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 156\n",
            "max reward: [5.4166236]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 210\n",
            "max reward: [5.8745847]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 146\n",
            "max reward: [5.9508576]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.864502]\n",
            " \n",
            "episode: 311 epsilon: 0.38037391787543123\n",
            "duration: 52\n",
            "max reward: [5.608435]\n",
            "result: died\n",
            "total steps: 96177 \n",
            "\n",
            "episode: 312 epsilon: 0.3801761748557769\n",
            "duration: 977\n",
            "max reward: [5.777791]\n",
            "result: died\n",
            "total steps: 97154 \n",
            "\n",
            "episode: 313 epsilon: 0.3764799391402278\n",
            "duration: 1002\n",
            "max reward: [4.0475464]\n",
            "result: timeout\n",
            "total steps: 98156 \n",
            "\n",
            "episode: 314 epsilon: 0.3727264465522026\n",
            "duration: 1002\n",
            "max reward: [4.9799066]\n",
            "result: timeout\n",
            "total steps: 99158 \n",
            "\n",
            "episode: 315 epsilon: 0.3690103761616006\n",
            "duration: 232\n",
            "max reward: [5.162577]\n",
            "result: died\n",
            "total steps: 99390 \n",
            "\n",
            "episode: 316 epsilon: 0.36815526440209284\n",
            "duration: 886\n",
            "max reward: [5.347971]\n",
            "result: died\n",
            "total steps: 100276 \n",
            "\n",
            "episode: 317 epsilon: 0.36490781619861923\n",
            "duration: 655\n",
            "max reward: [5.0465145]\n",
            "result: died\n",
            "total steps: 100931 \n",
            "\n",
            "episode: 318 epsilon: 0.3625254806687185\n",
            "duration: 206\n",
            "max reward: [5.358123]\n",
            "result: died\n",
            "total steps: 101137 \n",
            "\n",
            "episode: 319 epsilon: 0.3617794468571892\n",
            "duration: 326\n",
            "max reward: [5.008415]\n",
            "result: died\n",
            "total steps: 101463 \n",
            "\n",
            "episode: 320 epsilon: 0.36060196619672724\n",
            "duration: 375\n",
            "max reward: [5.0290613]\n",
            "result: died\n",
            "total steps: 101838 \n",
            "\n",
            "Evaluating...\n",
            "duration: 145\n",
            "max reward: [5.0947695]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 384\n",
            "max reward: [5.065692]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.79348]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 900\n",
            "max reward: [5.2074084]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 620\n",
            "max reward: [5.04469]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 716\n",
            "max reward: [5.1999817]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [6.4531507]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.8904915]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 388\n",
            "max reward: [5.246853]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 97\n",
            "max reward: [5.09046]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.2\n",
            "Average max reward: [5.3086977]\n",
            " \n",
            "episode: 321 epsilon: 0.35925224113968013\n",
            "duration: 191\n",
            "max reward: [6.711092]\n",
            "result: died\n",
            "total steps: 102029 \n",
            "\n",
            "episode: 322 epsilon: 0.3585667242361491\n",
            "duration: 174\n",
            "max reward: [5.0548887]\n",
            "result: died\n",
            "total steps: 102203 \n",
            "\n",
            "episode: 323 epsilon: 0.35794336061959925\n",
            "duration: 79\n",
            "max reward: [5.94248]\n",
            "result: died\n",
            "total steps: 102282 \n",
            "\n",
            "episode: 324 epsilon: 0.35766069703152786\n",
            "duration: 196\n",
            "max reward: [5.5728645]\n",
            "result: died\n",
            "total steps: 102478 \n",
            "\n",
            "episode: 325 epsilon: 0.3569603686113963\n",
            "duration: 289\n",
            "max reward: [5.3141356]\n",
            "result: died\n",
            "total steps: 102767 \n",
            "\n",
            "episode: 326 epsilon: 0.35593024239546767\n",
            "duration: 69\n",
            "max reward: [5.0066156]\n",
            "result: died\n",
            "total steps: 102836 \n",
            "\n",
            "episode: 327 epsilon: 0.35568473523792465\n",
            "duration: 1002\n",
            "max reward: [4.444427]\n",
            "result: timeout\n",
            "total steps: 103838 \n",
            "\n",
            "episode: 328 epsilon: 0.35213857014759303\n",
            "duration: 382\n",
            "max reward: [5.0863094]\n",
            "result: died\n",
            "total steps: 104220 \n",
            "\n",
            "episode: 329 epsilon: 0.35079596681464514\n",
            "duration: 333\n",
            "max reward: [6.3299103]\n",
            "result: died\n",
            "total steps: 104553 \n",
            "\n",
            "episode: 330 epsilon: 0.34962975905872923\n",
            "duration: 236\n",
            "max reward: [5.069038]\n",
            "result: died\n",
            "total steps: 104789 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5244257]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.6059241]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.8317554]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.5615299]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.0075839]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.2999997]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.0973713]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.69999933]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6299994]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [1.0558589]\n",
            " \n",
            "episode: 331 epsilon: 0.3488056057108182\n",
            "duration: 807\n",
            "max reward: [6.0463333]\n",
            "result: died\n",
            "total steps: 105596 \n",
            "\n",
            "episode: 332 epsilon: 0.3460020719464426\n",
            "duration: 1002\n",
            "max reward: [4.5448256]\n",
            "result: timeout\n",
            "total steps: 106598 \n",
            "\n",
            "episode: 333 epsilon: 0.34255244269007845\n",
            "duration: 1002\n",
            "max reward: [2.5163057]\n",
            "result: timeout\n",
            "total steps: 107600 \n",
            "\n",
            "episode: 334 epsilon: 0.3391372061237332\n",
            "duration: 715\n",
            "max reward: [5.367976]\n",
            "result: died\n",
            "total steps: 108315 \n",
            "\n",
            "episode: 335 epsilon: 0.3367210232471659\n",
            "duration: 501\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 108816 \n",
            "\n",
            "episode: 336 epsilon: 0.33503826973801004\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 109818 \n",
            "\n",
            "episode: 337 epsilon: 0.3316979492284011\n",
            "duration: 1002\n",
            "max reward: [2.596546]\n",
            "result: timeout\n",
            "total steps: 110820 \n",
            "\n",
            "episode: 338 epsilon: 0.32839093160420785\n",
            "duration: 792\n",
            "max reward: [5.096492]\n",
            "result: died\n",
            "total steps: 111612 \n",
            "\n",
            "episode: 339 epsilon: 0.32580034767972954\n",
            "duration: 1002\n",
            "max reward: [3.0457544]\n",
            "result: timeout\n",
            "total steps: 112614 \n",
            "\n",
            "episode: 340 epsilon: 0.32255212894864754\n",
            "duration: 1002\n",
            "max reward: [2.541542]\n",
            "result: timeout\n",
            "total steps: 113616 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4871192]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.7516859]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.538764]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.8941746]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.4808974]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.5084023]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.9431996]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3550441]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.8698945]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.8910398]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.6720219]\n",
            " \n",
            "episode: 341 epsilon: 0.31933629485128406\n",
            "duration: 273\n",
            "max reward: [5.7874513]\n",
            "result: died\n",
            "total steps: 113889 \n",
            "\n",
            "episode: 342 epsilon: 0.31846569567492305\n",
            "duration: 68\n",
            "max reward: [5.965349]\n",
            "result: died\n",
            "total steps: 113957 \n",
            "\n",
            "episode: 343 epsilon: 0.3182492126144465\n",
            "duration: 85\n",
            "max reward: [5.7273636]\n",
            "result: died\n",
            "total steps: 114042 \n",
            "\n",
            "episode: 344 epsilon: 0.3179788157186851\n",
            "duration: 101\n",
            "max reward: [5.384185]\n",
            "result: died\n",
            "total steps: 114143 \n",
            "\n",
            "episode: 345 epsilon: 0.31765781924531566\n",
            "duration: 66\n",
            "max reward: [5.2364664]\n",
            "result: died\n",
            "total steps: 114209 \n",
            "\n",
            "episode: 346 epsilon: 0.3174482342552684\n",
            "duration: 105\n",
            "max reward: [6.5963144]\n",
            "result: died\n",
            "total steps: 114314 \n",
            "\n",
            "episode: 347 epsilon: 0.3171150885414079\n",
            "duration: 106\n",
            "max reward: [107.245094]\n",
            "result: coin\n",
            "total steps: 114420 \n",
            "\n",
            "episode: 348 epsilon: 0.31677912463987923\n",
            "duration: 154\n",
            "max reward: [5.6039495]\n",
            "result: died\n",
            "total steps: 114574 \n",
            "\n",
            "episode: 349 epsilon: 0.31629166023186717\n",
            "duration: 1002\n",
            "max reward: [2.6825814]\n",
            "result: timeout\n",
            "total steps: 115576 \n",
            "\n",
            "episode: 350 epsilon: 0.31313824280132435\n",
            "duration: 1002\n",
            "max reward: [1.552901]\n",
            "result: timeout\n",
            "total steps: 116578 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4089229]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6057174]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.57505083]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5357866]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4676602]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.4671228]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5938575]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.6720207]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.5515988]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [0.7632723]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [0.56410104]\n",
            " \n",
            "episode: 351 epsilon: 0.3100162648386573\n",
            "duration: 1002\n",
            "max reward: [1.903095]\n",
            "result: timeout\n",
            "total steps: 117580 \n",
            "\n",
            "episode: 352 epsilon: 0.3069254128934074\n",
            "duration: 1002\n",
            "max reward: [2.8497863]\n",
            "result: timeout\n",
            "total steps: 118582 \n",
            "\n",
            "episode: 353 epsilon: 0.3038653766402065\n",
            "duration: 1002\n",
            "max reward: [2.1958184]\n",
            "result: timeout\n",
            "total steps: 119584 \n",
            "\n",
            "episode: 354 epsilon: 0.3008358488476202\n",
            "duration: 1002\n",
            "max reward: [3.023747]\n",
            "result: timeout\n",
            "total steps: 120586 \n",
            "\n",
            "episode: 355 epsilon: 0.2978365253473016\n",
            "duration: 1002\n",
            "max reward: [2.668489]\n",
            "result: timeout\n",
            "total steps: 121588 \n",
            "\n",
            "episode: 356 epsilon: 0.294867105003452\n",
            "duration: 1002\n",
            "max reward: [2.8998313]\n",
            "result: timeout\n",
            "total steps: 122590 \n",
            "\n",
            "episode: 357 epsilon: 0.2919272896825866\n",
            "duration: 979\n",
            "max reward: [5.957576]\n",
            "result: died\n",
            "total steps: 123569 \n",
            "\n",
            "episode: 358 epsilon: 0.28908326572905335\n",
            "duration: 188\n",
            "max reward: [5.59986]\n",
            "result: died\n",
            "total steps: 123757 \n",
            "\n",
            "episode: 359 epsilon: 0.2885402997374365\n",
            "duration: 1002\n",
            "max reward: [3.5215998]\n",
            "result: timeout\n",
            "total steps: 124759 \n",
            "\n",
            "episode: 360 epsilon: 0.28566356245660185\n",
            "duration: 1002\n",
            "max reward: [3.3572674]\n",
            "result: timeout\n",
            "total steps: 125761 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.0882225]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.3219213]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.0153513]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.308006]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.5941153]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.323596]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [3.3651214]\n",
            " \n",
            "episode: 361 epsilon: 0.28281550615166723\n",
            "duration: 1002\n",
            "max reward: [3.5148478]\n",
            "result: timeout\n",
            "total steps: 126763 \n",
            "\n",
            "episode: 362 epsilon: 0.2799958448742479\n",
            "duration: 179\n",
            "max reward: [5.05176]\n",
            "result: died\n",
            "total steps: 126942 \n",
            "\n",
            "episode: 363 epsilon: 0.2794951006117408\n",
            "duration: 148\n",
            "max reward: [5.522111]\n",
            "result: died\n",
            "total steps: 127090 \n",
            "\n",
            "episode: 364 epsilon: 0.2790817538149147\n",
            "duration: 43\n",
            "max reward: [6.0768785]\n",
            "result: died\n",
            "total steps: 127133 \n",
            "\n",
            "episode: 365 epsilon: 0.2789617744581847\n",
            "duration: 444\n",
            "max reward: [5.2942815]\n",
            "result: died\n",
            "total steps: 127577 \n",
            "\n",
            "episode: 366 epsilon: 0.2777259297850097\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 128579 \n",
            "\n",
            "episode: 367 epsilon: 0.2749570114162619\n",
            "duration: 1002\n",
            "max reward: [2.1350625]\n",
            "result: timeout\n",
            "total steps: 129581 \n",
            "\n",
            "episode: 368 epsilon: 0.2722156990724132\n",
            "duration: 1002\n",
            "max reward: [1.5896089]\n",
            "result: timeout\n",
            "total steps: 130583 \n",
            "\n",
            "episode: 369 epsilon: 0.2695017175223052\n",
            "duration: 1002\n",
            "max reward: [2.137555]\n",
            "result: timeout\n",
            "total steps: 131585 \n",
            "\n",
            "episode: 370 epsilon: 0.2668147942788246\n",
            "duration: 1002\n",
            "max reward: [2.0135677]\n",
            "result: timeout\n",
            "total steps: 132587 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.465933]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.5733707]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.983444]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.6733706]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3450048]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.6324708]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.3187695]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.7596278]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [1.8606458]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [2.916719]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [2.5529356]\n",
            " \n",
            "episode: 371 epsilon: 0.26415465957154605\n",
            "duration: 1002\n",
            "max reward: [2.2137866]\n",
            "result: timeout\n",
            "total steps: 133589 \n",
            "\n",
            "episode: 372 epsilon: 0.26152104631964623\n",
            "duration: 1002\n",
            "max reward: [3.4709682]\n",
            "result: timeout\n",
            "total steps: 134591 \n",
            "\n",
            "episode: 373 epsilon: 0.2589136901050891\n",
            "duration: 634\n",
            "max reward: [6.104329]\n",
            "result: died\n",
            "total steps: 135225 \n",
            "\n",
            "episode: 374 epsilon: 0.2572773699258599\n",
            "duration: 764\n",
            "max reward: [6.290923]\n",
            "result: died\n",
            "total steps: 135989 \n",
            "\n",
            "episode: 375 epsilon: 0.2553192603228069\n",
            "duration: 341\n",
            "max reward: [5.406519]\n",
            "result: died\n",
            "total steps: 136330 \n",
            "\n",
            "episode: 376 epsilon: 0.25445010439817684\n",
            "duration: 64\n",
            "max reward: [5.6263337]\n",
            "result: died\n",
            "total steps: 136394 \n",
            "\n",
            "episode: 377 epsilon: 0.25428730843162806\n",
            "duration: 246\n",
            "max reward: [5.355186]\n",
            "result: died\n",
            "total steps: 136640 \n",
            "\n",
            "episode: 378 epsilon: 0.2536625304448855\n",
            "duration: 153\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 136793 \n",
            "\n",
            "episode: 379 epsilon: 0.2532747235212528\n",
            "duration: 189\n",
            "max reward: [5.421668]\n",
            "result: died\n",
            "total steps: 136982 \n",
            "\n",
            "episode: 380 epsilon: 0.25279648637026453\n",
            "duration: 64\n",
            "max reward: [5.7748914]\n",
            "result: died\n",
            "total steps: 137046 \n",
            "\n",
            "Evaluating...\n",
            "duration: 113\n",
            "max reward: [5.406202]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 67\n",
            "max reward: [5.3998175]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 390\n",
            "max reward: [5.5143766]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 67\n",
            "max reward: [5.293484]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 69\n",
            "max reward: [5.3609447]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 50\n",
            "max reward: [5.3556376]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 97\n",
            "max reward: [5.606223]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 106\n",
            "max reward: [5.5109735]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 250\n",
            "max reward: [5.274473]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 134\n",
            "max reward: [5.408574]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.413071]\n",
            " \n",
            "episode: 381 epsilon: 0.25263474838066485\n",
            "duration: 98\n",
            "max reward: [5.3419595]\n",
            "result: died\n",
            "total steps: 137144 \n",
            "\n",
            "episode: 382 epsilon: 0.2523872876028381\n",
            "duration: 400\n",
            "max reward: [5.4662986]\n",
            "result: died\n",
            "total steps: 137544 \n",
            "\n",
            "episode: 383 epsilon: 0.2513797548612865\n",
            "duration: 122\n",
            "max reward: [7.5319185]\n",
            "result: died\n",
            "total steps: 137666 \n",
            "\n",
            "episode: 384 epsilon: 0.25107325856111457\n",
            "duration: 75\n",
            "max reward: [5.379552]\n",
            "result: died\n",
            "total steps: 137741 \n",
            "\n",
            "episode: 385 epsilon: 0.2508850242138974\n",
            "duration: 27\n",
            "max reward: [5.2067795]\n",
            "result: died\n",
            "total steps: 137768 \n",
            "\n",
            "episode: 386 epsilon: 0.2508172944012958\n",
            "duration: 49\n",
            "max reward: [5.53559]\n",
            "result: died\n",
            "total steps: 137817 \n",
            "\n",
            "episode: 387 epsilon: 0.25069442403273795\n",
            "duration: 119\n",
            "max reward: [7.7555094]\n",
            "result: died\n",
            "total steps: 137936 \n",
            "\n",
            "episode: 388 epsilon: 0.25039627510193685\n",
            "duration: 230\n",
            "max reward: [8.]\n",
            "result: died\n",
            "total steps: 138166 \n",
            "\n",
            "episode: 389 epsilon: 0.24982102545987994\n",
            "duration: 24\n",
            "max reward: [5.3355417]\n",
            "result: died\n",
            "total steps: 138190 \n",
            "\n",
            "episode: 390 epsilon: 0.2497610756080396\n",
            "duration: 103\n",
            "max reward: [5.182495]\n",
            "result: died\n",
            "total steps: 138293 \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.023464]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.023464]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 156\n",
            "max reward: [5.065408]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.023464]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [5.151288]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.023464]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.99897]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 357\n",
            "max reward: [5.0809135]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 740\n",
            "max reward: [5.147338]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [5.575329]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.2\n",
            "Average max reward: [5.1113105]\n",
            " \n",
            "episode: 391 epsilon: 0.24950395414045076\n",
            "duration: 81\n",
            "max reward: [5.629536]\n",
            "result: died\n",
            "total steps: 138374 \n",
            "\n",
            "episode: 392 epsilon: 0.24930193776527418\n",
            "duration: 80\n",
            "max reward: [7.837137]\n",
            "result: died\n",
            "total steps: 138454 \n",
            "\n",
            "episode: 393 epsilon: 0.24910257597041255\n",
            "duration: 153\n",
            "max reward: [5.6197824]\n",
            "result: died\n",
            "total steps: 138607 \n",
            "\n",
            "episode: 394 epsilon: 0.24872174044264808\n",
            "duration: 218\n",
            "max reward: [5.3457494]\n",
            "result: died\n",
            "total steps: 138825 \n",
            "\n",
            "episode: 395 epsilon: 0.2481801176318475\n",
            "duration: 1002\n",
            "max reward: [8.]\n",
            "result: timeout\n",
            "total steps: 139827 \n",
            "\n",
            "episode: 396 epsilon: 0.2457057700367174\n",
            "duration: 609\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 140436 \n",
            "\n",
            "episode: 397 epsilon: 0.24421396904189224\n",
            "duration: 353\n",
            "max reward: [107.8035]\n",
            "result: coin\n",
            "total steps: 140789 \n",
            "\n",
            "episode: 398 epsilon: 0.2433534135053043\n",
            "duration: 1002\n",
            "max reward: [2.9376278]\n",
            "result: timeout\n",
            "total steps: 141791 \n",
            "\n",
            "episode: 399 epsilon: 0.24092718799127344\n",
            "duration: 126\n",
            "max reward: [5.689662]\n",
            "result: died\n",
            "total steps: 141917 \n",
            "\n",
            "episode: 400 epsilon: 0.2406238109021074\n",
            "duration: 385\n",
            "max reward: [5.982063]\n",
            "result: died\n",
            "total steps: 142302 \n",
            "\n",
            "Evaluating...\n",
            "duration: 117\n",
            "max reward: [5.4503117]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [5.47691]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [5.8009458]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 779\n",
            "max reward: [5.5835342]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 697\n",
            "max reward: [5.706986]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 413\n",
            "max reward: [5.4429455]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 231\n",
            "max reward: [5.482734]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 731\n",
            "max reward: [5.720348]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.5320015]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 472\n",
            "max reward: [5.677586]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.2\n",
            "Average max reward: [5.48743]\n",
            " \n",
            "episode: 401 epsilon: 0.23969919026695582\n",
            "duration: 1002\n",
            "max reward: [3.0766454]\n",
            "result: timeout\n",
            "total steps: 143304 \n",
            "\n",
            "episode: 402 epsilon: 0.23730939723820285\n",
            "duration: 217\n",
            "max reward: [5.331277]\n",
            "result: died\n",
            "total steps: 143521 \n",
            "\n",
            "episode: 403 epsilon: 0.23679499417537517\n",
            "duration: 956\n",
            "max reward: [6.248251]\n",
            "result: died\n",
            "total steps: 144477 \n",
            "\n",
            "episode: 404 epsilon: 0.2345420204046053\n",
            "duration: 311\n",
            "max reward: [5.7177925]\n",
            "result: died\n",
            "total steps: 144788 \n",
            "\n",
            "episode: 405 epsilon: 0.23381372780315207\n",
            "duration: 333\n",
            "max reward: [6.0068564]\n",
            "result: died\n",
            "total steps: 145121 \n",
            "\n",
            "episode: 406 epsilon: 0.23303642302031877\n",
            "duration: 97\n",
            "max reward: [5.5681825]\n",
            "result: died\n",
            "total steps: 145218 \n",
            "\n",
            "episode: 407 epsilon: 0.2328104872865352\n",
            "duration: 192\n",
            "max reward: [5.709031]\n",
            "result: died\n",
            "total steps: 145410 \n",
            "\n",
            "episode: 408 epsilon: 0.23236391999273256\n",
            "duration: 396\n",
            "max reward: [7.4655037]\n",
            "result: died\n",
            "total steps: 145806 \n",
            "\n",
            "episode: 409 epsilon: 0.23144557838603114\n",
            "duration: 82\n",
            "max reward: [5.378047]\n",
            "result: died\n",
            "total steps: 145888 \n",
            "\n",
            "episode: 410 epsilon: 0.23125587080249377\n",
            "duration: 81\n",
            "max reward: [5.0722456]\n",
            "result: died\n",
            "total steps: 145969 \n",
            "\n",
            "Evaluating...\n",
            "duration: 143\n",
            "max reward: [5.432637]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [5.576307]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 202\n",
            "max reward: [5.352627]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [5.409462]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [5.7222958]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [5.7222958]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 151\n",
            "max reward: [5.4424276]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [5.44944]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [5.603755]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [5.603755]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.5315003]\n",
            " \n",
            "episode: 411 epsilon: 0.23106862939015319\n",
            "duration: 233\n",
            "max reward: [5.4495273]\n",
            "result: died\n",
            "total steps: 146202 \n",
            "\n",
            "episode: 412 epsilon: 0.23053086622105465\n",
            "duration: 82\n",
            "max reward: [5.431217]\n",
            "result: died\n",
            "total steps: 146284 \n",
            "\n",
            "episode: 413 epsilon: 0.23034190839405036\n",
            "duration: 83\n",
            "max reward: [5.008726]\n",
            "result: died\n",
            "total steps: 146367 \n",
            "\n",
            "episode: 414 epsilon: 0.23015080392940712\n",
            "duration: 45\n",
            "max reward: [5.460908]\n",
            "result: died\n",
            "total steps: 146412 \n",
            "\n",
            "episode: 415 epsilon: 0.23004725936691278\n",
            "duration: 96\n",
            "max reward: [106.98418]\n",
            "result: coin\n",
            "total steps: 146508 \n",
            "\n",
            "episode: 416 epsilon: 0.22982651996978393\n",
            "duration: 71\n",
            "max reward: [5.684725]\n",
            "result: died\n",
            "total steps: 146579 \n",
            "\n",
            "episode: 417 epsilon: 0.2296634010546726\n",
            "duration: 57\n",
            "max reward: [5.5010157]\n",
            "result: died\n",
            "total steps: 146636 \n",
            "\n",
            "episode: 418 epsilon: 0.2295325302178033\n",
            "duration: 132\n",
            "max reward: [5.298304]\n",
            "result: died\n",
            "total steps: 146768 \n",
            "\n",
            "episode: 419 epsilon: 0.2292297471586989\n",
            "duration: 95\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 146863 \n",
            "\n",
            "episode: 420 epsilon: 0.22901208230607337\n",
            "duration: 70\n",
            "max reward: [5.6232715]\n",
            "result: died\n",
            "total steps: 146933 \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [5.3695383]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.6541777]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 73\n",
            "max reward: [5.144986]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.5923324]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 69\n",
            "max reward: [5.144986]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 53\n",
            "max reward: [5.144986]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.34247]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 55\n",
            "max reward: [7.158783]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.4069757]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [5.247999]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.520724]\n",
            " \n",
            "episode: 421 epsilon: 0.22885182994332967\n",
            "duration: 64\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 146997 \n",
            "\n",
            "episode: 422 epsilon: 0.22870541163102362\n",
            "duration: 96\n",
            "max reward: [5.6740427]\n",
            "result: died\n",
            "total steps: 147093 \n",
            "\n",
            "episode: 423 epsilon: 0.22848595978959566\n",
            "duration: 38\n",
            "max reward: [5.0073137]\n",
            "result: died\n",
            "total steps: 147131 \n",
            "\n",
            "episode: 424 epsilon: 0.22839915161947252\n",
            "duration: 37\n",
            "max reward: [5.2895985]\n",
            "result: died\n",
            "total steps: 147168 \n",
            "\n",
            "episode: 425 epsilon: 0.22831465956536726\n",
            "duration: 47\n",
            "max reward: [5.255205]\n",
            "result: died\n",
            "total steps: 147215 \n",
            "\n",
            "episode: 426 epsilon: 0.22820737688877543\n",
            "duration: 53\n",
            "max reward: [6.1792307]\n",
            "result: died\n",
            "total steps: 147268 \n",
            "\n",
            "episode: 427 epsilon: 0.22808645902508867\n",
            "duration: 106\n",
            "max reward: [5.234503]\n",
            "result: died\n",
            "total steps: 147374 \n",
            "\n",
            "episode: 428 epsilon: 0.22784481547223104\n",
            "duration: 22\n",
            "max reward: [5.2023416]\n",
            "result: died\n",
            "total steps: 147396 \n",
            "\n",
            "episode: 429 epsilon: 0.22779469512626732\n",
            "duration: 39\n",
            "max reward: [5.994452]\n",
            "result: died\n",
            "total steps: 147435 \n",
            "\n",
            "episode: 430 epsilon: 0.22770587251670282\n",
            "duration: 65\n",
            "max reward: [5.2865915]\n",
            "result: died\n",
            "total steps: 147500 \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [5.4532814]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 82\n",
            "max reward: [5.7410545]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 61\n",
            "max reward: [5.361971]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.3763685]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [5.594548]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [5.769434]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 114\n",
            "max reward: [5.494548]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.0303044]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 128\n",
            "max reward: [5.126096]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [5.427519]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.4375124]\n",
            " \n",
            "episode: 431 epsilon: 0.2275579117920119\n",
            "duration: 103\n",
            "max reward: [5.3691363]\n",
            "result: died\n",
            "total steps: 147603 \n",
            "\n",
            "episode: 432 epsilon: 0.22732364780952802\n",
            "duration: 62\n",
            "max reward: [6.6140995]\n",
            "result: died\n",
            "total steps: 147665 \n",
            "\n",
            "episode: 433 epsilon: 0.22718275083046297\n",
            "duration: 55\n",
            "max reward: [5.504033]\n",
            "result: died\n",
            "total steps: 147720 \n",
            "\n",
            "episode: 434 epsilon: 0.22705783467259857\n",
            "duration: 45\n",
            "max reward: [6.1678143]\n",
            "result: died\n",
            "total steps: 147765 \n",
            "\n",
            "episode: 435 epsilon: 0.22695568163315363\n",
            "duration: 100\n",
            "max reward: [5.2474]\n",
            "result: died\n",
            "total steps: 147865 \n",
            "\n",
            "episode: 436 epsilon: 0.22672883939154476\n",
            "duration: 50\n",
            "max reward: [5.885518]\n",
            "result: died\n",
            "total steps: 147915 \n",
            "\n",
            "episode: 437 epsilon: 0.226615503308231\n",
            "duration: 77\n",
            "max reward: [5.462104]\n",
            "result: died\n",
            "total steps: 147992 \n",
            "\n",
            "episode: 438 epsilon: 0.22644107653361006\n",
            "duration: 80\n",
            "max reward: [5.525102]\n",
            "result: died\n",
            "total steps: 148072 \n",
            "\n",
            "episode: 439 epsilon: 0.22625999611420852\n",
            "duration: 204\n",
            "max reward: [5.573714]\n",
            "result: died\n",
            "total steps: 148276 \n",
            "\n",
            "episode: 440 epsilon: 0.2257988962039535\n",
            "duration: 104\n",
            "max reward: [5.7489977]\n",
            "result: died\n",
            "total steps: 148380 \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [5.899062]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [5.318923]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 103\n",
            "max reward: [5.8731594]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 107\n",
            "max reward: [6.008971]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [5.631176]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 135\n",
            "max reward: [5.6961985]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [5.1858816]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 112\n",
            "max reward: [5.899062]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 161\n",
            "max reward: [5.899062]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.5118327]\n",
            "result: died \n",
            "\n",
            "Average duration: 1000.0\n",
            "Average max reward: [5.692333]\n",
            " \n",
            "episode: 441 epsilon: 0.22556418742162324\n",
            "duration: 59\n",
            "max reward: [5.7309895]\n",
            "result: died\n",
            "total steps: 148439 \n",
            "\n",
            "episode: 442 epsilon: 0.22543114380277143\n",
            "duration: 37\n",
            "max reward: [6.002821]\n",
            "result: died\n",
            "total steps: 148476 \n",
            "\n",
            "episode: 443 epsilon: 0.22534774970842322\n",
            "duration: 101\n",
            "max reward: [5.5701904]\n",
            "result: died\n",
            "total steps: 148577 \n",
            "\n",
            "episode: 444 epsilon: 0.22512026338115118\n",
            "duration: 23\n",
            "max reward: [5.454985]\n",
            "result: died\n",
            "total steps: 148600 \n",
            "\n",
            "episode: 445 epsilon: 0.22506849167454804\n",
            "duration: 48\n",
            "max reward: [5.4166946]\n",
            "result: died\n",
            "total steps: 148648 \n",
            "\n",
            "episode: 446 epsilon: 0.22496048472228652\n",
            "duration: 29\n",
            "max reward: [5.3852296]\n",
            "result: died\n",
            "total steps: 148677 \n",
            "\n",
            "episode: 447 epsilon: 0.2248952556403911\n",
            "duration: 25\n",
            "max reward: [5.5858107]\n",
            "result: died\n",
            "total steps: 148702 \n",
            "\n",
            "episode: 448 epsilon: 0.2248390388538721\n",
            "duration: 24\n",
            "max reward: [5.460645]\n",
            "result: died\n",
            "total steps: 148726 \n",
            "\n",
            "episode: 449 epsilon: 0.22478508395939348\n",
            "duration: 34\n",
            "max reward: [5.6043854]\n",
            "result: died\n",
            "total steps: 148760 \n",
            "\n",
            "episode: 450 epsilon: 0.22470867002195277\n",
            "duration: 51\n",
            "max reward: [6.291335]\n",
            "result: died\n",
            "total steps: 148811 \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.37558]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.603728]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [6.002821]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [6.489995]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 49\n",
            "max reward: [6.1669874]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [5.529028]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 515.6\n",
            "Average max reward: [56.91681]\n",
            " \n",
            "episode: 451 epsilon: 0.22459409781863685\n",
            "duration: 40\n",
            "max reward: [6.0043774]\n",
            "result: died\n",
            "total steps: 148851 \n",
            "\n",
            "episode: 452 epsilon: 0.22450427814464172\n",
            "duration: 43\n",
            "max reward: [6.232235]\n",
            "result: died\n",
            "total steps: 148894 \n",
            "\n",
            "episode: 453 epsilon: 0.22440776205748544\n",
            "duration: 24\n",
            "max reward: [6.240378]\n",
            "result: died\n",
            "total steps: 148918 \n",
            "\n",
            "episode: 454 epsilon: 0.22435391065701818\n",
            "duration: 30\n",
            "max reward: [6.054361]\n",
            "result: died\n",
            "total steps: 148948 \n",
            "\n",
            "episode: 455 epsilon: 0.22428661457873755\n",
            "duration: 23\n",
            "max reward: [5.287391]\n",
            "result: died\n",
            "total steps: 148971 \n",
            "\n",
            "episode: 456 epsilon: 0.2242350345893106\n",
            "duration: 31\n",
            "max reward: [6.647705]\n",
            "result: died\n",
            "total steps: 149002 \n",
            "\n",
            "episode: 457 epsilon: 0.22416553250196805\n",
            "duration: 66\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 149068 \n",
            "\n",
            "episode: 458 epsilon: 0.22401763206303035\n",
            "duration: 44\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 149112 \n",
            "\n",
            "episode: 459 epsilon: 0.2239190859866493\n",
            "duration: 74\n",
            "max reward: [6.0175276]\n",
            "result: died\n",
            "total steps: 149186 \n",
            "\n",
            "episode: 460 epsilon: 0.22375344715694484\n",
            "duration: 60\n",
            "max reward: [5.98604]\n",
            "result: died\n",
            "total steps: 149246 \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [5.24185]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.5179]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.994385]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.20856]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [107.91884]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [5.323643]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 28\n",
            "max reward: [106.95061]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [5.4125175]\n",
            "result: died \n",
            "\n",
            "Average duration: 323.6\n",
            "Average max reward: [76.8969]\n",
            "Saving monster.model.460 ...\n",
            "Done saving.\n",
            " \n",
            "episode: 461 epsilon: 0.22361923535621728\n",
            "duration: 128\n",
            "max reward: [6.0870523]\n",
            "result: died\n",
            "total steps: 149374 \n",
            "\n",
            "episode: 462 epsilon: 0.22333318584570333\n",
            "duration: 57\n",
            "max reward: [5.388714]\n",
            "result: died\n",
            "total steps: 149431 \n",
            "\n",
            "episode: 463 epsilon: 0.22320592220335497\n",
            "duration: 71\n",
            "max reward: [5.225379]\n",
            "result: died\n",
            "total steps: 149502 \n",
            "\n",
            "episode: 464 epsilon: 0.223047502244331\n",
            "duration: 112\n",
            "max reward: [5.9567194]\n",
            "result: died\n",
            "total steps: 149614 \n",
            "\n",
            "episode: 465 epsilon: 0.22279782888499777\n",
            "duration: 315\n",
            "max reward: [6.049713]\n",
            "result: died\n",
            "total steps: 149929 \n",
            "\n",
            "episode: 466 epsilon: 0.22209711992002848\n",
            "duration: 42\n",
            "max reward: [108.]\n",
            "result: coin\n",
            "total steps: 149971 \n",
            "\n",
            "episode: 467 epsilon: 0.2220038587158859\n",
            "duration: 25\n",
            "max reward: [5.984154]\n",
            "result: died\n",
            "total steps: 149996 \n",
            "\n",
            "episode: 468 epsilon: 0.2219483646882494\n",
            "duration: 161\n",
            "max reward: [5.6275854]\n",
            "result: died\n",
            "total steps: 150157 \n",
            "\n",
            "episode: 469 epsilon: 0.22159131532296605\n",
            "duration: 23\n",
            "max reward: [5.725411]\n",
            "result: died\n",
            "total steps: 150180 \n",
            "\n",
            "episode: 470 epsilon: 0.2215403551810827\n",
            "duration: 1002\n",
            "max reward: [4.6346936]\n",
            "result: timeout\n",
            "total steps: 151182 \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.9993477]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.4091673]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.437111]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [4.111949]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.3164377]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.346179]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.7263365]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.3059874]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.101633]\n",
            "result: timeout \n",
            "\n",
            "Evaluating...\n",
            "duration: 1002\n",
            "max reward: [3.4091673]\n",
            "result: timeout \n",
            "\n",
            "Average duration: 1002.0\n",
            "Average max reward: [3.516332]\n",
            " \n",
            "episode: 471 epsilon: 0.21933160513979316\n",
            "duration: 937\n",
            "max reward: [6.6209326]\n",
            "result: died\n",
            "total steps: 152119 \n",
            "\n",
            "episode: 472 epsilon: 0.21728606631500283\n",
            "duration: 394\n",
            "max reward: [5.346714]\n",
            "result: died\n",
            "total steps: 152513 \n",
            "\n",
            "episode: 473 epsilon: 0.21643164353191394\n",
            "duration: 719\n",
            "max reward: [107.96939]\n",
            "result: coin\n",
            "total steps: 153232 \n",
            "\n",
            "episode: 474 epsilon: 0.21488108096717043\n",
            "duration: 72\n",
            "max reward: [5.5557876]\n",
            "result: died\n",
            "total steps: 153304 \n",
            "\n",
            "episode: 475 epsilon: 0.21472642227268532\n",
            "duration: 111\n",
            "max reward: [5.693766]\n",
            "result: died\n",
            "total steps: 153415 \n",
            "\n",
            "episode: 476 epsilon: 0.21448820817724426\n",
            "duration: 85\n",
            "max reward: [5.517519]\n",
            "result: died\n",
            "total steps: 153500 \n",
            "\n",
            "episode: 477 epsilon: 0.2143059706622097\n",
            "duration: 87\n",
            "max reward: [5.3904486]\n",
            "result: died\n",
            "total steps: 153587 \n",
            "\n",
            "episode: 478 epsilon: 0.2141196055483131\n",
            "duration: 195\n",
            "max reward: [7.8209753]\n",
            "result: died\n",
            "total steps: 153782 \n",
            "\n",
            "episode: 479 epsilon: 0.21370247914791118\n",
            "duration: 212\n",
            "max reward: [5.9872246]\n",
            "result: died\n",
            "total steps: 153994 \n",
            "\n",
            "episode: 480 epsilon: 0.2132499097851445\n",
            "duration: 49\n",
            "max reward: [6.1120906]\n",
            "result: died\n",
            "total steps: 154043 \n",
            "\n",
            "Evaluating...\n",
            "duration: 57\n",
            "max reward: [7.6000557]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [107.49814]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.445942]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 174\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 130\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 42\n",
            "max reward: [107.94267]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.2816086]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [106.94064]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Average duration: 347.7\n",
            "Average max reward: [77.47091]\n",
            " \n",
            "episode: 481 epsilon: 0.21314544292582047\n",
            "duration: 80\n",
            "max reward: [5.4176836]\n",
            "result: died\n",
            "total steps: 154123 \n",
            "\n",
            "episode: 482 epsilon: 0.21297499475983678\n",
            "duration: 179\n",
            "max reward: [7.0430193]\n",
            "result: died\n",
            "total steps: 154302 \n",
            "\n",
            "episode: 483 epsilon: 0.2125941105123175\n",
            "duration: 232\n",
            "max reward: [7.267149]\n",
            "result: died\n",
            "total steps: 154534 \n",
            "\n",
            "episode: 484 epsilon: 0.21210146386700587\n",
            "duration: 945\n",
            "max reward: [5.6066923]\n",
            "result: died\n",
            "total steps: 155479 \n",
            "\n",
            "episode: 485 epsilon: 0.21010654586692137\n",
            "duration: 1002\n",
            "max reward: [3.]\n",
            "result: timeout\n",
            "total steps: 156481 \n",
            "\n",
            "episode: 486 epsilon: 0.20801179052774424\n",
            "duration: 1002\n",
            "max reward: [1.1497834]\n",
            "result: timeout\n",
            "total steps: 157483 \n",
            "\n",
            "episode: 487 epsilon: 0.2059379198302755\n",
            "duration: 558\n",
            "max reward: [5.746441]\n",
            "result: died\n",
            "total steps: 158041 \n",
            "\n",
            "episode: 488 epsilon: 0.20479198636534177\n",
            "duration: 111\n",
            "max reward: [5.8593388]\n",
            "result: died\n",
            "total steps: 158152 \n",
            "\n",
            "episode: 489 epsilon: 0.20456479337591238\n",
            "duration: 399\n",
            "max reward: [6.2292957]\n",
            "result: died\n",
            "total steps: 158551 \n",
            "\n",
            "episode: 490 epsilon: 0.2037502060327844\n",
            "duration: 239\n",
            "max reward: [7.7171106]\n",
            "result: died\n",
            "total steps: 158790 \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [6.262514]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [6.005291]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [5.398509]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [6.262514]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [6.262514]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [5.893219]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 176\n",
            "max reward: [6.262514]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 457\n",
            "max reward: [107.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 54\n",
            "max reward: [5.832019]\n",
            "result: died \n",
            "\n",
            "Average duration: 849.6\n",
            "Average max reward: [26.411911]\n",
            " \n",
            "episode: 491 epsilon: 0.20326382449782202\n",
            "duration: 372\n",
            "max reward: [5.345831]\n",
            "result: died\n",
            "total steps: 159162 \n",
            "\n",
            "episode: 492 epsilon: 0.20250908775140064\n",
            "duration: 355\n",
            "max reward: [5.675463]\n",
            "result: died\n",
            "total steps: 159517 \n",
            "\n",
            "episode: 493 epsilon: 0.20179145504160675\n",
            "duration: 1002\n",
            "max reward: [5.067356]\n",
            "result: timeout\n",
            "total steps: 160519 \n",
            "\n",
            "episode: 494 epsilon: 0.19977960088397162\n",
            "duration: 1002\n",
            "max reward: [3.9437866]\n",
            "result: timeout\n",
            "total steps: 161521 \n",
            "\n",
            "episode: 495 epsilon: 0.19778780484599648\n",
            "duration: 1002\n",
            "max reward: [0.9642689]\n",
            "result: timeout\n",
            "total steps: 162523 \n",
            "\n",
            "episode: 496 epsilon: 0.19581586694888933\n",
            "duration: 1002\n",
            "max reward: [4.773491]\n",
            "result: timeout\n",
            "total steps: 163525 \n",
            "\n",
            "episode: 497 epsilon: 0.19386358920764\n",
            "duration: 622\n",
            "max reward: [5.317869]\n",
            "result: died\n",
            "total steps: 164147 \n",
            "\n",
            "episode: 498 epsilon: 0.19266150005560426\n",
            "duration: 995\n",
            "max reward: [5.4972324]\n",
            "result: died\n",
            "total steps: 165142 \n",
            "\n",
            "episode: 499 epsilon: 0.19075402356265592\n",
            "duration: 644\n",
            "max reward: [5.8220224]\n",
            "result: died\n",
            "total steps: 165786 \n",
            "\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "SAVE_FILENAME = 'monster.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = ONE_MONSTER\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksX662zTHmq1"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This loads the saved model (even if there is a number appended to the end) and evaluates it.\n",
        "\n",
        "It also produces a dump of screenshots for each run of ```evaluate()``` in ```/content/cache/```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaQzdCi8nfs3"
      },
      "source": [
        "###### Evaluate the easy.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1NEZBIlj3IC",
        "outputId": "fcd22a09-e16e-4ef6-c0cd-45237b178303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/easy.model.170 ...\n",
            "Done loading.\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.13231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [106.806076]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 33\n",
            "max reward: [106.81758]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [106.93531]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 50\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 52\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 145\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [107.00875]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 108\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'easy.model'\n",
        "TEST_SEED = EASY_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1es9x93nkJ-"
      },
      "source": [
        "###### Evaluate the medium.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5RAP5k4k-4o",
        "outputId": "0fad72d8-c5a9-4147-9a85-c6f1196988d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/medium.model.380 ...\n",
            "Done loading.\n",
            "Evaluating...\n",
            "duration: 114\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 602\n",
            "max reward: [118.3721]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 118\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 86\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 153\n",
            "max reward: [118.352776]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 439\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 206\n",
            "max reward: [118.8987]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 272\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 108\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 81\n",
            "max reward: [118.0763]\n",
            "result: coin \n",
            "\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'medium.model'\n",
        "TEST_SEED = MEDIUM_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKBYHZNnl1b"
      },
      "source": [
        "###### Evaluate the monster.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEugWNGhlDyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9759863b-48f3-488f-d548-645cdb31cad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/monster.model.460 ...\n",
            "Done loading.\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [5.4868336]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.20856]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [107.06554]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.93341]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [6.6005507]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [106.87934]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.25714]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [5.87613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.2906632]\n",
            "result: died \n",
            "\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'monster.model'\n",
        "TEST_SEED = ONE_MONSTER\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0QC8n8Xj7WD"
      },
      "source": [
        "#GRADING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7PfcWHnoFJ"
      },
      "source": [
        "###### Grade the easy.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-LNqy8SVoJx",
        "outputId": "7fbfe1d4-b451-400e-e798-7e6b8fc7ca59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/easy.model.170 ...\n",
            "Done loading.\n",
            "EASY LEVEL TRY 0\n",
            "Evaluating...\n",
            "duration: 50\n",
            "max reward: [107.0753]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.13148]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [106.92502]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 58\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 64\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 52\n",
            "max reward: [106.82183]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [106.979095]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 52\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 130\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [107.12547]\n",
            "result: coin \n",
            "\n",
            "EASY SCORE 0 4.0 52.5\n",
            "EASY LEVEL TRY 1\n",
            "Evaluating...\n",
            "duration: 143\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 62\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 32\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 92\n",
            "max reward: [107.01174]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [106.95583]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [106.85184]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "EASY SCORE 1 4.0 57.5\n",
            "EASY LEVEL TRY 2\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [106.90471]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 141\n",
            "max reward: [106.89514]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [107.24538]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [107.04209]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 45\n",
            "max reward: [107.03418]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 94\n",
            "max reward: [107.033676]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 36\n",
            "max reward: [106.94118]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [107.15009]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 47\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "EASY SCORE 2 4.0 54.1\n",
            "EASY LEVEL TRY 3\n",
            "Evaluating...\n",
            "duration: 31\n",
            "max reward: [107.16231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 143\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [106.99911]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [107.14778]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 49\n",
            "max reward: [106.90545]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.10231]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 23\n",
            "max reward: [107.0646]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [107.00719]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 71\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [107.03492]\n",
            "result: coin \n",
            "\n",
            "EASY SCORE 3 5.0 48.9\n",
            "BEST EASY SCORE 5.0 48.9\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'easy.model'\n",
        "TEST_SEED = EASY_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_easy_duration = None\n",
        "best_easy_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"EASY LEVEL TRY\", i)\n",
        "    easy_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 150:\n",
        "      easy_grade = easy_grade + 3\n",
        "    if average_duration < 100:\n",
        "      easy_grade = easy_grade + 1\n",
        "    if average_duration < 50:\n",
        "      easy_grade = easy_grade + 1\n",
        "    if easy_grade > best_easy_grade:\n",
        "      best_easy_grade = easy_grade\n",
        "    if best_easy_duration is None or average_duration < best_easy_duration:\n",
        "      best_easy_duration = average_duration\n",
        "    print(\"EASY SCORE\", i, easy_grade, average_duration)\n",
        "    if easy_grade >= 5.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"EASY GRADING BROKE\")\n",
        "\n",
        "print(\"BEST EASY SCORE\", best_easy_grade, best_easy_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq0uNKnSnqLX"
      },
      "source": [
        "###### Grade the medium.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX31CQrNcBh_",
        "outputId": "d77df40c-55d3-412e-9d3d-6388635c62f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/medium.model.380 ...\n",
            "Done loading.\n",
            "MEDIUM LEVEL TRY 0\n",
            "Evaluating...\n",
            "duration: 114\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 155\n",
            "max reward: [118.38433]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [118.07838]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 102\n",
            "max reward: [117.96403]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 160\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 149\n",
            "max reward: [118.169266]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 203\n",
            "max reward: [119.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 84\n",
            "max reward: [118.552895]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 70\n",
            "max reward: [117.93533]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 88\n",
            "max reward: [118.97]\n",
            "result: coin \n",
            "\n",
            "MEDIUM SCORE 0 1.0 119.3\n",
            "BEST MEDIUM SCORE 1.0 119.3\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'medium.model'\n",
        "TEST_SEED = MEDIUM_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_medium_duration = None\n",
        "best_medium_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"MEDIUM LEVEL TRY\", i)\n",
        "    medium_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 150:\n",
        "      medium_grade = medium_grade + 1\n",
        "    if medium_grade > best_medium_grade:\n",
        "      best_medium_grade = medium_grade\n",
        "    if best_medium_duration is None or average_duration < best_medium_duration:\n",
        "      best_medium_duration = average_duration\n",
        "    print(\"MEDIUM SCORE\", i, medium_grade, average_duration)\n",
        "    if medium_grade >= 1.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"MEDIUM GRADING BROKE\")\n",
        "\n",
        "print(\"BEST MEDIUM SCORE\", best_medium_grade, best_medium_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdzsgwmmnr2S"
      },
      "source": [
        "###### Grade the monster.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkhVr7D0cK61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3593d80c-a712-44ee-a540-43b15cc10a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved_models/monster.model.460 ...\n",
            "Done loading.\n",
            "MONSTER LEVEL TRY 0\n",
            "Evaluating...\n",
            "duration: 38\n",
            "max reward: [5.87613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 66\n",
            "max reward: [5.0165167]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 55\n",
            "max reward: [5.93013]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 45\n",
            "max reward: [107.605225]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.614759]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "MONSTER SCORE 0 0.0 425.4\n",
            "MONSTER LEVEL TRY 1\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [5.3265867]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [5.913707]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 47\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.5179]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [5.87613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.335838]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.57261]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 100\n",
            "max reward: [5.5334167]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [8.]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "MONSTER SCORE 1 0.0 617.2\n",
            "MONSTER LEVEL TRY 2\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.20856]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.243416]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.46969]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.238556]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 57\n",
            "max reward: [107.214554]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [106.92421]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 78\n",
            "max reward: [5.621786]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 58\n",
            "max reward: [5.30742]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.6971383]\n",
            "result: died \n",
            "\n",
            "MONSTER SCORE 2 0.0 329.2\n",
            "MONSTER LEVEL TRY 3\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 60\n",
            "max reward: [5.913707]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [6.0307474]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.45967]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [6.6091986]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 81\n",
            "max reward: [5.6702237]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 68\n",
            "max reward: [5.87613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 22\n",
            "max reward: [6.2687206]\n",
            "result: died \n",
            "\n",
            "MONSTER SCORE 3 0.0 615.8\n",
            "MONSTER LEVEL TRY 4\n",
            "Evaluating...\n",
            "duration: 47\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 46\n",
            "max reward: [107.32383]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 51\n",
            "max reward: [6.0408907]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [6.012742]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [8.]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 45\n",
            "max reward: [5.87613]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.5976]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "MONSTER SCORE 4 0.0 423.4\n",
            "MONSTER LEVEL TRY 5\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [5.614759]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 24\n",
            "max reward: [106.88774]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 35\n",
            "max reward: [6.079794]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 21\n",
            "max reward: [6.240275]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 43\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 41\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.5179]\n",
            "result: coin \n",
            "\n",
            "MONSTER SCORE 5 0.0 327.4\n",
            "MONSTER LEVEL TRY 6\n",
            "Evaluating...\n",
            "duration: 44\n",
            "max reward: [107.5179]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 39\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 56\n",
            "max reward: [107.40057]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 27\n",
            "max reward: [106.95495]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 40\n",
            "max reward: [107.54837]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 25\n",
            "max reward: [6.290266]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 29\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 26\n",
            "max reward: [106.95648]\n",
            "result: coin \n",
            "\n",
            "Evaluating...\n",
            "duration: 37\n",
            "max reward: [5.913707]\n",
            "result: died \n",
            "\n",
            "Evaluating...\n",
            "duration: 30\n",
            "max reward: [108.]\n",
            "result: coin \n",
            "\n",
            "MONSTER SCORE 6 1.0 229.1\n",
            "BEST MONSTER SCORE 1.0 229.1\n"
          ]
        }
      ],
      "source": [
        "TEST_FILENAME = 'monster.model'\n",
        "TEST_SEED = ONE_MONSTER\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_monster_duration = None\n",
        "best_monster_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"MONSTER LEVEL TRY\", i)\n",
        "    monster_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 300:\n",
        "      monster_grade = monster_grade + 1\n",
        "    if monster_grade > best_monster_grade:\n",
        "      best_monster_grade = monster_grade\n",
        "    if best_monster_duration is None or average_duration < best_monster_duration:\n",
        "      best_monster_duration = average_duration\n",
        "    print(\"MONSTER SCORE\", i, monster_grade, average_duration)\n",
        "    if monster_grade >= 1.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"MONSTER GRADING BROKE\")\n",
        "\n",
        "print(\"BEST MONSTER SCORE\", best_monster_grade, best_monster_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsTL1mJHpPu"
      },
      "source": [
        "# Download The Model\n",
        "\n",
        "If you want to download the model to a computer without NVIDIA GPU and CUDA support, you can use the cell below. It takes the model indicated, converts it to run on CPU, and saves it. You can then open the file browser to the left and download the model file.<br/> <br/><b>Change the model name in load_model argument below to download medium and monster models respectively.</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwWE-O3MYNVy"
      },
      "outputs": [],
      "source": [
        "### USE THIS TO COVERT A MODEL FROM CUDA TO CPU\n",
        "### DOWNLOAD USING THE FILES MENU TO THE LEFT\n",
        "\n",
        "### CHANGE THE MODEL NAME BELOW FOR MEDIUM AND MONSTER MODELS\n",
        "net = load_model('easy.model')\n",
        "net = net.to('cpu')\n",
        "torch.save(net, os.path.join(MODEL_PATH, 'saved_cpu.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ4XRSaksJxE"
      },
      "source": [
        "# Show Evaluation Movies\n",
        "\n",
        "The following cells will render an animation in the notebook. It is flickery but you should get an idea of what your agent was duing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQjdIOubsZVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e438b2ea-ddb9-4141-85b2-1260aba48875"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-bd2443600150>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_run_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshow_movie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_run_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-3abaabc46edd>\u001b[0m in \u001b[0;36mshow_movie\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mipythondisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mipythondisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_anim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2367\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2231\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2233\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    455\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    459\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    399\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3141\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0m_draw_rasterized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_rasterized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3065\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             im, l, b, trans = self.make_image(\n\u001b[0m\u001b[1;32m    642\u001b[0m                 renderer, renderer.get_image_magnification())\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    947\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[1;32m    948\u001b[0m                 else self.figure.bbox)\n\u001b[0;32m--> 949\u001b[0;31m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[1;32m    950\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 output_alpha = _resample(  # resample alpha channel\n\u001b[1;32m    554\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[0;32m--> 555\u001b[0;31m                 output = _resample(  # resample rgb channels\n\u001b[0m\u001b[1;32m    556\u001b[0m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n\u001b[1;32m    557\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_alpha\u001b[0m  \u001b[0;31m# recombine rgb and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     _image.resample(data, out, transform,\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \u001b[0m_interpd_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# This seems to work but is really slow and flickery\n",
        "\n",
        "eval_run_number = 1\n",
        "show_movie(eval_run_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avR87kPlsv46"
      },
      "source": [
        "Alternatively, the following makes an animated gif. The animation is faster and doesn't flicker once loaded. Unfortunately, colab loads animated gifs really slow.\n",
        "Better to download and view on one's own machine (though this is cumbersome)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnqqHTBotBQX"
      },
      "outputs": [],
      "source": [
        "eval_run_number = 1\n",
        "make_anim(eval_run_number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}